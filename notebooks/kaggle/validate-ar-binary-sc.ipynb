{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12342718,"sourceType":"datasetVersion","datasetId":7780982},{"sourceId":12359253,"sourceType":"datasetVersion","datasetId":7792091},{"sourceId":12458325,"sourceType":"datasetVersion","datasetId":7858910},{"sourceId":12468454,"sourceType":"datasetVersion","datasetId":7863991}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ── cell: 1 ──\n# 📁 Prepare Workspace\n\n\nMODEL_NAME= \"moussaKam_AraBART\" # \"fewshot_gemini\" #\"agemagician_mlong-t5-tglobal-large\"  #\"UBC-NLP_AraT5-base\"\nMODEL_TYPE= \"fine_tuned_model\"\n\nimport os\n\npath = \"/kaggle/working\"\nis_empty = (len(os.listdir(path)) <= 3)\n\n# !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n\nif is_empty:\n    print(f\"✅ '{path}' is empty. copying project strcuture into it ...\")\n    !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n    !mkdir -p /kaggle/working/evaluation/ar_style_classifier/\n    !cp -r /kaggle/input/binary-style-classifier-models-21-authors/working/* /kaggle/working/evaluation/ar_style_classifier/\n    !cp -r /kaggle/input/ast-fine-tuned-models-generations/results/results/* /kaggle/working/models/fine_tuned_model/results/\n\n    !mkdir -p /kaggle/working/models/baseline_model/results/fewshot_gemini\n    !cp -r /kaggle/input/ast-fine-tuned-models-generations/baseline_model/baseline_model/results/fewshot_gemini/generation_log.json /kaggle/working/models/baseline_model/results/fewshot_gemini\nelse:\n    print(f\"❌ '{path}' is not empty — contains {len(os.listdir(path))} items.\")\n\n\n# !cp -r /kaggle/input/binary-style-classifier-models-21-authors/working/* /kaggle/working/evaluation/ar_style_classifier/\n!cp -r /kaggle/input/ast-fine-tuned-models-generations/results/results/* /kaggle/working/models/fine_tuned_model/results/\n\n\n\n# Change to project directory\n%cd /kaggle/working/ \n\n# Confirm\n!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:46:13.499329Z","iopub.execute_input":"2025-07-15T10:46:13.499537Z","iopub.status.idle":"2025-07-15T10:46:17.212677Z","shell.execute_reply.started":"2025-07-15T10:46:13.499519Z","shell.execute_reply":"2025-07-15T10:46:17.211677Z"}},"outputs":[{"name":"stdout","text":"❌ '/kaggle/working' is not empty — contains 11 items.\n/kaggle/working\nconfig.py  evaluation\tmodels\t __pycache__\t  state.db\ndata\t   __init__.py\tprompts  run_pipeline.py  utils\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ls -R /kaggle/working/models/MODEL_TYPE}/results/\n!ls -R /kaggle/working/models/{MODEL_TYPE}/results/{MODEL_NAME}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:46:17.214631Z","iopub.execute_input":"2025-07-15T10:46:17.214970Z","iopub.status.idle":"2025-07-15T10:46:17.464357Z","shell.execute_reply.started":"2025-07-15T10:46:17.214938Z","shell.execute_reply":"2025-07-15T10:46:17.463493Z"}},"outputs":[{"name":"stdout","text":"ls: cannot access '/kaggle/working/models/MODEL_TYPE}/results/': No such file or directory\n/kaggle/working/models/fine_tuned_model/results/google_mt5-small:\ngeneration_log_ERROR.json  generation_log.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers==4.41.2 datasets==2.19.1 scikit-learn wandb python-dotenv evaluate --quiet\n!pip install -U peft==0.11.1 --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:46:17.465403Z","iopub.execute_input":"2025-07-15T10:46:17.465718Z","iopub.status.idle":"2025-07-15T10:48:22.551871Z","shell.execute_reply.started":"2025-07-15T10:46:17.465684Z","shell.execute_reply":"2025-07-15T10:48:22.551040Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m:01\u001b[0mmm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import sys\nimport os\nimport torch\n\nproject_root = os.getcwd()  # Should be /content/drive/MyDrive/AraGenEval2025\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\nprint(f\"Project root added to sys.path: {project_root}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nfrom config import Config\n\nprint(Config.VAL_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:22.554336Z","iopub.execute_input":"2025-07-15T10:48:22.554550Z","iopub.status.idle":"2025-07-15T10:48:22.636196Z","shell.execute_reply.started":"2025-07-15T10:48:22.554529Z","shell.execute_reply":"2025-07-15T10:48:22.635466Z"}},"outputs":[{"name":"stdout","text":"Project root added to sys.path: /kaggle/working\nUsing device: cuda\ndata/AuthorshipStyleTransferVal.xlsx\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Mapping","metadata":{}},{"cell_type":"code","source":"# ── cell: 3 ──\n# 🗺 Author Mappings\nauthor2id = {\n    \"أحمد أمين\": 0,  \"أحمد تيمور باشا\": 1,  \"أحمد شوقي\": 2,\n    \"أمين الريحاني\": 3,  \"ثروت أباظة\": 4,  \"جبران خليل جبران\": 5,\n    \"جُرجي زيدان\": 6,  \"حسن حنفي\": 7,  \"روبرت بار\": 8,\n    \"سلامة موسى\": 9,  \"طه حسين\": 10, \"عباس محمود العقاد\": 11,\n    \"عبد الغفار مكاوي\": 12, \"غوستاف لوبون\": 13,  \"فؤاد زكريا\": 14,\n    \"كامل كيلاني\": 15,  \"محمد حسين هيكل\": 16,  \"نجيب محفوظ\": 17,\n    \"نوال السعداوي\": 18, \"ويليام شيكسبير\": 19, \"يوسف إدريس\": 20\n}\nid2author   = {v: k for k, v in author2id.items()}\nid2english  = {\n    0:\"Ahmed_Amin\",    1:\"Ahmad_Taymour_Basha\",   2:\"Ahmed_Shawqi\",\n    3:\"Ameen_Rihani\",  4:\"Tharwat_Abaza\",         5:\"Gibran_Khalil_Gibran\",\n    6:\"Jurji_Zaydan\",  7:\"Hassan_Hanifi\",         8:\"Robert_Barr\",\n    9:\"Salama_Moussa\",10:\"Taha_Hussein\",         11:\"Abbas_Al-Aqqad\",\n   12:\"AbdelGhaffar_Makawi\",13:\"Gustave_Lebon\",    14:\"Fouad_Zakaria\",\n   15:\"Kamel_Kilani\",16:\"Mohamed_Hosseini_Hekal\",17:\"Naguib_Mahfouz\",\n   18:\"Nawal_El_Saadawi\",19:\"William_Shakespeare\",20:\"Youssef_Edrees\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:22.637058Z","iopub.execute_input":"2025-07-15T10:48:22.637330Z","iopub.status.idle":"2025-07-15T10:48:22.643965Z","shell.execute_reply.started":"2025-07-15T10:48:22.637305Z","shell.execute_reply":"2025-07-15T10:48:22.643248Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport gc\n\ndef load_and_sample_dataset(\n    file_path,\n    sample_mode=\"all\",    # Options: \"all\", \"random\", \"stratified\"\n    sample_size=None,     # For \"random\"/\"stratified\": float (proportion) or int (absolute count)\n    random_state=42\n):\n    \"\"\"\n    Load dataset from Excel and apply sampling.\n\n    Args:\n        file_path (str): Path to dataset file.\n        sample_mode (str): \"all\", \"random\", or \"stratified\".\n        sample_size (float|int): Proportion (0-1) or count for sampling.\n        random_state (int): Random seed for reproducibility.\n\n    Returns:\n        pd.DataFrame: Sampled dataset.\n    \"\"\"\n    print(f\"📂 Loading dataset from: {file_path}\")\n    df = pd.read_excel(file_path, engine='openpyxl')\n\n    if sample_mode == \"all\":\n        sampled_df = df.copy()\n        print(f\"✅ Loaded full dataset with {len(sampled_df)} samples.\")\n\n    elif sample_mode == \"random\":\n        if sample_size is None:\n            raise ValueError(\"For 'random' mode, SAMPLE_SIZE must be set.\")\n        sampled_df = df.sample(\n            n=sample_size if isinstance(sample_size, int) else int(len(df) * sample_size),\n            random_state=random_state\n        )\n        print(f\"✅ Randomly sampled {len(sampled_df)} samples.\")\n\n    elif sample_mode == \"stratified\":\n        if sample_size is None or not (0 < sample_size < 1):\n            raise ValueError(\"For 'stratified' mode, SAMPLE_SIZE must be a proportion between 0 and 1.\")\n        sampled_df, _ = train_test_split(\n            df,\n            train_size=sample_size,\n            stratify=df['author'],\n            random_state=random_state\n        )\n        print(f\"✅ Stratified sampled {len(sampled_df)} samples (author distribution preserved).\")\n\n    else:\n        raise ValueError(f\"Invalid sample_mode: {sample_mode}\")\n\n    del df\n    gc.collect()\n\n    return sampled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:22.644763Z","iopub.execute_input":"2025-07-15T10:48:22.645044Z","iopub.status.idle":"2025-07-15T10:48:23.973318Z","shell.execute_reply.started":"2025-07-15T10:48:22.645021Z","shell.execute_reply":"2025-07-15T10:48:23.972740Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# === CONFIG ===\nSAMPLE_MODE = \"stratified\"  # \"all\", \"random\", \"stratified\"\nSAMPLE_SIZE = 0.1           # 10% for stratified or random. Set to None for full dataset.\n\n# === LOAD AND SAMPLE ===\nval_df = load_and_sample_dataset(\n    Config.VAL_FILE,\n    # sample_mode=SAMPLE_MODE,\n    # sample_size=SAMPLE_SIZE\n)\n\n# Expect columns: id, text_in_msa\nval_df = val_df.rename(columns={\"id\":\"sample_id\",\"text_in_msa\":\"neutral_text\"})\nprint(f\"✅ Loaded {len(val_df)} neutral texts.\")\n\nprint(\"Validation shape:\", val_df.shape)\nprint(\"Authors in Val:\", val_df['author'].nunique())\n\n# train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:23.974080Z","iopub.execute_input":"2025-07-15T10:48:23.974473Z","iopub.status.idle":"2025-07-15T10:48:25.255161Z","shell.execute_reply.started":"2025-07-15T10:48:23.974448Z","shell.execute_reply":"2025-07-15T10:48:25.254609Z"}},"outputs":[{"name":"stdout","text":"📂 Loading dataset from: data/AuthorshipStyleTransferVal.xlsx\n✅ Loaded full dataset with 4157 samples.\n✅ Loaded 4157 neutral texts.\nValidation shape: (4157, 4)\nAuthors in Val: 21\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Sliding Window","metadata":{}},{"cell_type":"code","source":"# ── cell: 6 ──\n# 🪓 Sliding Window Chunking Utility\nfrom transformers import AutoTokenizer\nimport torch\n\ndef get_chunks(text, tokenizer, max_len=512, stride=256):\n    return tokenizer(\n        text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_len,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_tensors=\"pt\"\n    )\n\ndef device():\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:25.255881Z","iopub.execute_input":"2025-07-15T10:48:25.256163Z","iopub.status.idle":"2025-07-15T10:48:25.980197Z","shell.execute_reply.started":"2025-07-15T10:48:25.256146Z","shell.execute_reply":"2025-07-15T10:48:25.979598Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Load Model ","metadata":{}},{"cell_type":"code","source":"# ── cell: 7 ──\n# 🔧 Load Classifier Function\nfrom transformers import AutoModelForSequenceClassification\n\ndef load_binary_classifier(author_id):\n    path = f\"evaluation/ar_style_classifier/author_SC_{author_id}\"\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.to(device())\n    model.eval()\n    return tokenizer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:25.980910Z","iopub.execute_input":"2025-07-15T10:48:25.981096Z","iopub.status.idle":"2025-07-15T10:48:25.995318Z","shell.execute_reply.started":"2025-07-15T10:48:25.981081Z","shell.execute_reply":"2025-07-15T10:48:25.994793Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Preparations","metadata":{}},{"cell_type":"code","source":"# ── cell: 5 ──\n# 📝 Load Generation Log\nimport json\nwith open(f\"models/{MODEL_TYPE}/results/{MODEL_NAME}/generation_log.json\",\"r\", encoding=\"utf-8\") as f:\n    gen = json.load(f)\n# gen: list of {id, author, output, ground_truth}\nlog_df = pd.DataFrame(gen)\nlog_df = log_df.rename(columns={\"id\":\"sample_id\",\"output\":\"generated_text\",\"author\":\"true_author\"})\nprint(f\"✅ Loaded {len(log_df)} generated entries.\")\n\n# Merge neutral and generated\ndf = pd.merge(log_df, val_df, on=\"sample_id\", how=\"left\")\ndf[\"true_id\"] = df[\"true_author\"].map(author2id)\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:25.997481Z","iopub.execute_input":"2025-07-15T10:48:25.997698Z","iopub.status.idle":"2025-07-15T10:48:26.229176Z","shell.execute_reply.started":"2025-07-15T10:48:25.997682Z","shell.execute_reply":"2025-07-15T10:48:26.228350Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded 4157 generated entries.\n   sample_id true_author                                     generated_text  \\\n0       3835  يوسف إدريس  ومنذ أن كنت طفلًا في الخمسين من عمري لم أقم مط...   \n1       3836  يوسف إدريس  وذلك الزمن العام هو العداد الجماعي الذي يسجل ا...   \n2       3837  يوسف إدريس  ومصر الغنية الراقية والمصنِّعة والعالم العربي ...   \n3       3838  يوسف إدريس  ونظرًا لغرابة تجربتي، وللأفكار التي راودتني حو...   \n4       3839  يوسف إدريس  ولكن ما ذكره ليس مرارةً ولا ندمًا؛ فقد كان ما ...   \n\n                                        ground_truth  \\\n0  من طفل في الخمسين\\n\\nعمري ما احتفلت أو حفلت بع...   \n1  ذلك الزمن العام هو العداد العام الذي\\n\\nدام يع...   \n2  مصر الغنية المثقفة المصنِّعة، والعرب\\n\\nوقد\\n\\...   \n3  ولأنها غريبة وراودتني فيها عن الناس وعن الحياة...   \n4  وليس ما ذكرته مرارة ولا ندمًا؛ فقد كان لا يمكن...   \n\n                                        neutral_text  \\\n0  منذ أن كنت طفلاً في الخمسين من عمري، لم أقم مط...   \n1  ذلك الزمن العام يُعتبر بمثابة العداد الجماعي ا...   \n2  مصر، الغنية، الراقية، والمصنِّعة، والعالم العر...   \n3  نظرًا لغرابة تجربتي، وللأفكار التي راودتني حول...   \n4  إن ما تم ذكره ليس مرارةً ولا ندمًا؛ فقد كان ما...   \n\n                                text_in_author_style      author  true_id  \n0  من طفل في الخمسين\\n\\nعمري ما احتفلت أو حفلت بع...  يوسف إدريس       20  \n1  ذلك الزمن العام هو العداد العام الذي\\n\\nدام يع...  يوسف إدريس       20  \n2  مصر الغنية المثقفة المصنِّعة، والعرب\\n\\nوقد\\n\\...  يوسف إدريس       20  \n3  ولأنها غريبة وراودتني فيها عن الناس وعن الحياة...  يوسف إدريس       20  \n4  وليس ما ذكرته مرارة ولا ندمًا؛ فقد كان لا يمكن...  يوسف إدريس       20  \n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# ── cell: 8 ──\n# 🔍 Probability Prediction (in-style)\nimport numpy as np\n\n# def predict_instyle_prob(text, tokenizer, model):\n#     enc = get_chunks(text, tokenizer)\n#     enc = {k: v.to(device()) for k,v in enc.items()}\n#     with torch.no_grad():\n#         logits = model(**enc).logits  # [chunks, 2]\n#         probs = torch.softmax(logits, dim=-1)  # same shape\n#     instyle_probs = probs[:,1]  # author style class\n#     return instyle_probs.mean().item()\n\n\nVALID_KEYS = {\"input_ids\", \"attention_mask\", \"token_type_ids\"}\n\ndef predict_instyle_prob(text, tokenizer, model):\n    enc = get_chunks(text, tokenizer)\n    enc = {k: v.to(device()) for k,v in enc.items() if k in VALID_KEYS}\n    with torch.no_grad():\n        logits = model(**enc).logits  # Now works\n        probs = torch.softmax(logits, dim=-1)\n    return probs[:,1].mean().item() # author style class\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:26.229893Z","iopub.execute_input":"2025-07-15T10:48:26.230106Z","iopub.status.idle":"2025-07-15T10:48:26.235354Z","shell.execute_reply.started":"2025-07-15T10:48:26.230089Z","shell.execute_reply":"2025-07-15T10:48:26.234587Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# # ── cell: 9 ──\n# # 📊 Style Evaluation Loop\n# from tqdm import tqdm\n# results = []\n# for author_id in range(21):\n#     tokenizer, model = load_binary_classifier(author_id)\n#     for row in tqdm(df.itertuples(), desc=f\"Author {author_id}\"):\n#         px = predict_instyle_prob(row.neutral_text, tokenizer, model)\n#         py = predict_instyle_prob(row.generated_text, tokenizer, model)\n#         diff = py - px\n#         success = diff > 0\n#         results.append({\n#             \"sample_id\": row.sample_id,\n#             \"true_id\": row.true_id,\n#             \"classifier_id\": author_id,\n#             \"neutral_prob\": px,\n#             \"generated_prob\": py,\n#             \"diff\": diff,\n#             \"success\": int(success)\n#         })\n# results_df = pd.DataFrame(results)\n\n\n\n# ── cell: 9 ──\n# 📊 Style Evaluation Loop (per‑author filtering)\nfrom tqdm import tqdm\n\nresults = []\n\nfor author_id in range(21):\n    # 1) select only the samples meant for this author\n    author_df = df[df[\"true_id\"] == author_id]\n\n    # 2) load that author’s classifier\n    tokenizer, model = load_binary_classifier(author_id)\n\n    # 3) run only their own samples\n    for row in tqdm(author_df.itertuples(), desc=f\"Author {author_id}\"):\n        px   = predict_instyle_prob(row.neutral_text,   tokenizer, model)\n        py   = predict_instyle_prob(row.generated_text, tokenizer, model)\n        diff = py - px\n        success = int(diff > 0)\n\n        results.append({\n            \"sample_id\":      row.sample_id,\n            \"true_id\":        row.true_id,\n            \"classifier_id\":  author_id,\n            \"neutral_prob\":   px,\n            \"generated_prob\": py,\n            \"diff\":           diff,\n            \"success\":        success\n        })\n\n# build DataFrame after the loop\nresults_df = pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:26.236080Z","iopub.execute_input":"2025-07-15T10:48:26.236315Z","iopub.status.idle":"2025-07-15T10:52:11.086204Z","shell.execute_reply.started":"2025-07-15T10:48:26.236299Z","shell.execute_reply":"2025-07-15T10:52:11.085329Z"}},"outputs":[{"name":"stderr","text":"Author 0: 246it [00:12, 20.03it/s]\nAuthor 1: 57it [00:03, 16.93it/s]\nAuthor 2: 58it [00:03, 19.18it/s]\nAuthor 3: 142it [00:07, 19.11it/s]\nAuthor 4: 90it [00:05, 17.54it/s]\nAuthor 5: 30it [00:01, 17.91it/s]\nAuthor 6: 327it [00:14, 22.23it/s]\nAuthor 7: 548it [00:26, 20.77it/s]\nAuthor 8: 82it [00:03, 24.66it/s]\nAuthor 9: 119it [00:05, 21.30it/s]\nAuthor 10: 255it [00:11, 22.23it/s]\nAuthor 11: 267it [00:11, 22.88it/s]\nAuthor 12: 396it [00:18, 21.13it/s]\nAuthor 13: 150it [00:06, 23.54it/s]\nAuthor 14: 125it [00:05, 24.46it/s]\nAuthor 15: 25it [00:01, 21.31it/s]\nAuthor 16: 260it [00:11, 22.65it/s]\nAuthor 17: 327it [00:16, 19.63it/s]\nAuthor 18: 295it [00:12, 22.81it/s]\nAuthor 19: 238it [00:11, 21.39it/s]\nAuthor 20: 120it [00:05, 22.87it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Save results","metadata":{}},{"cell_type":"code","source":"save_dir = f\"evaluation/ar_style_classifier/results/{MODEL_NAME}\"\n!mkdir -p {save_dir}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.087106Z","iopub.execute_input":"2025-07-15T10:52:11.087865Z","iopub.status.idle":"2025-07-15T10:52:11.269511Z","shell.execute_reply.started":"2025-07-15T10:52:11.087839Z","shell.execute_reply":"2025-07-15T10:52:11.268785Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ── cell: 10 ──\n# 💾 Save Results and Metrics\nresults_df.to_csv(\n    f\"evaluation/ar_style_classifier/results/{MODEL_NAME}/ar_bsc_evaluation_results_.csv\",\n    index=False\n)\n\n# 🧾 Aggregate metrics per classifier\nmetrics = results_df.groupby(\"classifier_id\").agg(\n    accuracy=(\"success\", \"mean\"),\n    avg_diff=(\"diff\", \"mean\"),\n    count=(\"success\", \"size\")\n).reset_index()\n\n# 🌐 Compute overall metrics across all classifiers and samples\noverall_accuracy = results_df[\"success\"].mean()\noverall_avg_diff = results_df[\"diff\"].mean()\n\n# Create an overall-metrics DataFrame row\noverall_row = pd.DataFrame([{\n    \"classifier_id\": -1,\n    \"accuracy\": overall_accuracy,\n    \"avg_diff\": overall_avg_diff,\n    \"count\": len(results_df)\n}])\n\n# Concatenate to include overall stats\nmetrics = pd.concat([metrics, overall_row], ignore_index=True)\n\n# 🖨 Print overall metrics\nprint(f\"Overall Style-Transfer Accuracy: {overall_accuracy:.4f}\")\nprint(f\"Overall Average Diff (strength): {overall_avg_diff:.4f}\")\n\n# 💾 Save both per-author and overall metrics\nmetrics.to_csv(\n    f\"evaluation/ar_style_classifier/results/{MODEL_NAME}/ar_bsc_metrics_per_author_and_overall.csv\",\n    index=False\n)\n\nprint(\"✅ Saved evaluation and per-author metrics CSVs (including overall metric).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.270632Z","iopub.execute_input":"2025-07-15T10:52:11.271026Z","iopub.status.idle":"2025-07-15T10:52:11.334720Z","shell.execute_reply.started":"2025-07-15T10:52:11.271000Z","shell.execute_reply":"2025-07-15T10:52:11.334122Z"}},"outputs":[{"name":"stdout","text":"Overall Style-Transfer Accuracy: 0.5978\nOverall Average Diff (strength): 0.0433\n✅ Saved evaluation and per-author metrics CSVs (including overall metric).\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ── cell: 11 ──\n# 📈 Per-Sample Summary: Identify strongest style match and overall success rate\n# For each sample, find the classifier with highest generated_prob\ntop_matches = results_df.loc[results_df.groupby('sample_id')['generated_prob'].idxmax()]\n# top_matches = top_matches.merge(id2author.to_frame('classifier_id').reset_index(), on='classifier_id')\nid2author_df = pd.DataFrame(list(id2author.items()), columns=['classifier_id', 'predicted_author'])\ntop_matches = top_matches.merge(id2author_df, on='classifier_id')\ntop_matches = top_matches.rename(columns={\n    'classifier_id': 'predicted_id',\n    'index': 'predicted_author'\n})\n# Calculate overall success: did the true author classifier succeed?\ntrue_success = results_df[\n    (results_df['classifier_id'] == results_df['true_id'])\n].groupby('sample_id')['success'].first().reset_index()\n\n# Merge for overview\noverview = top_matches.merge(true_success, on='sample_id')\n\noverview.to_csv(f\"evaluation/ar_style_classifier/results/{MODEL_NAME}/ar_bsc_evaluation_per_sample_overview.csv\", index=False)\nprint(\"✅ Saved per-sample overview CSV: strongest match and true-author success.\")\n\n# 💡 Note: Probabilities are averaged across all chunks of a sample, so each sample has a single neutral_prob and generated_prob per classifier.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.335498Z","iopub.execute_input":"2025-07-15T10:52:11.336006Z","iopub.status.idle":"2025-07-15T10:52:11.386235Z","shell.execute_reply.started":"2025-07-15T10:52:11.335985Z","shell.execute_reply":"2025-07-15T10:52:11.385691Z"}},"outputs":[{"name":"stdout","text":"✅ Saved per-sample overview CSV: strongest match and true-author success.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ── cell: 12 ──\n# 📦 Package Results Archive\nimport os, subprocess\n\nzip_name = f\"ar_style_classifier_results_{MODEL_NAME}.zip\"\nresults_dir = os.path.join(\"evaluation\", \"ar_style_classifier\", \"results\", MODEL_NAME)\n\n# Run compression\n!cd evaluation/ar_style_classifier && zip -r {zip_name} results/{MODEL_NAME}\n\nprint(f\"✅ Created archive: evaluation/ar_style_classifier/{zip_name}\")\n\n# !cd evaluation/ar_style_classifier && zip -r ar_style_classifier_results_{MODEL_NAME}.zip results/{MODEL_NAME}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.387021Z","iopub.execute_input":"2025-07-15T10:52:11.387275Z","iopub.status.idle":"2025-07-15T10:52:11.619706Z","shell.execute_reply.started":"2025-07-15T10:52:11.387252Z","shell.execute_reply":"2025-07-15T10:52:11.618886Z"}},"outputs":[{"name":"stdout","text":"updating: results/google_mt5-small/ (stored 0%)\nupdating: results/google_mt5-small/ar_bsc_evaluation_results_.csv (deflated 59%)\nupdating: results/google_mt5-small/ar_bsc_evaluation_per_sample_overview.csv (deflated 67%)\nupdating: results/google_mt5-small/ar_bsc_metrics_per_author_and_overall.csv (deflated 48%)\n✅ Created archive: evaluation/ar_style_classifier/ar_style_classifier_results_google_mt5-small.zip\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(f\"evaluation/ar_style_classifier/{zip_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.620751Z","iopub.execute_input":"2025-07-15T10:52:11.620992Z","iopub.status.idle":"2025-07-15T10:52:11.627674Z","shell.execute_reply.started":"2025-07-15T10:52:11.620969Z","shell.execute_reply":"2025-07-15T10:52:11.627088Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/evaluation/ar_style_classifier/ar_style_classifier_results_google_mt5-small.zip","text/html":"<a href='evaluation/ar_style_classifier/ar_style_classifier_results_google_mt5-small.zip' target='_blank'>evaluation/ar_style_classifier/ar_style_classifier_results_google_mt5-small.zip</a><br>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# # --- Inference on full validation texts ---\n# def predict_full_text(text):\n#     enc = tokenizer(\n#         text,\n#         truncation=True,\n#         padding=\"max_length\",\n#         max_length=512,\n#         stride=256,\n#         return_overflowing_tokens=True,\n#         return_tensors=\"pt\"\n#     ).to(device)\n#     logits = model(**enc).logits  # [num_chunks, num_labels]\n#     avg_logits = logits.mean(dim=0)\n#     probs = torch.softmax(avg_logits, dim=0)\n#     return torch.argmax(probs).item(), probs.cpu().tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.628311Z","iopub.execute_input":"2025-07-15T10:52:11.628489Z","iopub.status.idle":"2025-07-15T10:52:11.641751Z","shell.execute_reply.started":"2025-07-15T10:52:11.628474Z","shell.execute_reply":"2025-07-15T10:52:11.641052Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# # --- Evaluate across validation set ---\n# preds, refs = [], []\n# for example in val_df.itertuples():\n#     pred, _ = predict_full_text(example.text_in_author_style)\n#     preds.append(pred)\n#     refs.append(author2id[example.author])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.642470Z","iopub.execute_input":"2025-07-15T10:52:11.642675Z","iopub.status.idle":"2025-07-15T10:52:11.659762Z","shell.execute_reply.started":"2025-07-15T10:52:11.642660Z","shell.execute_reply":"2025-07-15T10:52:11.658922Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# from sklearn.metrics import accuracy_score, f1_score\n# print(\"Validation Accuracy:\", accuracy_score(refs, preds))\n# print(\"Validation F1‑macro:\", f1_score(refs, preds, average=\"macro\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.660800Z","iopub.execute_input":"2025-07-15T10:52:11.661075Z","iopub.status.idle":"2025-07-15T10:52:11.673472Z","shell.execute_reply.started":"2025-07-15T10:52:11.661053Z","shell.execute_reply":"2025-07-15T10:52:11.672759Z"}},"outputs":[],"execution_count":20}]}