{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12342718,"sourceType":"datasetVersion","datasetId":7780982},{"sourceId":12359253,"sourceType":"datasetVersion","datasetId":7792091},{"sourceId":12458325,"sourceType":"datasetVersion","datasetId":7858910},{"sourceId":12468454,"sourceType":"datasetVersion","datasetId":7863991}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# â”€â”€ cell: 1 â”€â”€\n# ğŸ“ Prepare Workspace\n\n\nMODEL_NAME= \"moussaKam_AraBART\" # \"fewshot_gemini\" #\"agemagician_mlong-t5-tglobal-large\"  #\"UBC-NLP_AraT5-base\"\nMODEL_TYPE= \"fine_tuned_model\"\n\nimport os\n\npath = \"/kaggle/working\"\nis_empty = (len(os.listdir(path)) <= 3)\n\n# !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n\nif is_empty:\n    print(f\"âœ… '{path}' is empty. copying project strcuture into it ...\")\n    !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n    !mkdir -p /kaggle/working/evaluation/ar_style_classifier/\n    !cp -r /kaggle/input/binary-style-classifier-models-21-authors/working/* /kaggle/working/evaluation/ar_style_classifier/\n    !cp -r /kaggle/input/ast-fine-tuned-models-generations/results/results/* /kaggle/working/models/fine_tuned_model/results/\n\n    !mkdir -p /kaggle/working/models/baseline_model/results/fewshot_gemini\n    !cp -r /kaggle/input/ast-fine-tuned-models-generations/baseline_model/baseline_model/results/fewshot_gemini/generation_log.json /kaggle/working/models/baseline_model/results/fewshot_gemini\nelse:\n    print(f\"âŒ '{path}' is not empty â€” contains {len(os.listdir(path))} items.\")\n\n\n# !cp -r /kaggle/input/binary-style-classifier-models-21-authors/working/* /kaggle/working/evaluation/ar_style_classifier/\n!cp -r /kaggle/input/ast-fine-tuned-models-generations/results/results/* /kaggle/working/models/fine_tuned_model/results/\n\n\n\n# Change to project directory\n%cd /kaggle/working/ \n\n# Confirm\n!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:46:13.499329Z","iopub.execute_input":"2025-07-15T10:46:13.499537Z","iopub.status.idle":"2025-07-15T10:46:17.212677Z","shell.execute_reply.started":"2025-07-15T10:46:13.499519Z","shell.execute_reply":"2025-07-15T10:46:17.211677Z"}},"outputs":[{"name":"stdout","text":"âŒ '/kaggle/working' is not empty â€” contains 11 items.\n/kaggle/working\nconfig.py  evaluation\tmodels\t __pycache__\t  state.db\ndata\t   __init__.py\tprompts  run_pipeline.py  utils\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ls -R /kaggle/working/models/MODEL_TYPE}/results/\n!ls -R /kaggle/working/models/{MODEL_TYPE}/results/{MODEL_NAME}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:46:17.214631Z","iopub.execute_input":"2025-07-15T10:46:17.214970Z","iopub.status.idle":"2025-07-15T10:46:17.464357Z","shell.execute_reply.started":"2025-07-15T10:46:17.214938Z","shell.execute_reply":"2025-07-15T10:46:17.463493Z"}},"outputs":[{"name":"stdout","text":"ls: cannot access '/kaggle/working/models/MODEL_TYPE}/results/': No such file or directory\n/kaggle/working/models/fine_tuned_model/results/google_mt5-small:\ngeneration_log_ERROR.json  generation_log.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers==4.41.2 datasets==2.19.1 scikit-learn wandb python-dotenv evaluate --quiet\n!pip install -U peft==0.11.1 --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:46:17.465403Z","iopub.execute_input":"2025-07-15T10:46:17.465718Z","iopub.status.idle":"2025-07-15T10:48:22.551871Z","shell.execute_reply.started":"2025-07-15T10:46:17.465684Z","shell.execute_reply":"2025-07-15T10:48:22.551040Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m:01\u001b[0mmm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import sys\nimport os\nimport torch\n\nproject_root = os.getcwd()  # Should be /content/drive/MyDrive/AraGenEval2025\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\nprint(f\"Project root added to sys.path: {project_root}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nfrom config import Config\n\nprint(Config.VAL_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:22.554336Z","iopub.execute_input":"2025-07-15T10:48:22.554550Z","iopub.status.idle":"2025-07-15T10:48:22.636196Z","shell.execute_reply.started":"2025-07-15T10:48:22.554529Z","shell.execute_reply":"2025-07-15T10:48:22.635466Z"}},"outputs":[{"name":"stdout","text":"Project root added to sys.path: /kaggle/working\nUsing device: cuda\ndata/AuthorshipStyleTransferVal.xlsx\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Mapping","metadata":{}},{"cell_type":"code","source":"# â”€â”€ cell: 3 â”€â”€\n# ğŸ—º Author Mappings\nauthor2id = {\n    \"Ø£Ø­Ù…Ø¯ Ø£Ù…ÙŠÙ†\": 0,  \"Ø£Ø­Ù…Ø¯ ØªÙŠÙ…ÙˆØ± Ø¨Ø§Ø´Ø§\": 1,  \"Ø£Ø­Ù…Ø¯ Ø´ÙˆÙ‚ÙŠ\": 2,\n    \"Ø£Ù…ÙŠÙ† Ø§Ù„Ø±ÙŠØ­Ø§Ù†ÙŠ\": 3,  \"Ø«Ø±ÙˆØª Ø£Ø¨Ø§Ø¸Ø©\": 4,  \"Ø¬Ø¨Ø±Ø§Ù† Ø®Ù„ÙŠÙ„ Ø¬Ø¨Ø±Ø§Ù†\": 5,\n    \"Ø¬ÙØ±Ø¬ÙŠ Ø²ÙŠØ¯Ø§Ù†\": 6,  \"Ø­Ø³Ù† Ø­Ù†ÙÙŠ\": 7,  \"Ø±ÙˆØ¨Ø±Øª Ø¨Ø§Ø±\": 8,\n    \"Ø³Ù„Ø§Ù…Ø© Ù…ÙˆØ³Ù‰\": 9,  \"Ø·Ù‡ Ø­Ø³ÙŠÙ†\": 10, \"Ø¹Ø¨Ø§Ø³ Ù…Ø­Ù…ÙˆØ¯ Ø§Ù„Ø¹Ù‚Ø§Ø¯\": 11,\n    \"Ø¹Ø¨Ø¯ Ø§Ù„ØºÙØ§Ø± Ù…ÙƒØ§ÙˆÙŠ\": 12, \"ØºÙˆØ³ØªØ§Ù Ù„ÙˆØ¨ÙˆÙ†\": 13,  \"ÙØ¤Ø§Ø¯ Ø²ÙƒØ±ÙŠØ§\": 14,\n    \"ÙƒØ§Ù…Ù„ ÙƒÙŠÙ„Ø§Ù†ÙŠ\": 15,  \"Ù…Ø­Ù…Ø¯ Ø­Ø³ÙŠÙ† Ù‡ÙŠÙƒÙ„\": 16,  \"Ù†Ø¬ÙŠØ¨ Ù…Ø­ÙÙˆØ¸\": 17,\n    \"Ù†ÙˆØ§Ù„ Ø§Ù„Ø³Ø¹Ø¯Ø§ÙˆÙŠ\": 18, \"ÙˆÙŠÙ„ÙŠØ§Ù… Ø´ÙŠÙƒØ³Ø¨ÙŠØ±\": 19, \"ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³\": 20\n}\nid2author   = {v: k for k, v in author2id.items()}\nid2english  = {\n    0:\"Ahmed_Amin\",    1:\"Ahmad_Taymour_Basha\",   2:\"Ahmed_Shawqi\",\n    3:\"Ameen_Rihani\",  4:\"Tharwat_Abaza\",         5:\"Gibran_Khalil_Gibran\",\n    6:\"Jurji_Zaydan\",  7:\"Hassan_Hanifi\",         8:\"Robert_Barr\",\n    9:\"Salama_Moussa\",10:\"Taha_Hussein\",         11:\"Abbas_Al-Aqqad\",\n   12:\"AbdelGhaffar_Makawi\",13:\"Gustave_Lebon\",    14:\"Fouad_Zakaria\",\n   15:\"Kamel_Kilani\",16:\"Mohamed_Hosseini_Hekal\",17:\"Naguib_Mahfouz\",\n   18:\"Nawal_El_Saadawi\",19:\"William_Shakespeare\",20:\"Youssef_Edrees\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:22.637058Z","iopub.execute_input":"2025-07-15T10:48:22.637330Z","iopub.status.idle":"2025-07-15T10:48:22.643965Z","shell.execute_reply.started":"2025-07-15T10:48:22.637305Z","shell.execute_reply":"2025-07-15T10:48:22.643248Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport gc\n\ndef load_and_sample_dataset(\n    file_path,\n    sample_mode=\"all\",    # Options: \"all\", \"random\", \"stratified\"\n    sample_size=None,     # For \"random\"/\"stratified\": float (proportion) or int (absolute count)\n    random_state=42\n):\n    \"\"\"\n    Load dataset from Excel and apply sampling.\n\n    Args:\n        file_path (str): Path to dataset file.\n        sample_mode (str): \"all\", \"random\", or \"stratified\".\n        sample_size (float|int): Proportion (0-1) or count for sampling.\n        random_state (int): Random seed for reproducibility.\n\n    Returns:\n        pd.DataFrame: Sampled dataset.\n    \"\"\"\n    print(f\"ğŸ“‚ Loading dataset from: {file_path}\")\n    df = pd.read_excel(file_path, engine='openpyxl')\n\n    if sample_mode == \"all\":\n        sampled_df = df.copy()\n        print(f\"âœ… Loaded full dataset with {len(sampled_df)} samples.\")\n\n    elif sample_mode == \"random\":\n        if sample_size is None:\n            raise ValueError(\"For 'random' mode, SAMPLE_SIZE must be set.\")\n        sampled_df = df.sample(\n            n=sample_size if isinstance(sample_size, int) else int(len(df) * sample_size),\n            random_state=random_state\n        )\n        print(f\"âœ… Randomly sampled {len(sampled_df)} samples.\")\n\n    elif sample_mode == \"stratified\":\n        if sample_size is None or not (0 < sample_size < 1):\n            raise ValueError(\"For 'stratified' mode, SAMPLE_SIZE must be a proportion between 0 and 1.\")\n        sampled_df, _ = train_test_split(\n            df,\n            train_size=sample_size,\n            stratify=df['author'],\n            random_state=random_state\n        )\n        print(f\"âœ… Stratified sampled {len(sampled_df)} samples (author distribution preserved).\")\n\n    else:\n        raise ValueError(f\"Invalid sample_mode: {sample_mode}\")\n\n    del df\n    gc.collect()\n\n    return sampled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:22.644763Z","iopub.execute_input":"2025-07-15T10:48:22.645044Z","iopub.status.idle":"2025-07-15T10:48:23.973318Z","shell.execute_reply.started":"2025-07-15T10:48:22.645021Z","shell.execute_reply":"2025-07-15T10:48:23.972740Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# === CONFIG ===\nSAMPLE_MODE = \"stratified\"  # \"all\", \"random\", \"stratified\"\nSAMPLE_SIZE = 0.1           # 10% for stratified or random. Set to None for full dataset.\n\n# === LOAD AND SAMPLE ===\nval_df = load_and_sample_dataset(\n    Config.VAL_FILE,\n    # sample_mode=SAMPLE_MODE,\n    # sample_size=SAMPLE_SIZE\n)\n\n# Expect columns: id, text_in_msa\nval_df = val_df.rename(columns={\"id\":\"sample_id\",\"text_in_msa\":\"neutral_text\"})\nprint(f\"âœ… Loaded {len(val_df)} neutral texts.\")\n\nprint(\"Validation shape:\", val_df.shape)\nprint(\"Authors in Val:\", val_df['author'].nunique())\n\n# train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:23.974080Z","iopub.execute_input":"2025-07-15T10:48:23.974473Z","iopub.status.idle":"2025-07-15T10:48:25.255161Z","shell.execute_reply.started":"2025-07-15T10:48:23.974448Z","shell.execute_reply":"2025-07-15T10:48:25.254609Z"}},"outputs":[{"name":"stdout","text":"ğŸ“‚ Loading dataset from: data/AuthorshipStyleTransferVal.xlsx\nâœ… Loaded full dataset with 4157 samples.\nâœ… Loaded 4157 neutral texts.\nValidation shape: (4157, 4)\nAuthors in Val: 21\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Sliding Window","metadata":{}},{"cell_type":"code","source":"# â”€â”€ cell: 6 â”€â”€\n# ğŸª“ Sliding Window Chunking Utility\nfrom transformers import AutoTokenizer\nimport torch\n\ndef get_chunks(text, tokenizer, max_len=512, stride=256):\n    return tokenizer(\n        text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_len,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_tensors=\"pt\"\n    )\n\ndef device():\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:25.255881Z","iopub.execute_input":"2025-07-15T10:48:25.256163Z","iopub.status.idle":"2025-07-15T10:48:25.980197Z","shell.execute_reply.started":"2025-07-15T10:48:25.256146Z","shell.execute_reply":"2025-07-15T10:48:25.979598Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Load Model ","metadata":{}},{"cell_type":"code","source":"# â”€â”€ cell: 7 â”€â”€\n# ğŸ”§ Load Classifier Function\nfrom transformers import AutoModelForSequenceClassification\n\ndef load_binary_classifier(author_id):\n    path = f\"evaluation/ar_style_classifier/author_SC_{author_id}\"\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.to(device())\n    model.eval()\n    return tokenizer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:25.980910Z","iopub.execute_input":"2025-07-15T10:48:25.981096Z","iopub.status.idle":"2025-07-15T10:48:25.995318Z","shell.execute_reply.started":"2025-07-15T10:48:25.981081Z","shell.execute_reply":"2025-07-15T10:48:25.994793Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Preparations","metadata":{}},{"cell_type":"code","source":"# â”€â”€ cell: 5 â”€â”€\n# ğŸ“ Load Generation Log\nimport json\nwith open(f\"models/{MODEL_TYPE}/results/{MODEL_NAME}/generation_log.json\",\"r\", encoding=\"utf-8\") as f:\n    gen = json.load(f)\n# gen: list of {id, author, output, ground_truth}\nlog_df = pd.DataFrame(gen)\nlog_df = log_df.rename(columns={\"id\":\"sample_id\",\"output\":\"generated_text\",\"author\":\"true_author\"})\nprint(f\"âœ… Loaded {len(log_df)} generated entries.\")\n\n# Merge neutral and generated\ndf = pd.merge(log_df, val_df, on=\"sample_id\", how=\"left\")\ndf[\"true_id\"] = df[\"true_author\"].map(author2id)\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:25.997481Z","iopub.execute_input":"2025-07-15T10:48:25.997698Z","iopub.status.idle":"2025-07-15T10:48:26.229176Z","shell.execute_reply.started":"2025-07-15T10:48:25.997682Z","shell.execute_reply":"2025-07-15T10:48:26.228350Z"}},"outputs":[{"name":"stdout","text":"âœ… Loaded 4157 generated entries.\n   sample_id true_author                                     generated_text  \\\n0       3835  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³  ÙˆÙ…Ù†Ø° Ø£Ù† ÙƒÙ†Øª Ø·ÙÙ„Ù‹Ø§ ÙÙŠ Ø§Ù„Ø®Ù…Ø³ÙŠÙ† Ù…Ù† Ø¹Ù…Ø±ÙŠ Ù„Ù… Ø£Ù‚Ù… Ù…Ø·...   \n1       3836  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³  ÙˆØ°Ù„Ùƒ Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ø¹Ø§Ù… Ù‡Ùˆ Ø§Ù„Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØ³Ø¬Ù„ Ø§...   \n2       3837  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³  ÙˆÙ…ØµØ± Ø§Ù„ØºÙ†ÙŠØ© Ø§Ù„Ø±Ø§Ù‚ÙŠØ© ÙˆØ§Ù„Ù…ØµÙ†Ù‘ÙØ¹Ø© ÙˆØ§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ ...   \n3       3838  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³  ÙˆÙ†Ø¸Ø±Ù‹Ø§ Ù„ØºØ±Ø§Ø¨Ø© ØªØ¬Ø±Ø¨ØªÙŠØŒ ÙˆÙ„Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„ØªÙŠ Ø±Ø§ÙˆØ¯ØªÙ†ÙŠ Ø­Ùˆ...   \n4       3839  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³  ÙˆÙ„ÙƒÙ† Ù…Ø§ Ø°ÙƒØ±Ù‡ Ù„ÙŠØ³ Ù…Ø±Ø§Ø±Ø©Ù‹ ÙˆÙ„Ø§ Ù†Ø¯Ù…Ù‹Ø§Ø› ÙÙ‚Ø¯ ÙƒØ§Ù† Ù…Ø§ ...   \n\n                                        ground_truth  \\\n0  Ù…Ù† Ø·ÙÙ„ ÙÙŠ Ø§Ù„Ø®Ù…Ø³ÙŠÙ†\\n\\nØ¹Ù…Ø±ÙŠ Ù…Ø§ Ø§Ø­ØªÙÙ„Øª Ø£Ùˆ Ø­ÙÙ„Øª Ø¨Ø¹...   \n1  Ø°Ù„Ùƒ Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ø¹Ø§Ù… Ù‡Ùˆ Ø§Ù„Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø°ÙŠ\\n\\nØ¯Ø§Ù… ÙŠØ¹...   \n2  Ù…ØµØ± Ø§Ù„ØºÙ†ÙŠØ© Ø§Ù„Ù…Ø«Ù‚ÙØ© Ø§Ù„Ù…ØµÙ†Ù‘ÙØ¹Ø©ØŒ ÙˆØ§Ù„Ø¹Ø±Ø¨\\n\\nÙˆÙ‚Ø¯\\n\\...   \n3  ÙˆÙ„Ø£Ù†Ù‡Ø§ ØºØ±ÙŠØ¨Ø© ÙˆØ±Ø§ÙˆØ¯ØªÙ†ÙŠ ÙÙŠÙ‡Ø§ Ø¹Ù† Ø§Ù„Ù†Ø§Ø³ ÙˆØ¹Ù† Ø§Ù„Ø­ÙŠØ§Ø©...   \n4  ÙˆÙ„ÙŠØ³ Ù…Ø§ Ø°ÙƒØ±ØªÙ‡ Ù…Ø±Ø§Ø±Ø© ÙˆÙ„Ø§ Ù†Ø¯Ù…Ù‹Ø§Ø› ÙÙ‚Ø¯ ÙƒØ§Ù† Ù„Ø§ ÙŠÙ…ÙƒÙ†...   \n\n                                        neutral_text  \\\n0  Ù…Ù†Ø° Ø£Ù† ÙƒÙ†Øª Ø·ÙÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ø®Ù…Ø³ÙŠÙ† Ù…Ù† Ø¹Ù…Ø±ÙŠØŒ Ù„Ù… Ø£Ù‚Ù… Ù…Ø·...   \n1  Ø°Ù„Ùƒ Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ø¹Ø§Ù… ÙŠÙØ¹ØªØ¨Ø± Ø¨Ù…Ø«Ø§Ø¨Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠ Ø§...   \n2  Ù…ØµØ±ØŒ Ø§Ù„ØºÙ†ÙŠØ©ØŒ Ø§Ù„Ø±Ø§Ù‚ÙŠØ©ØŒ ÙˆØ§Ù„Ù…ØµÙ†Ù‘ÙØ¹Ø©ØŒ ÙˆØ§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±...   \n3  Ù†Ø¸Ø±Ù‹Ø§ Ù„ØºØ±Ø§Ø¨Ø© ØªØ¬Ø±Ø¨ØªÙŠØŒ ÙˆÙ„Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„ØªÙŠ Ø±Ø§ÙˆØ¯ØªÙ†ÙŠ Ø­ÙˆÙ„...   \n4  Ø¥Ù† Ù…Ø§ ØªÙ… Ø°ÙƒØ±Ù‡ Ù„ÙŠØ³ Ù…Ø±Ø§Ø±Ø©Ù‹ ÙˆÙ„Ø§ Ù†Ø¯Ù…Ù‹Ø§Ø› ÙÙ‚Ø¯ ÙƒØ§Ù† Ù…Ø§...   \n\n                                text_in_author_style      author  true_id  \n0  Ù…Ù† Ø·ÙÙ„ ÙÙŠ Ø§Ù„Ø®Ù…Ø³ÙŠÙ†\\n\\nØ¹Ù…Ø±ÙŠ Ù…Ø§ Ø§Ø­ØªÙÙ„Øª Ø£Ùˆ Ø­ÙÙ„Øª Ø¨Ø¹...  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³       20  \n1  Ø°Ù„Ùƒ Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ø¹Ø§Ù… Ù‡Ùˆ Ø§Ù„Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø°ÙŠ\\n\\nØ¯Ø§Ù… ÙŠØ¹...  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³       20  \n2  Ù…ØµØ± Ø§Ù„ØºÙ†ÙŠØ© Ø§Ù„Ù…Ø«Ù‚ÙØ© Ø§Ù„Ù…ØµÙ†Ù‘ÙØ¹Ø©ØŒ ÙˆØ§Ù„Ø¹Ø±Ø¨\\n\\nÙˆÙ‚Ø¯\\n\\...  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³       20  \n3  ÙˆÙ„Ø£Ù†Ù‡Ø§ ØºØ±ÙŠØ¨Ø© ÙˆØ±Ø§ÙˆØ¯ØªÙ†ÙŠ ÙÙŠÙ‡Ø§ Ø¹Ù† Ø§Ù„Ù†Ø§Ø³ ÙˆØ¹Ù† Ø§Ù„Ø­ÙŠØ§Ø©...  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³       20  \n4  ÙˆÙ„ÙŠØ³ Ù…Ø§ Ø°ÙƒØ±ØªÙ‡ Ù…Ø±Ø§Ø±Ø© ÙˆÙ„Ø§ Ù†Ø¯Ù…Ù‹Ø§Ø› ÙÙ‚Ø¯ ÙƒØ§Ù† Ù„Ø§ ÙŠÙ…ÙƒÙ†...  ÙŠÙˆØ³Ù Ø¥Ø¯Ø±ÙŠØ³       20  \n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# â”€â”€ cell: 8 â”€â”€\n# ğŸ” Probability Prediction (in-style)\nimport numpy as np\n\n# def predict_instyle_prob(text, tokenizer, model):\n#     enc = get_chunks(text, tokenizer)\n#     enc = {k: v.to(device()) for k,v in enc.items()}\n#     with torch.no_grad():\n#         logits = model(**enc).logits  # [chunks, 2]\n#         probs = torch.softmax(logits, dim=-1)  # same shape\n#     instyle_probs = probs[:,1]  # author style class\n#     return instyle_probs.mean().item()\n\n\nVALID_KEYS = {\"input_ids\", \"attention_mask\", \"token_type_ids\"}\n\ndef predict_instyle_prob(text, tokenizer, model):\n    enc = get_chunks(text, tokenizer)\n    enc = {k: v.to(device()) for k,v in enc.items() if k in VALID_KEYS}\n    with torch.no_grad():\n        logits = model(**enc).logits  # Now works\n        probs = torch.softmax(logits, dim=-1)\n    return probs[:,1].mean().item() # author style class\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:26.229893Z","iopub.execute_input":"2025-07-15T10:48:26.230106Z","iopub.status.idle":"2025-07-15T10:48:26.235354Z","shell.execute_reply.started":"2025-07-15T10:48:26.230089Z","shell.execute_reply":"2025-07-15T10:48:26.234587Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# # â”€â”€ cell: 9 â”€â”€\n# # ğŸ“Š Style Evaluation Loop\n# from tqdm import tqdm\n# results = []\n# for author_id in range(21):\n#     tokenizer, model = load_binary_classifier(author_id)\n#     for row in tqdm(df.itertuples(), desc=f\"Author {author_id}\"):\n#         px = predict_instyle_prob(row.neutral_text, tokenizer, model)\n#         py = predict_instyle_prob(row.generated_text, tokenizer, model)\n#         diff = py - px\n#         success = diff > 0\n#         results.append({\n#             \"sample_id\": row.sample_id,\n#             \"true_id\": row.true_id,\n#             \"classifier_id\": author_id,\n#             \"neutral_prob\": px,\n#             \"generated_prob\": py,\n#             \"diff\": diff,\n#             \"success\": int(success)\n#         })\n# results_df = pd.DataFrame(results)\n\n\n\n# â”€â”€ cell: 9 â”€â”€\n# ğŸ“Š Style Evaluation Loop (perâ€‘author filtering)\nfrom tqdm import tqdm\n\nresults = []\n\nfor author_id in range(21):\n    # 1) select only the samples meant for this author\n    author_df = df[df[\"true_id\"] == author_id]\n\n    # 2) load that authorâ€™s classifier\n    tokenizer, model = load_binary_classifier(author_id)\n\n    # 3) run only their own samples\n    for row in tqdm(author_df.itertuples(), desc=f\"Author {author_id}\"):\n        px   = predict_instyle_prob(row.neutral_text,   tokenizer, model)\n        py   = predict_instyle_prob(row.generated_text, tokenizer, model)\n        diff = py - px\n        success = int(diff > 0)\n\n        results.append({\n            \"sample_id\":      row.sample_id,\n            \"true_id\":        row.true_id,\n            \"classifier_id\":  author_id,\n            \"neutral_prob\":   px,\n            \"generated_prob\": py,\n            \"diff\":           diff,\n            \"success\":        success\n        })\n\n# build DataFrame after the loop\nresults_df = pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:48:26.236080Z","iopub.execute_input":"2025-07-15T10:48:26.236315Z","iopub.status.idle":"2025-07-15T10:52:11.086204Z","shell.execute_reply.started":"2025-07-15T10:48:26.236299Z","shell.execute_reply":"2025-07-15T10:52:11.085329Z"}},"outputs":[{"name":"stderr","text":"Author 0: 246it [00:12, 20.03it/s]\nAuthor 1: 57it [00:03, 16.93it/s]\nAuthor 2: 58it [00:03, 19.18it/s]\nAuthor 3: 142it [00:07, 19.11it/s]\nAuthor 4: 90it [00:05, 17.54it/s]\nAuthor 5: 30it [00:01, 17.91it/s]\nAuthor 6: 327it [00:14, 22.23it/s]\nAuthor 7: 548it [00:26, 20.77it/s]\nAuthor 8: 82it [00:03, 24.66it/s]\nAuthor 9: 119it [00:05, 21.30it/s]\nAuthor 10: 255it [00:11, 22.23it/s]\nAuthor 11: 267it [00:11, 22.88it/s]\nAuthor 12: 396it [00:18, 21.13it/s]\nAuthor 13: 150it [00:06, 23.54it/s]\nAuthor 14: 125it [00:05, 24.46it/s]\nAuthor 15: 25it [00:01, 21.31it/s]\nAuthor 16: 260it [00:11, 22.65it/s]\nAuthor 17: 327it [00:16, 19.63it/s]\nAuthor 18: 295it [00:12, 22.81it/s]\nAuthor 19: 238it [00:11, 21.39it/s]\nAuthor 20: 120it [00:05, 22.87it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Save results","metadata":{}},{"cell_type":"code","source":"save_dir = f\"evaluation/ar_style_classifier/results/{MODEL_NAME}\"\n!mkdir -p {save_dir}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.087106Z","iopub.execute_input":"2025-07-15T10:52:11.087865Z","iopub.status.idle":"2025-07-15T10:52:11.269511Z","shell.execute_reply.started":"2025-07-15T10:52:11.087839Z","shell.execute_reply":"2025-07-15T10:52:11.268785Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# â”€â”€ cell: 10 â”€â”€\n# ğŸ’¾ Save Results and Metrics\nresults_df.to_csv(\n    f\"evaluation/ar_style_classifier/results/{MODEL_NAME}/ar_bsc_evaluation_results_.csv\",\n    index=False\n)\n\n# ğŸ§¾ Aggregate metrics per classifier\nmetrics = results_df.groupby(\"classifier_id\").agg(\n    accuracy=(\"success\", \"mean\"),\n    avg_diff=(\"diff\", \"mean\"),\n    count=(\"success\", \"size\")\n).reset_index()\n\n# ğŸŒ Compute overall metrics across all classifiers and samples\noverall_accuracy = results_df[\"success\"].mean()\noverall_avg_diff = results_df[\"diff\"].mean()\n\n# Create an overall-metrics DataFrame row\noverall_row = pd.DataFrame([{\n    \"classifier_id\": -1,\n    \"accuracy\": overall_accuracy,\n    \"avg_diff\": overall_avg_diff,\n    \"count\": len(results_df)\n}])\n\n# Concatenate to include overall stats\nmetrics = pd.concat([metrics, overall_row], ignore_index=True)\n\n# ğŸ–¨ Print overall metrics\nprint(f\"Overall Style-Transfer Accuracy: {overall_accuracy:.4f}\")\nprint(f\"Overall Average Diff (strength): {overall_avg_diff:.4f}\")\n\n# ğŸ’¾ Save both per-author and overall metrics\nmetrics.to_csv(\n    f\"evaluation/ar_style_classifier/results/{MODEL_NAME}/ar_bsc_metrics_per_author_and_overall.csv\",\n    index=False\n)\n\nprint(\"âœ… Saved evaluation and per-author metrics CSVs (including overall metric).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.270632Z","iopub.execute_input":"2025-07-15T10:52:11.271026Z","iopub.status.idle":"2025-07-15T10:52:11.334720Z","shell.execute_reply.started":"2025-07-15T10:52:11.271000Z","shell.execute_reply":"2025-07-15T10:52:11.334122Z"}},"outputs":[{"name":"stdout","text":"Overall Style-Transfer Accuracy: 0.5978\nOverall Average Diff (strength): 0.0433\nâœ… Saved evaluation and per-author metrics CSVs (including overall metric).\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# â”€â”€ cell: 11 â”€â”€\n# ğŸ“ˆ Per-Sample Summary: Identify strongest style match and overall success rate\n# For each sample, find the classifier with highest generated_prob\ntop_matches = results_df.loc[results_df.groupby('sample_id')['generated_prob'].idxmax()]\n# top_matches = top_matches.merge(id2author.to_frame('classifier_id').reset_index(), on='classifier_id')\nid2author_df = pd.DataFrame(list(id2author.items()), columns=['classifier_id', 'predicted_author'])\ntop_matches = top_matches.merge(id2author_df, on='classifier_id')\ntop_matches = top_matches.rename(columns={\n    'classifier_id': 'predicted_id',\n    'index': 'predicted_author'\n})\n# Calculate overall success: did the true author classifier succeed?\ntrue_success = results_df[\n    (results_df['classifier_id'] == results_df['true_id'])\n].groupby('sample_id')['success'].first().reset_index()\n\n# Merge for overview\noverview = top_matches.merge(true_success, on='sample_id')\n\noverview.to_csv(f\"evaluation/ar_style_classifier/results/{MODEL_NAME}/ar_bsc_evaluation_per_sample_overview.csv\", index=False)\nprint(\"âœ… Saved per-sample overview CSV: strongest match and true-author success.\")\n\n# ğŸ’¡ Note: Probabilities are averaged across all chunks of a sample, so each sample has a single neutral_prob and generated_prob per classifier.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.335498Z","iopub.execute_input":"2025-07-15T10:52:11.336006Z","iopub.status.idle":"2025-07-15T10:52:11.386235Z","shell.execute_reply.started":"2025-07-15T10:52:11.335985Z","shell.execute_reply":"2025-07-15T10:52:11.385691Z"}},"outputs":[{"name":"stdout","text":"âœ… Saved per-sample overview CSV: strongest match and true-author success.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# â”€â”€ cell: 12 â”€â”€\n# ğŸ“¦ Package Results Archive\nimport os, subprocess\n\nzip_name = f\"ar_style_classifier_results_{MODEL_NAME}.zip\"\nresults_dir = os.path.join(\"evaluation\", \"ar_style_classifier\", \"results\", MODEL_NAME)\n\n# Run compression\n!cd evaluation/ar_style_classifier && zip -r {zip_name} results/{MODEL_NAME}\n\nprint(f\"âœ… Created archive: evaluation/ar_style_classifier/{zip_name}\")\n\n# !cd evaluation/ar_style_classifier && zip -r ar_style_classifier_results_{MODEL_NAME}.zip results/{MODEL_NAME}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.387021Z","iopub.execute_input":"2025-07-15T10:52:11.387275Z","iopub.status.idle":"2025-07-15T10:52:11.619706Z","shell.execute_reply.started":"2025-07-15T10:52:11.387252Z","shell.execute_reply":"2025-07-15T10:52:11.618886Z"}},"outputs":[{"name":"stdout","text":"updating: results/google_mt5-small/ (stored 0%)\nupdating: results/google_mt5-small/ar_bsc_evaluation_results_.csv (deflated 59%)\nupdating: results/google_mt5-small/ar_bsc_evaluation_per_sample_overview.csv (deflated 67%)\nupdating: results/google_mt5-small/ar_bsc_metrics_per_author_and_overall.csv (deflated 48%)\nâœ… Created archive: evaluation/ar_style_classifier/ar_style_classifier_results_google_mt5-small.zip\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(f\"evaluation/ar_style_classifier/{zip_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.620751Z","iopub.execute_input":"2025-07-15T10:52:11.620992Z","iopub.status.idle":"2025-07-15T10:52:11.627674Z","shell.execute_reply.started":"2025-07-15T10:52:11.620969Z","shell.execute_reply":"2025-07-15T10:52:11.627088Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/evaluation/ar_style_classifier/ar_style_classifier_results_google_mt5-small.zip","text/html":"<a href='evaluation/ar_style_classifier/ar_style_classifier_results_google_mt5-small.zip' target='_blank'>evaluation/ar_style_classifier/ar_style_classifier_results_google_mt5-small.zip</a><br>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# # --- Inference on full validation texts ---\n# def predict_full_text(text):\n#     enc = tokenizer(\n#         text,\n#         truncation=True,\n#         padding=\"max_length\",\n#         max_length=512,\n#         stride=256,\n#         return_overflowing_tokens=True,\n#         return_tensors=\"pt\"\n#     ).to(device)\n#     logits = model(**enc).logits  # [num_chunks, num_labels]\n#     avg_logits = logits.mean(dim=0)\n#     probs = torch.softmax(avg_logits, dim=0)\n#     return torch.argmax(probs).item(), probs.cpu().tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.628311Z","iopub.execute_input":"2025-07-15T10:52:11.628489Z","iopub.status.idle":"2025-07-15T10:52:11.641751Z","shell.execute_reply.started":"2025-07-15T10:52:11.628474Z","shell.execute_reply":"2025-07-15T10:52:11.641052Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# # --- Evaluate across validation set ---\n# preds, refs = [], []\n# for example in val_df.itertuples():\n#     pred, _ = predict_full_text(example.text_in_author_style)\n#     preds.append(pred)\n#     refs.append(author2id[example.author])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.642470Z","iopub.execute_input":"2025-07-15T10:52:11.642675Z","iopub.status.idle":"2025-07-15T10:52:11.659762Z","shell.execute_reply.started":"2025-07-15T10:52:11.642660Z","shell.execute_reply":"2025-07-15T10:52:11.658922Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# from sklearn.metrics import accuracy_score, f1_score\n# print(\"Validation Accuracy:\", accuracy_score(refs, preds))\n# print(\"Validation F1â€‘macro:\", f1_score(refs, preds, average=\"macro\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:52:11.660800Z","iopub.execute_input":"2025-07-15T10:52:11.661075Z","iopub.status.idle":"2025-07-15T10:52:11.673472Z","shell.execute_reply.started":"2025-07-15T10:52:11.661053Z","shell.execute_reply":"2025-07-15T10:52:11.672759Z"}},"outputs":[],"execution_count":20}]}