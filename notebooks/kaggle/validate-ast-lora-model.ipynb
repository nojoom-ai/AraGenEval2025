{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12342718,"sourceType":"datasetVersion","datasetId":7780982},{"sourceId":12454188,"sourceType":"datasetVersion","datasetId":7856142},{"sourceId":12502104,"sourceType":"datasetVersion","datasetId":7890475},{"sourceId":12528598,"sourceType":"datasetVersion","datasetId":7908780}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":128.955805,"end_time":"2025-07-14T06:47:18.107738","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-14T06:45:09.151933","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"fTyhOO2v7ssz","outputId":"7eecf25f-5840-4ed0-c27a-fa479b3c0838","papermill":{"duration":0.038757,"end_time":"2025-07-14T06:46:43.831489","exception":false,"start_time":"2025-07-14T06:46:43.792732","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:23:51.648397Z","iopub.execute_input":"2025-07-20T23:23:51.648659Z","iopub.status.idle":"2025-07-20T23:23:51.652365Z","shell.execute_reply.started":"2025-07-20T23:23:51.648631Z","shell.execute_reply":"2025-07-20T23:23:51.651670Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# project_dir = \"/content/drive/MyDrive/AraGenEval2025\"\n# os.chdir(project_dir)\n# print(f\"✅ Current directory: {os.getcwd()}\")\n","metadata":{"id":"cab9aba0","papermill":{"duration":0.041911,"end_time":"2025-07-14T06:47:15.232757","exception":false,"start_time":"2025-07-14T06:47:15.190846","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:23:51.654176Z","iopub.execute_input":"2025-07-20T23:23:51.654750Z","iopub.status.idle":"2025-07-20T23:23:51.673154Z","shell.execute_reply.started":"2025-07-20T23:23:51.654723Z","shell.execute_reply":"2025-07-20T23:23:51.672470Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\n\npath = \"/kaggle/working\"\nis_empty = (len(os.listdir(path)) < 4)\n\n!ls /kaggle/working/\n\nMODEL_WEIGHTS_SRC =\"/kaggle/input/ubc-nlp-rat5v2-base-1024-model/UBC-NLP_AraT5v2-base-1024/final\"+\"/*\"\n\nlora_injection_src= \"/kaggle/input/ubc-ara-t5-v2-lora-ast-model/UBC-NLP_AraT5v2-base-1024_with_lora/UBC-NLP_AraT5v2-base-1024_with_lora/final\"+\"/*\"\n\nMODEL_NAME = \"UBC-NLP_AraT5v2-base-1024\"\n# MODEL_NAME = \"UBC-NLP/AraT5v2-base-1024\" #\"agemagician_mlong-t5-tglobal-large\" #\"facebook_mbart-large-50-many-to-many-mmt\" # \"UBC-NLP_AraT5-base\" #\"google/mt5-small\"\n\nMODEL_WEIGHTS_DEST_DIR =f\"/kaggle/working/models/fine_tuned_model/model_weights/{MODEL_NAME}\"\n\nbase_model_path = MODEL_WEIGHTS_DEST_DIR\nlora_path       = f\"./{MODEL_NAME}_with_lora\"\n\n# !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n\nif is_empty:\n    print(f\"✅ '{path}' is empty. copying project strcuture into it ...\")\n    !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n    !mkdir -p {MODEL_WEIGHTS_DEST_DIR}\n    !cp -r {MODEL_WEIGHTS_SRC} {MODEL_WEIGHTS_DEST_DIR}\n    !mkdir -p {lora_path}\n    !cp -r {lora_injection_src} {lora_path}\nelse:\n    print(f\"❌ '{path}' is not empty — contains {len(os.listdir(path))} items.\")\n\n# !mkdir -p {MODEL_WEIGHTS_DEST_DIR}\n# !cp -r {MODEL_WEIGHTS_SRC} {MODEL_WEIGHTS_DEST_DIR}\n\n# Change to project directory\n%cd /kaggle/working/ \n\n# Confirm\n!ls","metadata":{"papermill":{"duration":19.447473,"end_time":"2025-07-14T06:47:03.311976","exception":false,"start_time":"2025-07-14T06:46:43.864503","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:23:51.673850Z","iopub.execute_input":"2025-07-20T23:23:51.674448Z","iopub.status.idle":"2025-07-20T23:24:03.567094Z","shell.execute_reply.started":"2025-07-20T23:23:51.674408Z","shell.execute_reply":"2025-07-20T23:24:03.566342Z"}},"outputs":[{"name":"stdout","text":"✅ '/kaggle/working' is empty. copying project strcuture into it ...\n/kaggle/working\nconfig.py  evaluation\tmodels\t run_pipeline.py\t\t      utils\ndata\t   __init__.py\tprompts  UBC-NLP_AraT5v2-base-1024_with_lora\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!ls -R {MODEL_WEIGHTS_DEST_DIR}","metadata":{"papermill":{"duration":0.154964,"end_time":"2025-07-14T06:47:03.499832","exception":false,"start_time":"2025-07-14T06:47:03.344868","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:24:03.568010Z","iopub.execute_input":"2025-07-20T23:24:03.568212Z","iopub.status.idle":"2025-07-20T23:24:03.686310Z","shell.execute_reply.started":"2025-07-20T23:24:03.568190Z","shell.execute_reply":"2025-07-20T23:24:03.685410Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/models/fine_tuned_model/model_weights/UBC-NLP_AraT5v2-base-1024:\nadded_tokens.json\tmodel.safetensors\t tokenizer_config.json\nconfig.json\t\tspecial_tokens_map.json  training_args.bin\ngeneration_config.json\tspiece.model\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!ls -R {lora_path}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:24:03.687449Z","iopub.execute_input":"2025-07-20T23:24:03.688168Z","iopub.status.idle":"2025-07-20T23:24:05.861131Z","shell.execute_reply.started":"2025-07-20T23:24:03.688141Z","shell.execute_reply":"2025-07-20T23:24:05.860471Z"}},"outputs":[{"name":"stdout","text":"./UBC-NLP_AraT5v2-base-1024_with_lora:\nadapter_config.json\t   README.md\t\t    tokenizer_config.json\nadapter_model.safetensors  special_tokens_map.json\nadded_tokens.json\t   spiece.model\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install transformers[sentencepiece] torch pandas openpyxl psutil python-dotenv peft  --quiet","metadata":{"id":"559ddd13","outputId":"7921c04f-9a42-44dc-e548-5c1805145485","papermill":{"duration":90.123476,"end_time":"2025-07-14T06:46:43.758212","exception":false,"start_time":"2025-07-14T06:45:13.634736","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:24:05.862041Z","iopub.execute_input":"2025-07-20T23:24:05.862307Z","iopub.status.idle":"2025-07-20T23:25:20.193156Z","shell.execute_reply.started":"2025-07-20T23:24:05.862282Z","shell.execute_reply":"2025-07-20T23:25:20.192402Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Ensure PyTorch can expand allocations if needed\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"id":"b1rI7F4dlowA","papermill":{"duration":0.039224,"end_time":"2025-07-14T06:47:03.708322","exception":false,"start_time":"2025-07-14T06:47:03.669098","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:25:20.195568Z","iopub.execute_input":"2025-07-20T23:25:20.195820Z","iopub.status.idle":"2025-07-20T23:25:20.200189Z","shell.execute_reply.started":"2025-07-20T23:25:20.195796Z","shell.execute_reply":"2025-07-20T23:25:20.199475Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\nimport os\nimport json\nimport dotenv\nimport torch\nimport pandas as pd\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport gc\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfrom config import Config\n\n\n# project root in PYTHONPATH\nimport sys\nproject_root = os.getcwd()\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"✅ Using device: {device}\")\n\n# Device & parallel setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"✅ Using device: {device}, count: {torch.cuda.device_count()} GPUs\")","metadata":{"id":"390a0abf","papermill":{"duration":11.413602,"end_time":"2025-07-14T06:47:15.156088","exception":false,"start_time":"2025-07-14T06:47:03.742486","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:25:20.200967Z","iopub.execute_input":"2025-07-20T23:25:20.201157Z","iopub.status.idle":"2025-07-20T23:25:28.335198Z","shell.execute_reply.started":"2025-07-20T23:25:20.201141Z","shell.execute_reply":"2025-07-20T23:25:28.334487Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda, count: 1 GPUs\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\ndef safe_model_name(model_name):\n    return model_name.replace(\"/\", \"_\")\n\n# archive & inspect\ndef create_run_dir(base, name):\n    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    d  = os.path.join(base, safe_model_name(name), f\"run_{ts}\")\n    os.makedirs(d, exist_ok=True)\n    return d\n\ndef run_evaluation():\n  pass\n    # os.system(\"python utils/prepare_metrics_log.py\")\n    # os.system(\"python evaluation/evaluate.py\")\n\n\ndef archive_results(run_dir, files_to_archive):\n    for fpath in files_to_archive:\n        if os.path.exists(fpath):\n            dest = os.path.join(run_dir, os.path.basename(fpath))\n            os.system(f\"cp {fpath} {dest}\")\n","metadata":{"id":"d797f524","papermill":{"duration":0.040399,"end_time":"2025-07-14T06:47:15.604797","exception":false,"start_time":"2025-07-14T06:47:15.564398","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:25:28.336116Z","iopub.execute_input":"2025-07-20T23:25:28.336582Z","iopub.status.idle":"2025-07-20T23:25:28.341766Z","shell.execute_reply.started":"2025-07-20T23:25:28.336564Z","shell.execute_reply":"2025-07-20T23:25:28.341194Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def load_model(model_name: str):\n    secure = safe_model_name(model_name)\n    model_path = os.path.join(\"models/fine_tuned_model/model_weights\", secure)\n    print(f\"Loading model from {model_path}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, legacy=False)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n    # mBART language tags\n    if \"bart\" in model_name.lower():\n        tokenizer.src_lang = tokenizer.tgt_lang = \"ar_AR\"\n    # wrap for multi‑GPU\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n    model.to(device).eval()\n    return tokenizer, model  ","metadata":{"id":"d09aa414","papermill":{"duration":0.04142,"end_time":"2025-07-14T06:47:15.530296","exception":false,"start_time":"2025-07-14T06:47:15.488876","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:25:28.342536Z","iopub.execute_input":"2025-07-20T23:25:28.342840Z","iopub.status.idle":"2025-07-20T23:25:28.358449Z","shell.execute_reply.started":"2025-07-20T23:25:28.342811Z","shell.execute_reply":"2025-07-20T23:25:28.357863Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ndef load_lora_model(base_model_path: str, lora_path: str):\n    print(f\"Loading base model from {base_model_path}\")\n    print(f\"Applying LoRA adapters from {lora_path}\")\n\n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False, legacy=False)\n\n    # Base model\n    base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_path)\n\n    # Apply LoRA\n    model = PeftModel.from_pretrained(base_model, lora_path)\n\n    # Optional: for mBART-like models\n    if \"bart\" in base_model_path.lower():\n        tokenizer.src_lang = tokenizer.tgt_lang = \"ar_AR\"\n\n    # DataParallel\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n\n    model.to(device).eval()\n    return tokenizer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:25:28.358988Z","iopub.execute_input":"2025-07-20T23:25:28.359144Z","iopub.status.idle":"2025-07-20T23:25:45.341213Z","shell.execute_reply.started":"2025-07-20T23:25:28.359130Z","shell.execute_reply":"2025-07-20T23:25:45.340644Z"}},"outputs":[{"name":"stderr","text":"2025-07-20 23:25:33.965469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753053934.167155      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753053934.229911      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def save_predictions_to_file(records, df, output_log_path):\n    # Map records by ID\n    records_by_id = {r[\"id\"]: r for r in records}\n\n    # Re-order by original ID order\n    ordered = [records_by_id[i] for i in df[\"id\"] if i in records_by_id]\n    missing = [i for i in df[\"id\"] if i not in records_by_id]\n    if missing:\n        print(f\"⚠️ Missing {len(missing)} records: {missing[:10]}...\")\n\n    # Save\n    with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(ordered, f, ensure_ascii=False, indent=2)\n    print(f\"✅ Generated {len(ordered)} records saved to {output_log_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:25:45.341955Z","iopub.execute_input":"2025-07-20T23:25:45.342459Z","iopub.status.idle":"2025-07-20T23:25:45.347581Z","shell.execute_reply.started":"2025-07-20T23:25:45.342416Z","shell.execute_reply":"2025-07-20T23:25:45.346809Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ### Batched inference function (robust, with error recovery)\n# def generate_predictions(tokenizer, model, validation_path, output_log_path, batch_size=8):\n#     import time\n#     import traceback\n\n#     # 1. Load & prepare\n#     df = pd.read_excel(validation_path)\n#     df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n\n#     # Handle DataParallel wrapping\n#     base_model = model.module if hasattr(model, \"module\") else model\n#     base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n#     assert base_model.config.eos_token_id is not None\n\n#     records = []\n\n#     from torch.utils.data import DataLoader, Dataset\n\n#     class TextDataset(Dataset):\n#         def __init__(self, texts): self.texts = texts\n#         def __len__(self): return len(self.texts)\n#         def __getitem__(self, idx): return self.texts[idx]\n\n#     def collate_fn(batch_texts):\n#         return tokenizer(\n#             batch_texts,\n#             return_tensors=\"pt\",\n#             padding=True,\n#             truncation=True,\n#             max_length=1024\n#         )\n\n#     loader = DataLoader(\n#         TextDataset(df[\"input_text\"].tolist()),\n#         batch_size=batch_size,\n#         shuffle=False,\n#         collate_fn=collate_fn,\n#         num_workers=4,\n#         pin_memory=True\n#     )\n\n#     for i, batch in enumerate(tqdm(loader, desc=\"Batches\")):\n#         success = False\n#         attempts = 0\n#         while not success and attempts < 3:\n#             try:\n#                 batch = {k: v.to(device) for k, v in batch.items()}\n#                 with torch.inference_mode():\n#                     gen_ids = base_model.generate(\n#                         input_ids=batch[\"input_ids\"],\n#                         attention_mask=batch[\"attention_mask\"],\n#                         max_new_tokens=1024\n#                     )\n#                 outs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n#                 start_idx = i * batch_size\n#                 for j, out in enumerate(outs):\n#                     row = df.iloc[start_idx + j]\n#                     records.append({\n#                         \"id\": int(row[\"id\"]),\n#                         \"author\": row[\"author\"],\n#                         \"output\": out,\n#                         \"ground_truth\": row[\"text_in_author_style\"]\n#                     })\n#                 success = True\n\n#             except RuntimeError as e:\n#                 attempts += 1\n#                 print(f\"❌ RuntimeError on batch {i}, attempt {attempts}: {str(e)}\")\n#                 traceback.print_exc()\n\n#                 torch.cuda.empty_cache()\n#                 gc.collect()\n\n#                 if \"CUDA out of memory\" in str(e):\n#                     print(\"⚠️ Reducing batch size temporarily and retrying...\")\n#                     time.sleep(2)\n#                     if batch[\"input_ids\"].shape[0] > 1:\n#                         # Retry each sample individually\n#                         for j in range(batch[\"input_ids\"].shape[0]):\n#                             try:\n#                                 single_batch = {k: v[j:j+1] for k, v in batch.items()}\n#                                 with torch.inference_mode():\n#                                     gen_ids = base_model.generate(\n#                                         input_ids=single_batch[\"input_ids\"],\n#                                         attention_mask=single_batch[\"attention_mask\"],\n#                                         max_new_tokens=1024\n#                                     )\n#                                 out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n#                                 row = df.iloc[i * batch_size + j]\n#                                 records.append({\n#                                     \"id\": int(row[\"id\"]),\n#                                     \"author\": row[\"author\"],\n#                                     \"output\": out,\n#                                     \"ground_truth\": row[\"text_in_author_style\"]\n#                                 })\n#                             except Exception as inner_e:\n#                                 print(f\"⚠️ Skipping failed sample {i * batch_size + j} id = {int(row['id'])}: {str(inner_e)}\")\n#                                 traceback.print_exc()\n#                     success = True  # Continue to next batch regardless\n#                 else:\n#                     raise e\n\n#         torch.cuda.empty_cache()\n#         gc.collect()\n\n#     # 4. Save\n#     with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n#         json.dump(records, f, ensure_ascii=False, indent=2)\n#     print(f\"✅ Generated {len(records)} records to {output_log_path}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-20T23:25:45.348377Z","iopub.execute_input":"2025-07-20T23:25:45.348656Z","iopub.status.idle":"2025-07-20T23:25:45.368859Z","shell.execute_reply.started":"2025-07-20T23:25:45.348630Z","shell.execute_reply":"2025-07-20T23:25:45.368296Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def generate_predictions(tokenizer, model, validation_path, output_log_path, batch_size=8):\n    import time, gc, json, traceback\n    import torch\n    from torch.utils.data import DataLoader, Dataset\n    from tqdm import tqdm\n    import pandas as pd\n\n    # 1. Load & prepare\n    df = pd.read_excel(validation_path)\n    df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n    \n    # Compute token lengths\n    df[\"token_len\"] = df[\"input_text\"].apply(lambda x: len(tokenizer.encode(x, truncation=True, max_length=1024)))\n    \n    # Save original order\n    original_order = df[[\"id\"]].copy()\n\n    # Sort by token length\n    df_sorted = df.sort_values(\"token_len\").reset_index(drop=True)\n\n    # Handle DataParallel wrapping\n    base_model = model.module if hasattr(model, \"module\") else model\n    base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n    assert base_model.config.eos_token_id is not None\n\n    records = []\n\n    class TextDataset(Dataset):\n        def __init__(self, texts): self.texts = texts\n        def __len__(self): return len(self.texts)\n        def __getitem__(self, idx): return self.texts[idx]\n\n    def collate_fn(batch_texts):\n        return tokenizer(\n            batch_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=1024\n        )\n\n    loader = DataLoader(\n        TextDataset(df_sorted[\"input_text\"].tolist()),\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    for i, batch in enumerate(tqdm(loader, desc=\"Batches\")):\n        success = False\n        attempts = 0\n        while not success and attempts < 3:\n            try:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.inference_mode():\n                    gen_ids = base_model.generate(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                        max_new_tokens=1024\n                    )\n                outs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n                start_idx = i * batch_size\n                for j, out in enumerate(outs):\n                    row = df_sorted.iloc[start_idx + j]\n                    records.append({\n                        \"id\": int(row[\"id\"]),\n                        \"author\": row[\"author\"],\n                        \"output\": out,\n                        \"ground_truth\": row[\"text_in_author_style\"]\n                    })\n                success = True\n\n            except RuntimeError as e:\n                attempts += 1\n                print(f\"❌ RuntimeError on batch {i}, attempt {attempts}: {str(e)}\")\n                traceback.print_exc()\n                torch.cuda.empty_cache()\n                gc.collect()\n\n                if \"CUDA out of memory\" in str(e):\n                    print(\"⚠️ Reducing batch size temporarily and retrying...\")\n                    time.sleep(2)\n                    if batch[\"input_ids\"].shape[0] > 1:\n                        for j in range(batch[\"input_ids\"].shape[0]):\n                            try:\n                                single_batch = {k: v[j:j+1] for k, v in batch.items()}\n                                with torch.inference_mode():\n                                    gen_ids = base_model.generate(\n                                        input_ids=single_batch[\"input_ids\"],\n                                        attention_mask=single_batch[\"attention_mask\"],\n                                        max_new_tokens=1024\n                                    )\n                                out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n                                row = df_sorted.iloc[i * batch_size + j]\n                                records.append({\n                                    \"id\": int(row[\"id\"]),\n                                    \"author\": row[\"author\"],\n                                    \"output\": out,\n                                    \"ground_truth\": row[\"text_in_author_style\"]\n                                })\n                            except Exception as inner_e:\n                                print(f\"⚠️ Skipping failed sample {i * batch_size + j} id = {int(row['id'])}: {str(inner_e)}\")\n                                traceback.print_exc()\n                    success = True\n                else:\n                    raise e\n\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    # Restore original order\n    df_records = pd.DataFrame(records)\n    df_final = original_order.merge(df_records, on=\"id\", how=\"left\")\n\n    with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(df_final.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n    print(f\"✅ Generated {len(df_final)} records to {output_log_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T23:25:45.369607Z","iopub.execute_input":"2025-07-20T23:25:45.369850Z","iopub.status.idle":"2025-07-20T23:25:45.391346Z","shell.execute_reply.started":"2025-07-20T23:25:45.369824Z","shell.execute_reply":"2025-07-20T23:25:45.390715Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def generate_predictions_by_budget(tokenizer, model, validation_path, output_log_path,\n                         initial_token_budget=9_000, max_retries=3):\n    import time, gc, json, traceback, torch\n    from tqdm import tqdm\n    import pandas as pd\n\n    # 1. Load & prepare\n    df = pd.read_excel(validation_path)\n    df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n\n    # Compute token lengths once\n    df[\"tok_len\"] = df[\"input_text\"].apply(\n        lambda x: len(tokenizer.encode(x, truncation=True, max_length=1024))\n    )\n\n    # Save original order\n    original_order = df[[\"id\"]].copy()\n\n    # Sort by length ascending\n    df_sorted = df.sort_values(\"tok_len\").reset_index(drop=True)\n\n    # Setup model\n    base_model = model.module if hasattr(model, \"module\") else model\n    base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n    device_ = next(base_model.parameters()).device\n\n    records = []\n    token_budget = initial_token_budget\n    retry = 0\n\n    while retry < max_retries:\n        records.clear()\n        try:\n            batch_idxs = []\n            current_sum = 0\n\n            # build batches by token budget\n            for idx, row in df_sorted.iterrows():\n                ln = row[\"tok_len\"]\n                if ln > token_budget:\n                    # single example too big: process alone\n                    batch_idxs.append([idx])\n                else:\n                    if current_sum + ln <= token_budget:\n                        batch_idxs[-1].append(idx) if batch_idxs else batch_idxs.append([idx])\n                        current_sum += ln\n                    else:\n                        batch_idxs.append([idx])\n                        current_sum = ln\n\n            # run generation per batch\n            for b in tqdm(batch_idxs, desc=\"Batches\"):\n                texts = df_sorted.loc[b, \"input_text\"].tolist()\n                enc = tokenizer(texts, return_tensors=\"pt\",\n                                padding=True, truncation=True, max_length=1300)\n                enc = {k: v.to(device_) for k, v in enc.items()}\n\n                with torch.inference_mode():\n                    gen = base_model.generate(\n                        **enc,\n                        max_new_tokens=1500,\n                        use_cache=False\n                    )\n                outs = tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n                for idx, out in zip(b, outs):\n                    row = df_sorted.loc[idx]\n                    records.append({\n                        \"id\":    int(row[\"id\"]),\n                        \"author\":row[\"author\"],\n                        \"output\":out,\n                        \"ground_truth\": row[\"text_in_author_style\"]\n                    })\n\n                torch.cuda.empty_cache()\n                gc.collect()\n\n            # success, break retry loop\n            break\n\n        except RuntimeError as e:\n            if \"out of memory\" in str(e).lower() and retry < max_retries - 1:\n                print(f\"⚠️ OOM on token_budget={token_budget}, retrying with half budget\")\n                torch.cuda.empty_cache(); gc.collect()\n                token_budget = token_budget // 2\n                retry += 1\n                time.sleep(2)\n                continue\n            else:\n                raise\n\n    # Restore original order\n    df_rec = pd.DataFrame(records)\n    df_final = original_order.merge(df_rec, on=\"id\", how=\"left\")\n\n    # Save results\n    with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(df_final.to_dict(orient=\"records\"),\n                  f, ensure_ascii=False, indent=2)\n\n    print(f\"✅ Generated {len(df_final)} records (token_budget={token_budget}) → {output_log_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install accelerate\n# accelerate config      # answer prompts (choose multi‑GPU)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# from config import Config  # import your custom Config module\n# import pandas as pd\n# import torch\n# gc = __import__('gc')\n# from tqdm import tqdm\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n# from accelerate import Accelerator\n# import pandas as pd\n# import torch\n# gc = __import__('gc')\n# from tqdm import tqdm\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n# from accelerate import Accelerator\n\n# def generate_predictions(\n#     tokenizer, model, validation_path, output_log_path,\n#     max_new_tokens=1700,\n#     token_budget=100_000,\n#     sort_inputs=True\n# ):\n#     # Initialize Accelerator\n#     accelerator = Accelerator()\n#     # Prepare model & tokenizer\n#     model, tokenizer = accelerator.prepare(model, tokenizer)\n#     model.eval()\n#     unwrapped = accelerator.unwrap_model(model)\n\n#     # 1. Load & prepare DataFrame\n#     df = pd.read_excel(validation_path)\n#     df['input_text'] = df['author'].astype(str) + ' [SEP] ' + df['text_in_msa'].astype(str)\n\n#     # 2. Compute token lengths\n#     max_len = 1024 #unwrapped.config.max_position_embeddings\n#     enc = tokenizer(\n#         df['input_text'].tolist(), return_length=True,\n#         padding=False, truncation=True, max_length=max_len\n#     )\n#     lengths = enc['length']\n\n#     # 3. Build token-budgeted batches\n#     indices = list(range(len(lengths)))\n#     if sort_inputs:\n#         indices.sort(key=lambda i: lengths[i], reverse=True)\n#     batches, curr, curr_sum = [], [], 0\n#     for i in indices:\n#         cost = lengths[i] + max_new_tokens\n#         if curr and curr_sum + cost > token_budget:\n#             batches.append(curr)\n#             curr, curr_sum = [], 0\n#         curr.append(i)\n#         curr_sum += cost\n#     if curr:\n#         batches.append(curr)\n\n#     # 4. Generation loop with instrumentation\n#     records = []\n#     for batch_idx, batch_ids in enumerate(tqdm(batches, desc='Accelerated Batches')):\n#         # Instrumentation\n#         sample_count = len(batch_ids)\n#         batch_token_cost = sum(lengths[i] + max_new_tokens for i in batch_ids)\n#         print(f\"\\n▶ Batch {batch_idx+1}/{len(batches)}: samples={sample_count}, token_cost={batch_token_cost}\")\n#         # Memory before\n#         for dev in range(torch.cuda.device_count()):\n#             alloc = torch.cuda.memory_allocated(dev) / 1024**2\n#             reserved = torch.cuda.memory_reserved(dev) / 1024**2\n#             print(f\"   • GPU {dev}: allocated={alloc:.1f} MiB, reserved={reserved:.1f} MiB\")\n\n#         texts = [df.iloc[i]['input_text'] for i in batch_ids]\n#         inputs = tokenizer(\n#             texts, return_tensors='pt', padding=True,\n#             truncation=True, max_length=max_len\n#         )\n#         inputs = {k: v.to(accelerator.device) for k, v in inputs.items()}\n\n#         with torch.no_grad():\n#             gen_ids = model.generate(\n#                 input_ids=inputs['input_ids'],\n#                 attention_mask=inputs['attention_mask'],\n#                 max_new_tokens=max_new_tokens\n#             )\n#         gen_ids = accelerator.gather(gen_ids)\n#         outs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n#         # Memory after\n#         for dev in range(torch.cuda.device_count()):\n#             alloc = torch.cuda.memory_allocated(dev) / 1024**2\n#             reserved = torch.cuda.memory_reserved(dev) / 1024**2\n#             print(f\"   • GPU {dev}: allocated={alloc:.1f} MiB, reserved={reserved:.1f} MiB\")\n\n#         for idx, out in zip(batch_ids, outs):\n#             row = df.iloc[idx]\n#             records.append({\n#                 'id': int(row['id']),\n#                 'author': row['author'],\n#                 'output': out,\n#                 'ground_truth': row['text_in_author_style']\n#             })\n\n#         # cleanup\n#         del inputs, gen_ids\n#         gc.collect()\n#         torch.cuda.empty_cache()\n\n#     # 5. Save results\n#     import json\n#     with open(output_log_path, 'w', encoding='utf-8') as f:\n#         json.dump(records, f, ensure_ascii=False, indent=2)\n#     print(f\"✅ Generated {len(records)} records and saved to {output_log_path}\")\n#     return records, df\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\nimport gc\ngc.collect()","metadata":{"id":"bQCIIwaEEEp9","papermill":{"duration":0.202454,"end_time":"2025-07-14T06:47:15.921644","exception":false,"start_time":"2025-07-14T06:47:15.719190","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"Lkh_YWV5Exw8","papermill":{"duration":0.16832,"end_time":"2025-07-14T06:47:16.124774","exception":false,"start_time":"2025-07-14T06:47:15.956454","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------- Cell 4: run the pipeline ----------------\n\n# if __name__ == '__main__':\n# adjust these paths as needed\nvalidation_file = Config.VAL_FILE\nlog_file = f\"models/fine_tuned_model/results/{safe_model_name(MODEL_NAME)}/generation_log.json\"\noutput_base_dir = \"analysis/fine_tuned_model\"\n\nos.makedirs(os.path.dirname(log_file), exist_ok=True)\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# tokenizer, model = load_model(MODEL_NAME)\n\n# base_model_path = \"models/base_models/facebook/arabart\"\n# lora_path       = \"models/fine_tuned_model/model_weights/lora_moussa_arabart\"\n\ntokenizer, model = load_lora_model(base_model_path, lora_path)\n","metadata":{"id":"FzfhVKAGXa8M","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate_predictions(tokenizer, model, validation_file, log_file, batch_size=48)\ngenerate_predictions_by_budget(tokenizer, model, validation_file, log_file)\n\n# records, df = generate_predictions(tokenizer, model, validation_file, log_file)\n# If saving separately is needed\n# save_predictions_to_file(records, df, log_file)\n\nrun_dir = create_run_dir(output_base_dir, MODEL_NAME)\nos.system(f\"cp {log_file} {run_dir}/\")\nprint(f\"\\n✅ Pipeline done. Logs in: {run_dir} \\n LOG FILE: {log_file}\\n\")","metadata":{"id":"FzfhVKAGXa8M","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Display a link to the copied log file in the run_dir\nlog_file_in_run_dir = os.path.join(run_dir, os.path.basename(log_file))\ndisplay(FileLink(log_file_in_run_dir))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(len(records))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 1: Delete large objects\n# # del model, tokenizer\n\n# # # Step 2: If model is still in GPU, move to CPU first\n# # model.to(\"cpu\")  \n# # # then \n# # del model\n\n# # Step 3: Run GC and empty cache\n# import gc, torch\n# gc.collect()\n# torch.cuda.empty_cache()\n\n# # Step 4: Confirm\n# print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n# print(f\"Cached:    {torch.cuda.memory_reserved() / 1e6:.1f} MB\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}