{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12342718,"sourceType":"datasetVersion","datasetId":7780982},{"sourceId":12410724,"sourceType":"datasetVersion","datasetId":7826999},{"sourceId":12454188,"sourceType":"datasetVersion","datasetId":7856142},{"sourceId":12524371,"sourceType":"datasetVersion","datasetId":7905884},{"sourceId":12528598,"sourceType":"datasetVersion","datasetId":7908780}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":128.955805,"end_time":"2025-07-14T06:47:18.107738","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-14T06:45:09.151933","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"fTyhOO2v7ssz","outputId":"7eecf25f-5840-4ed0-c27a-fa479b3c0838","papermill":{"duration":0.038757,"end_time":"2025-07-14T06:46:43.831489","exception":false,"start_time":"2025-07-14T06:46:43.792732","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:53:40.912894Z","iopub.execute_input":"2025-07-24T14:53:40.913207Z","iopub.status.idle":"2025-07-24T14:53:40.917554Z","shell.execute_reply.started":"2025-07-24T14:53:40.913182Z","shell.execute_reply":"2025-07-24T14:53:40.916859Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# project_dir = \"/content/drive/MyDrive/AraGenEval2025\"\n# os.chdir(project_dir)\n# print(f\"✅ Current directory: {os.getcwd()}\")\n","metadata":{"id":"cab9aba0","papermill":{"duration":0.041911,"end_time":"2025-07-14T06:47:15.232757","exception":false,"start_time":"2025-07-14T06:47:15.190846","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:53:40.968356Z","iopub.execute_input":"2025-07-24T14:53:40.968741Z","iopub.status.idle":"2025-07-24T14:53:40.971805Z","shell.execute_reply.started":"2025-07-24T14:53:40.968721Z","shell.execute_reply":"2025-07-24T14:53:40.971113Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install transformers[sentencepiece] torch pandas openpyxl psutil python-dotenv peft  --quiet","metadata":{"id":"559ddd13","outputId":"7921c04f-9a42-44dc-e548-5c1805145485","papermill":{"duration":90.123476,"end_time":"2025-07-14T06:46:43.758212","exception":false,"start_time":"2025-07-14T06:45:13.634736","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:53:40.975004Z","iopub.execute_input":"2025-07-24T14:53:40.975191Z","iopub.status.idle":"2025-07-24T14:55:18.055627Z","shell.execute_reply.started":"2025-07-24T14:53:40.975176Z","shell.execute_reply":"2025-07-24T14:55:18.054899Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n\npath = \"/kaggle/working\"\nis_empty = (len(os.listdir(path)) < 5)\n\n!ls /kaggle/working/\n\nMODEL_WEIGHTS_SRC =\"/kaggle/input/ubc-nlp-rat5v2-base-1024-model/UBC-NLP_AraT5v2-base-1024/final\"+\"/*\"\n\nlora_injection_src=  \"/kaggle/input/ubc-ara-t5-v2-lora-ast-model/UBC-NLP_AraT5v2-base-1024_with_lora/UBC-NLP_AraT5v2-base-1024_with_lora/final\"+\"/*\"\n\n# MODEL_NAME = \"facebook_mbart-large-50-many-to-many-mmt\" #\"agemagician_mlong-t5-tglobal-large\" #\"facebook_mbart-large-50-many-to-many-mmt\" # \"UBC-NLP_AraT5-base\" #\"google/mt5-small\"\nMODEL_NAME = \"UBC-NLP_AraT5v2-base-1024\" \n\nMODEL_WEIGHTS_DEST_DIR =f\"/kaggle/working/models/fine_tuned_model/model_weights/{MODEL_NAME}\"\n\nbase_model_path = MODEL_WEIGHTS_DEST_DIR\nlora_path       = f\"./{MODEL_NAME}_with_lora\"","metadata":{"papermill":{"duration":19.447473,"end_time":"2025-07-14T06:47:03.311976","exception":false,"start_time":"2025-07-14T06:46:43.864503","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:18.057325Z","iopub.execute_input":"2025-07-24T14:55:18.057607Z","iopub.status.idle":"2025-07-24T14:55:18.178877Z","shell.execute_reply.started":"2025-07-24T14:55:18.057581Z","shell.execute_reply":"2025-07-24T14:55:18.177886Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"## copy project structure and other needed inputs\n\nif is_empty:\n    print(f\"✅ '{path}' is empty. copying project strcuture into it ...\")\n    \n    #  project strcture\n    !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n    \n    # final test dataset\n    !cp /kaggle/input/arageneval2025-task1-finaldataset/PublicDataFinalPhaseTask1.xlsx /kaggle/working/data/\n    \n    # model weights\n    !mkdir -p {MODEL_WEIGHTS_DEST_DIR}\n    !cp -r {MODEL_WEIGHTS_SRC} {MODEL_WEIGHTS_DEST_DIR}\n    #lora weights\n    # !mkdir -p {lora_path}\n    # !cp -r {lora_injection_src} {lora_path}\nelse:\n    print(f\"❌ '{path}' is not empty — contains {len(os.listdir(path))} items.\")","metadata":{"papermill":{"duration":19.447473,"end_time":"2025-07-14T06:47:03.311976","exception":false,"start_time":"2025-07-14T06:46:43.864503","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:18.180067Z","iopub.execute_input":"2025-07-24T14:55:18.180323Z","iopub.status.idle":"2025-07-24T14:55:28.143496Z","shell.execute_reply.started":"2025-07-24T14:55:18.180298Z","shell.execute_reply":"2025-07-24T14:55:28.142702Z"}},"outputs":[{"name":"stdout","text":"✅ '/kaggle/working' is empty. copying project strcuture into it ...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Change to project directory\n%cd /kaggle/working/ \n\n# Confirm\n!ls","metadata":{"papermill":{"duration":19.447473,"end_time":"2025-07-14T06:47:03.311976","exception":false,"start_time":"2025-07-14T06:46:43.864503","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:28.145229Z","iopub.execute_input":"2025-07-24T14:55:28.145465Z","iopub.status.idle":"2025-07-24T14:55:28.263234Z","shell.execute_reply.started":"2025-07-24T14:55:28.145442Z","shell.execute_reply":"2025-07-24T14:55:28.262525Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\nconfig.py  evaluation\tmodels\t run_pipeline.py\ndata\t   __init__.py\tprompts  utils\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!ls -R {MODEL_WEIGHTS_DEST_DIR}","metadata":{"papermill":{"duration":0.154964,"end_time":"2025-07-14T06:47:03.499832","exception":false,"start_time":"2025-07-14T06:47:03.344868","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:28.264110Z","iopub.execute_input":"2025-07-24T14:55:28.264316Z","iopub.status.idle":"2025-07-24T14:55:35.480030Z","shell.execute_reply.started":"2025-07-24T14:55:28.264296Z","shell.execute_reply":"2025-07-24T14:55:35.479110Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/models/fine_tuned_model/model_weights/UBC-NLP_AraT5v2-base-1024:\nadded_tokens.json\tmodel.safetensors\t tokenizer_config.json\nconfig.json\t\tspecial_tokens_map.json  training_args.bin\ngeneration_config.json\tspiece.model\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!ls -R {lora_path}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:35.481240Z","iopub.execute_input":"2025-07-24T14:55:35.481494Z","iopub.status.idle":"2025-07-24T14:55:37.088332Z","shell.execute_reply.started":"2025-07-24T14:55:35.481470Z","shell.execute_reply":"2025-07-24T14:55:37.087601Z"}},"outputs":[{"name":"stdout","text":"ls: cannot access './UBC-NLP_AraT5v2-base-1024_with_lora': No such file or directory\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Ensure PyTorch can expand allocations if needed\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"id":"b1rI7F4dlowA","papermill":{"duration":0.039224,"end_time":"2025-07-14T06:47:03.708322","exception":false,"start_time":"2025-07-14T06:47:03.669098","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:37.089389Z","iopub.execute_input":"2025-07-24T14:55:37.089624Z","iopub.status.idle":"2025-07-24T14:55:37.093771Z","shell.execute_reply.started":"2025-07-24T14:55:37.089601Z","shell.execute_reply":"2025-07-24T14:55:37.093052Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\nimport os\nimport json\nimport dotenv\nimport torch\nimport pandas as pd\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport gc\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfrom config import Config\n\n\n# project root in PYTHONPATH\nimport sys\nproject_root = os.getcwd()\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"✅ Using device: {device}\")\n\n# Device & parallel setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"✅ Using device: {device}, count: {torch.cuda.device_count()} GPUs\")","metadata":{"id":"390a0abf","papermill":{"duration":11.413602,"end_time":"2025-07-14T06:47:15.156088","exception":false,"start_time":"2025-07-14T06:47:03.742486","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:37.094493Z","iopub.execute_input":"2025-07-24T14:55:37.094707Z","iopub.status.idle":"2025-07-24T14:55:44.735729Z","shell.execute_reply.started":"2025-07-24T14:55:37.094683Z","shell.execute_reply":"2025-07-24T14:55:44.734988Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda, count: 1 GPUs\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\ndef safe_model_name(model_name):\n    return model_name.replace(\"/\", \"_\")\n\n# archive & inspect\ndef create_run_dir(base, name):\n    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    d  = os.path.join(base, safe_model_name(name), f\"run_{ts}\")\n    os.makedirs(d, exist_ok=True)\n    return d\n\ndef run_evaluation():\n  pass\n    # os.system(\"python utils/prepare_metrics_log.py\")\n    # os.system(\"python evaluation/evaluate.py\")\n\n\ndef archive_results(run_dir, files_to_archive):\n    for fpath in files_to_archive:\n        if os.path.exists(fpath):\n            dest = os.path.join(run_dir, os.path.basename(fpath))\n            os.system(f\"cp {fpath} {dest}\")\n","metadata":{"id":"d797f524","papermill":{"duration":0.040399,"end_time":"2025-07-14T06:47:15.604797","exception":false,"start_time":"2025-07-14T06:47:15.564398","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:44.736599Z","iopub.execute_input":"2025-07-24T14:55:44.737485Z","iopub.status.idle":"2025-07-24T14:55:44.742202Z","shell.execute_reply.started":"2025-07-24T14:55:44.737465Z","shell.execute_reply":"2025-07-24T14:55:44.741533Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def load_model(model_name: str):\n    secure = safe_model_name(model_name)\n    model_path = os.path.join(\"models/fine_tuned_model/model_weights\", secure)\n    print(f\"Loading model from {model_path}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, legacy=False)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n    # mBART language tags\n    if \"bart\" in model_name.lower():\n        tokenizer.src_lang = tokenizer.tgt_lang = \"ar_AR\"\n    # wrap for multi‑GPU\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n    model.to(device).eval()\n    return tokenizer, model  ","metadata":{"id":"d09aa414","papermill":{"duration":0.04142,"end_time":"2025-07-14T06:47:15.530296","exception":false,"start_time":"2025-07-14T06:47:15.488876","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:44.744763Z","iopub.execute_input":"2025-07-24T14:55:44.745304Z","iopub.status.idle":"2025-07-24T14:55:44.762466Z","shell.execute_reply.started":"2025-07-24T14:55:44.745286Z","shell.execute_reply":"2025-07-24T14:55:44.761732Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ndef load_lora_model(base_model_path: str, lora_path: str):\n    print(f\"Loading base model from {base_model_path}\")\n    print(f\"Applying LoRA adapters from {lora_path}\")\n\n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False, legacy=False)\n\n    # Base model\n    base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_path)\n\n    # Apply LoRA\n    model = PeftModel.from_pretrained(base_model, lora_path)\n\n    # Optional: for mBART-like models\n    if \"bart\" in base_model_path.lower():\n        tokenizer.src_lang = tokenizer.tgt_lang = \"ar_AR\"\n\n    # DataParallel\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n\n    model.to(device).eval()\n    return tokenizer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:55:44.763157Z","iopub.execute_input":"2025-07-24T14:55:44.763346Z","iopub.status.idle":"2025-07-24T14:56:01.167655Z","shell.execute_reply.started":"2025-07-24T14:55:44.763331Z","shell.execute_reply":"2025-07-24T14:56:01.166817Z"}},"outputs":[{"name":"stderr","text":"2025-07-24 14:55:49.996639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753368950.167907      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753368950.218593      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def save_predictions_to_file(records, df, output_log_path):\n    # Map records by ID\n    records_by_id = {r[\"id\"]: r for r in records}\n\n    # Re-order by original ID order\n    ordered = [records_by_id[i] for i in df[\"id\"] if i in records_by_id]\n    missing = [i for i in df[\"id\"] if i not in records_by_id]\n    if missing:\n        print(f\"⚠️ Missing {len(missing)} records: {missing[:10]}...\")\n\n    # Save\n    with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(ordered, f, ensure_ascii=False, indent=2)\n    print(f\"✅ Generated {len(ordered)} records saved to {output_log_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.168549Z","iopub.execute_input":"2025-07-24T14:56:01.169015Z","iopub.status.idle":"2025-07-24T14:56:01.174097Z","shell.execute_reply.started":"2025-07-24T14:56:01.168996Z","shell.execute_reply":"2025-07-24T14:56:01.173366Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ### Batched inference function (robust, with error recovery)\n# def generate_predictions(tokenizer, model, validation_path, output_log_path, batch_size=8):\n#     import time\n#     import traceback\n\n#     # 1. Load & prepare\n#     df = pd.read_excel(validation_path)\n#     df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n\n#     # Handle DataParallel wrapping\n#     base_model = model.module if hasattr(model, \"module\") else model\n#     base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n#     assert base_model.config.eos_token_id is not None\n\n#     records = []\n\n#     from torch.utils.data import DataLoader, Dataset\n\n#     class TextDataset(Dataset):\n#         def __init__(self, texts): self.texts = texts\n#         def __len__(self): return len(self.texts)\n#         def __getitem__(self, idx): return self.texts[idx]\n\n#     def collate_fn(batch_texts):\n#         return tokenizer(\n#             batch_texts,\n#             return_tensors=\"pt\",\n#             padding=True,\n#             truncation=True,\n#             max_length=1024\n#         )\n\n#     loader = DataLoader(\n#         TextDataset(df[\"input_text\"].tolist()),\n#         batch_size=batch_size,\n#         shuffle=False,\n#         collate_fn=collate_fn,\n#         num_workers=4,\n#         pin_memory=True\n#     )\n\n#     for i, batch in enumerate(tqdm(loader, desc=\"Batches\")):\n#         success = False\n#         attempts = 0\n#         while not success and attempts < 3:\n#             try:\n#                 batch = {k: v.to(device) for k, v in batch.items()}\n#                 with torch.inference_mode():\n#                     gen_ids = base_model.generate(\n#                         input_ids=batch[\"input_ids\"],\n#                         attention_mask=batch[\"attention_mask\"],\n#                         max_new_tokens=1024\n#                     )\n#                 outs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n#                 start_idx = i * batch_size\n#                 for j, out in enumerate(outs):\n#                     row = df.iloc[start_idx + j]\n#                     records.append({\n#                         \"id\": int(row[\"id\"]),\n#                         \"author\": row[\"author\"],\n#                         \"output\": out,\n#                         \"ground_truth\": row[\"text_in_author_style\"]\n#                     })\n#                 success = True\n\n#             except RuntimeError as e:\n#                 attempts += 1\n#                 print(f\"❌ RuntimeError on batch {i}, attempt {attempts}: {str(e)}\")\n#                 traceback.print_exc()\n\n#                 torch.cuda.empty_cache()\n#                 gc.collect()\n\n#                 if \"CUDA out of memory\" in str(e):\n#                     print(\"⚠️ Reducing batch size temporarily and retrying...\")\n#                     time.sleep(2)\n#                     if batch[\"input_ids\"].shape[0] > 1:\n#                         # Retry each sample individually\n#                         for j in range(batch[\"input_ids\"].shape[0]):\n#                             try:\n#                                 single_batch = {k: v[j:j+1] for k, v in batch.items()}\n#                                 with torch.inference_mode():\n#                                     gen_ids = base_model.generate(\n#                                         input_ids=single_batch[\"input_ids\"],\n#                                         attention_mask=single_batch[\"attention_mask\"],\n#                                         max_new_tokens=1024\n#                                     )\n#                                 out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n#                                 row = df.iloc[i * batch_size + j]\n#                                 records.append({\n#                                     \"id\": int(row[\"id\"]),\n#                                     \"author\": row[\"author\"],\n#                                     \"output\": out,\n#                                     \"ground_truth\": row[\"text_in_author_style\"]\n#                                 })\n#                             except Exception as inner_e:\n#                                 print(f\"⚠️ Skipping failed sample {i * batch_size + j} id = {int(row['id'])}: {str(inner_e)}\")\n#                                 traceback.print_exc()\n#                     success = True  # Continue to next batch regardless\n#                 else:\n#                     raise e\n\n#         torch.cuda.empty_cache()\n#         gc.collect()\n\n#     # 4. Save\n#     with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n#         json.dump(records, f, ensure_ascii=False, indent=2)\n#     print(f\"✅ Generated {len(records)} records to {output_log_path}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-24T14:56:01.175000Z","iopub.execute_input":"2025-07-24T14:56:01.175180Z","iopub.status.idle":"2025-07-24T14:56:01.196925Z","shell.execute_reply.started":"2025-07-24T14:56:01.175158Z","shell.execute_reply":"2025-07-24T14:56:01.196284Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# def generate_predictions(tokenizer, model, validation_path, output_log_path, batch_size=8):\n#     import time, gc, json, traceback\n#     import torch\n#     from torch.utils.data import DataLoader, Dataset\n#     from tqdm import tqdm\n#     import pandas as pd\n\n#     max_input_length=1300\n#     max_generation_length = 1500\n\n#     # 1. Load & prepare\n#     df = pd.read_excel(validation_path)\n#     df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n    \n#     # Compute token lengths\n#     df[\"token_len\"] = df[\"input_text\"].apply(lambda x: len(tokenizer.encode(x, truncation=True, max_length=1024)))\n    \n#     # Save original order\n#     original_order = df[[\"id\"]].copy()\n\n#     # Sort by token length\n#     df_sorted = df.sort_values(\"token_len\").reset_index(drop=True)\n\n#     # Handle DataParallel wrapping\n#     base_model = model.module if hasattr(model, \"module\") else model\n#     base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n#     assert base_model.config.eos_token_id is not None\n\n#     records = []\n\n#     class TextDataset(Dataset):\n#         def __init__(self, texts): self.texts = texts\n#         def __len__(self): return len(self.texts)\n#         def __getitem__(self, idx): return self.texts[idx]\n\n#     def collate_fn(batch_texts):\n#         return tokenizer(\n#             batch_texts,\n#             return_tensors=\"pt\",\n#             padding=True,\n#             truncation=True,\n#             max_length=max_input_length\n#         )\n\n#     loader = DataLoader(\n#         TextDataset(df_sorted[\"input_text\"].tolist()),\n#         batch_size=batch_size,\n#         shuffle=False,\n#         collate_fn=collate_fn,\n#         num_workers=2,\n#         pin_memory=True\n#     )\n\n#     for i, batch in enumerate(tqdm(loader, desc=\"Batches\")):\n#         success = False\n#         attempts = 0\n#         while not success and attempts < 3:\n#             try:\n#                 batch = {k: v.to(device) for k, v in batch.items()}\n#                 with torch.inference_mode():\n#                     gen_ids = base_model.generate(\n#                         input_ids=batch[\"input_ids\"],\n#                         attention_mask=batch[\"attention_mask\"],\n#                         max_new_tokens=max_generation_length,\n#                         use_cache=False\n#                     )\n#                 outs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n#                 start_idx = i * batch_size\n#                 for j, out in enumerate(outs):\n#                     row = df_sorted.iloc[start_idx + j]\n#                     records.append({\n#                         \"id\": int(row[\"id\"]),\n#                         \"author\": row[\"author\"],\n#                         \"output\": out,\n#                         # \"ground_truth\": row[\"text_in_author_style\"]\n#                     })\n                \n#                 # Proactively clear memory between batches\n#                 del gen_ids, outs\n#                 torch.cuda.empty_cache()\n#                 gc.collect()\n                \n#                 success = True\n\n#             except RuntimeError as e:\n#                 attempts += 1\n#                 print(f\"❌ RuntimeError on batch {i}, attempt {attempts}: {str(e)}\")\n#                 traceback.print_exc()\n#                 torch.cuda.empty_cache()\n#                 gc.collect()\n\n#                 if \"CUDA out of memory\" in str(e):\n#                     print(\"⚠️ Reducing batch size temporarily and retrying...\")\n#                     time.sleep(2)\n#                     if batch[\"input_ids\"].shape[0] > 1:\n#                         for j in range(batch[\"input_ids\"].shape[0]):\n#                             try:\n#                                 single_batch = {k: v[j:j+1] for k, v in batch.items()}\n#                                 with torch.inference_mode():\n#                                     gen_ids = base_model.generate(\n#                                         input_ids=single_batch[\"input_ids\"],\n#                                         attention_mask=single_batch[\"attention_mask\"],\n#                                         max_new_tokens=max_generation_length,\n#                                         use_cache=False\n#                                     )\n#                                 out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n#                                 row = df_sorted.iloc[i * batch_size + j]\n#                                 records.append({\n#                                     \"id\": int(row[\"id\"]),\n#                                     \"author\": row[\"author\"],\n#                                     \"output\": out,\n#                                     # \"ground_truth\": row[\"text_in_author_style\"]\n#                                 })\n#                             except Exception as inner_e:\n#                                 print(f\"⚠️ Skipping failed sample {i * batch_size + j} id = {int(row['id'])}: {str(inner_e)}\")\n#                                 traceback.print_exc()\n#                     success = True\n#                 else:\n#                     raise e\n\n#         torch.cuda.empty_cache()\n#         gc.collect()\n\n#     # Restore original order\n#     df_records = pd.DataFrame(records)\n#     df_final = original_order.merge(df_records, on=\"id\", how=\"left\")\n\n#     with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n#         json.dump(df_final.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n#     print(f\"✅ Generated {len(df_final)} records to {output_log_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.197635Z","iopub.execute_input":"2025-07-24T14:56:01.197825Z","iopub.status.idle":"2025-07-24T14:56:01.217699Z","shell.execute_reply.started":"2025-07-24T14:56:01.197810Z","shell.execute_reply":"2025-07-24T14:56:01.217153Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def generate_predictions(tokenizer, model, validation_path, output_log_path, batch_size=8):\n    import time, gc, json, traceback\n    import torch\n    from tqdm import tqdm\n    import pandas as pd\n\n    max_input_length = 3350\n    max_generation_length = 3500\n\n    # Load and prepare\n    df = pd.read_excel(validation_path)\n    df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n    df[\"token_len\"] = df[\"input_text\"].apply(lambda x: len(tokenizer.encode(x, truncation=True, max_length=1024)))\n    original_order = df[[\"id\"]].copy()\n    df_sorted = df.sort_values(\"token_len\").reset_index(drop=True)\n\n    # Handle DataParallel\n    base_model = model.module if hasattr(model, \"module\") else model\n    base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n    assert base_model.config.eos_token_id is not None\n\n    device = next(base_model.parameters()).device\n    records = []\n\n    def collate_fn(batch_texts):\n        return tokenizer(\n            batch_texts,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            truncation=True,\n            max_length=max_input_length\n        )\n\n    current_batch_size = batch_size  # Will be updated globally\n\n    def process_batch(batch_texts, start_idx):\n        nonlocal records, current_batch_size\n        try:\n            tokenized = collate_fn(batch_texts)\n            input_lens = tokenized[\"attention_mask\"].sum(dim=1)\n            dynamic_max_gen_len = min(max_generation_length, input_lens.max().item() + 256)\n            tokenized = {k: v.to(device) for k, v in tokenized.items()}\n\n            with torch.inference_mode():\n                gen_ids = base_model.generate(\n                    input_ids=tokenized[\"input_ids\"],\n                    attention_mask=tokenized[\"attention_mask\"],\n                    max_new_tokens=dynamic_max_gen_len,\n                    use_cache=False\n                )\n            outputs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n            for j, out in enumerate(outputs):\n                row = df_sorted.iloc[start_idx + j]\n                records.append({\n                    \"id\": int(row[\"id\"]),\n                    \"author\": row[\"author\"],\n                    \"output\": out,\n                })\n\n            del gen_ids, outputs, tokenized\n            torch.cuda.empty_cache()\n            gc.collect()\n            return True\n\n        except RuntimeError as e:\n            if \"CUDA out of memory\" in str(e) and len(batch_texts) > 1:\n                torch.cuda.empty_cache()\n                gc.collect()\n                mid = len(batch_texts) // 2\n                print(f\"⚠️ OOM on batch [{start_idx}:{start_idx+len(batch_texts)}]. Retrying halves of size {mid}...\")\n\n                left_success = process_batch(batch_texts[:mid], start_idx)\n                right_success = process_batch(batch_texts[mid:], start_idx + mid)\n\n                # Update global batch size to the successful smaller value\n                if left_success and right_success and mid < current_batch_size:\n                    print(f\"✅ Updating future batch size to {mid}\")\n                    current_batch_size = mid\n\n                return left_success and right_success\n            else:\n                print(f\"❌ Failed batch [{start_idx}:{start_idx+len(batch_texts)}]: {str(e)}\")\n                traceback.print_exc()\n                return False\n\n    # Manual batching loop with adaptive batch size\n    i = 0\n    total = len(df_sorted)\n    input_texts = df_sorted[\"input_text\"].tolist()\n\n    with tqdm(total=total, desc=\"Batches\") as pbar:\n        while i < total:\n            end = min(i + current_batch_size, total)\n            batch_texts = input_texts[i:end]\n            success = process_batch(batch_texts, i)\n            if success:\n                pbar.update(len(batch_texts))\n                i += len(batch_texts)\n            else:\n                print(f\"⚠️ Skipping batch [{i}:{end}] completely after failure.\")\n                i += len(batch_texts)  # skip failed samples\n\n    # Restore original order\n    df_records = pd.DataFrame(records)\n    df_final = original_order.merge(df_records, on=\"id\", how=\"left\")\n\n    with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(df_final.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n\n    print(f\"✅ Generated {len(df_final)} records to {output_log_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.218481Z","iopub.execute_input":"2025-07-24T14:56:01.219157Z","iopub.status.idle":"2025-07-24T14:56:01.241679Z","shell.execute_reply.started":"2025-07-24T14:56:01.219140Z","shell.execute_reply":"2025-07-24T14:56:01.241024Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# # Single‑GPU token‑budgeted generation with last‑three‑batches shrink\n# import math\n# import threading\n# from queue import Queue\n\n# def generate_predictions_by_budget(\n#     tokenizer,\n#     model,\n#     validation_path,\n#     output_log_path,\n#     token_budget=30_000,\n#     budget_update_freq=5,\n#     budget_update_quota=2000,\n#     memory_usage_threshold=80\n# ):\n#     import time, gc, json, traceback\n#     import torch\n#     import pandas as pd\n#     from tqdm import tqdm\n#     import pynvml\n#     from transformers import DataCollatorWithPadding\n\n#     # --- Init NVML & Model ---\n#     pynvml.nvmlInit()\n#     device = next(model.module.parameters() if hasattr(model, \"module\") else model.parameters()).device\n#     handle = pynvml.nvmlDeviceGetHandleByIndex(device.index or 0)\n#     total_vram_mb = pynvml.nvmlDeviceGetMemoryInfo(handle).total / 1e6\n\n#     max_input_length, max_generation_length = 3350, 3500\n\n#     # --- Load & Pre‑tokenize ---\n#     df = pd.read_excel(validation_path)\n#     df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n#     texts = df[\"input_text\"].tolist()\n#     encodings = tokenizer(texts, truncation=True, max_length=max_input_length,\n#                           return_attention_mask=True, padding=False)\n#     token_lens = [len(ids) for ids in encodings[\"input_ids\"]]\n#     original = df[[\"id\"]].copy()\n#     order = sorted(range(len(texts)), key=lambda i: token_lens[i])\n\n#     collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")\n#     base_model = model.module if hasattr(model, \"module\") else model\n#     base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n#     base_model.to(device)\n\n#     # --- 1) Pre‑compute batches under token_budget ---\n#     raw_batches = []\n#     i = 0\n#     while i < len(order):\n#         batch_idxs, sum_toks = [], 0\n#         while i < len(order) and sum_toks + token_lens[order[i]] <= token_budget:\n#             batch_idxs.append(order[i])\n#             sum_toks += token_lens[order[i]]\n#             i += 1\n#         if not batch_idxs:\n#             batch_idxs = [order[i]]\n#             sum_toks = token_lens[order[i]]\n#             i += 1\n#         raw_batches.append((batch_idxs, sum_toks))\n\n#     # --- 2) Shrink the last three batches to 20% size ---\n#     def chunk_list(lst, pct):\n#         size = max(1, math.ceil(len(lst) * pct))\n#         return [lst[j:j+size] for j in range(0, len(lst), size)]\n\n#     for k in (-1):\n#         if 0 <= k + len(raw_batches) < len(raw_batches):\n#             idxs, _ = raw_batches[k]\n#             shrunk = chunk_list(idxs, 0.2)\n#             raw_batches[k] = [(chunk, sum(token_lens[j] for j in chunk)) for chunk in shrunk]\n\n#     # Flatten into final batch list\n#     final_batches = []\n#     for entry in raw_batches:\n#         if isinstance(entry[0], list):  # single batch\n#             final_batches.append(entry)\n#         else:\n#             # in case we replaced last three with multiple chunks\n#             for chunk_idxs, sum_toks in entry:\n#                 final_batches.append((chunk_idxs, sum_toks))\n\n#     # --- 3) Run generation with CPU/GPU overlap on final_batches ---\n#     records = []\n#     current_budget = token_budget\n#     clean_batches = 0\n\n#     batch_queue = Queue(maxsize=2)\n#     stop_event = threading.Event()\n\n#     def batch_producer():\n#         for batch_idxs, sum_toks in final_batches:\n#             batch_encoding = {k: [encodings[k][j] for j in batch_idxs] for k in encodings}\n#             batch = collator(batch_encoding)\n#             batch_queue.put((batch, batch_idxs, sum_toks))\n#             if stop_event.is_set():\n#                 break\n#         batch_queue.put(None)\n\n#     threading.Thread(target=batch_producer, daemon=True).start()\n\n#     try:\n#         pbar = tqdm(total=len(final_batches))\n#         for item in iter(batch_queue.get, None):\n#             batch, batch_idxs, tok_sum = item\n#             batch = {k: v.to(device) for k, v in batch.items()}\n#             lengths = batch[\"attention_mask\"].sum(1)\n#             gen_max = min(max_generation_length, lengths.max().item() + 256)\n\n#             try:\n#                 torch.cuda.synchronize()\n#                 t0 = time.time()\n#                 out_ids = base_model.generate(\n#                     input_ids=batch[\"input_ids\"],\n#                     attention_mask=batch[\"attention_mask\"],\n#                     max_new_tokens=gen_max,\n#                     use_cache=True,\n#                     num_beams=1\n#                 )\n#                 torch.cuda.synchronize()\n#                 dt = time.time() - t0\n\n#                 mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1e6\n#                 pct = mem / total_vram_mb * 100\n#                 print(f\"[Gen] {len(batch_idxs)} samples | Time: {dt:.1f}s | VRAM: {mem:.0f}MB ({pct:.0f}%)\")\n\n#                 outs = tokenizer.batch_decode(out_ids.cpu(), skip_special_tokens=True)\n#                 for idx, text in zip(batch_idxs, outs):\n#                     records.append({\n#                         \"id\": int(df.at[idx, \"id\"]),\n#                         \"author\": df.at[idx, \"author\"],\n#                         \"output\": text\n#                     })\n#                 clean_batches += 1\n#                 pbar.update(1)\n\n#             except RuntimeError as e:\n#                 if \"out of memory\" in str(e).lower() and len(batch_idxs) > 1:\n#                     torch.cuda.empty_cache(); gc.collect()\n#                     current_budget = max(current_budget // 2, tok_sum)\n#                     clean_batches = 0\n#                     print(f\"⚠️ OOM -> new budget {current_budget}\")\n#                     # split & requeue\n#                     mid = len(batch_idxs) // 2\n#                     for half in (batch_idxs[:mid], batch_idxs[mid:]):\n#                         half_sum = sum(token_lens[j] for j in half)\n#                         batch_queue.put((collator({k: [encodings[k][j] for j in half] for k in encodings}), half, half_sum))\n#                 else:\n#                     print(f\"Error: {e}\"); traceback.print_exc()\n#                     clean_batches = 0\n\n#             # budget bump\n#             if clean_batches >= budget_update_freq and pct < memory_usage_threshold:\n#                 current_budget += budget_update_quota\n#                 print(f\"⚡ budget -> {current_budget}\")\n#                 clean_batches = 0\n\n#         pbar.close()\n\n#     except KeyboardInterrupt:\n#         print(\"⚠️ Interrupted — saving partial results...\")\n\n#     finally:\n#         stop_event.set()\n#         # Save results\n#         try:\n#             df_out = original.merge(pd.DataFrame(records), on=\"id\", how=\"left\")\n#             with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n#                 json.dump(df_out.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n#             print(f\"✅ Saved {len(records)} records\")\n#         except Exception as ex:\n#             print(f\"❌ Save failed: {ex}\")\n#         finally:\n#             pynvml.nvmlShutdown()\n\n#     return records\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.242358Z","iopub.execute_input":"2025-07-24T14:56:01.242572Z","iopub.status.idle":"2025-07-24T14:56:01.264590Z","shell.execute_reply.started":"2025-07-24T14:56:01.242550Z","shell.execute_reply":"2025-07-24T14:56:01.263881Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# !pip install accelerate\n# accelerate config      # answer prompts (choose multi‑GPU)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.265219Z","iopub.execute_input":"2025-07-24T14:56:01.265382Z","iopub.status.idle":"2025-07-24T14:56:01.283300Z","shell.execute_reply.started":"2025-07-24T14:56:01.265368Z","shell.execute_reply":"2025-07-24T14:56:01.282634Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# import os\n# from config import Config  # import your custom Config module\n# import pandas as pd\n# import torch\n# gc = __import__('gc')\n# from tqdm import tqdm\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n# from accelerate import Accelerator\n# import pandas as pd\n# import torch\n# gc = __import__('gc')\n# from tqdm import tqdm\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n# from accelerate import Accelerator\n\n# def generate_predictions(\n#     tokenizer, model, validation_path, output_log_path,\n#     max_new_tokens=1700,\n#     token_budget=100_000,\n#     sort_inputs=True\n# ):\n#     # Initialize Accelerator\n#     accelerator = Accelerator()\n#     # Prepare model & tokenizer\n#     model, tokenizer = accelerator.prepare(model, tokenizer)\n#     model.eval()\n#     unwrapped = accelerator.unwrap_model(model)\n\n#     # 1. Load & prepare DataFrame\n#     df = pd.read_excel(validation_path)\n#     df['input_text'] = df['author'].astype(str) + ' [SEP] ' + df['text_in_msa'].astype(str)\n\n#     # 2. Compute token lengths\n#     max_len = 1024 #unwrapped.config.max_position_embeddings\n#     enc = tokenizer(\n#         df['input_text'].tolist(), return_length=True,\n#         padding=False, truncation=True, max_length=max_len\n#     )\n#     lengths = enc['length']\n\n#     # 3. Build token-budgeted batches\n#     indices = list(range(len(lengths)))\n#     if sort_inputs:\n#         indices.sort(key=lambda i: lengths[i], reverse=True)\n#     batches, curr, curr_sum = [], [], 0\n#     for i in indices:\n#         cost = lengths[i] + max_new_tokens\n#         if curr and curr_sum + cost > token_budget:\n#             batches.append(curr)\n#             curr, curr_sum = [], 0\n#         curr.append(i)\n#         curr_sum += cost\n#     if curr:\n#         batches.append(curr)\n\n#     # 4. Generation loop with instrumentation\n#     records = []\n#     for batch_idx, batch_ids in enumerate(tqdm(batches, desc='Accelerated Batches')):\n#         # Instrumentation\n#         sample_count = len(batch_ids)\n#         batch_token_cost = sum(lengths[i] + max_new_tokens for i in batch_ids)\n#         print(f\"\\n▶ Batch {batch_idx+1}/{len(batches)}: samples={sample_count}, token_cost={batch_token_cost}\")\n#         # Memory before\n#         for dev in range(torch.cuda.device_count()):\n#             alloc = torch.cuda.memory_allocated(dev) / 1024**2\n#             reserved = torch.cuda.memory_reserved(dev) / 1024**2\n#             print(f\"   • GPU {dev}: allocated={alloc:.1f} MiB, reserved={reserved:.1f} MiB\")\n\n#         texts = [df.iloc[i]['input_text'] for i in batch_ids]\n#         inputs = tokenizer(\n#             texts, return_tensors='pt', padding=True,\n#             truncation=True, max_length=max_len\n#         )\n#         inputs = {k: v.to(accelerator.device) for k, v in inputs.items()}\n\n#         with torch.no_grad():\n#             gen_ids = model.generate(\n#                 input_ids=inputs['input_ids'],\n#                 attention_mask=inputs['attention_mask'],\n#                 max_new_tokens=max_new_tokens\n#             )\n#         gen_ids = accelerator.gather(gen_ids)\n#         outs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n#         # Memory after\n#         for dev in range(torch.cuda.device_count()):\n#             alloc = torch.cuda.memory_allocated(dev) / 1024**2\n#             reserved = torch.cuda.memory_reserved(dev) / 1024**2\n#             print(f\"   • GPU {dev}: allocated={alloc:.1f} MiB, reserved={reserved:.1f} MiB\")\n\n#         for idx, out in zip(batch_ids, outs):\n#             row = df.iloc[idx]\n#             records.append({\n#                 'id': int(row['id']),\n#                 'author': row['author'],\n#                 'output': out,\n#                 'ground_truth': row['text_in_author_style']\n#             })\n\n#         # cleanup\n#         del inputs, gen_ids\n#         gc.collect()\n#         torch.cuda.empty_cache()\n\n#     # 5. Save results\n#     import json\n#     with open(output_log_path, 'w', encoding='utf-8') as f:\n#         json.dump(records, f, ensure_ascii=False, indent=2)\n#     print(f\"✅ Generated {len(records)} records and saved to {output_log_path}\")\n#     return records, df\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.284186Z","iopub.execute_input":"2025-07-24T14:56:01.284665Z","iopub.status.idle":"2025-07-24T14:56:01.299515Z","shell.execute_reply.started":"2025-07-24T14:56:01.284638Z","shell.execute_reply":"2025-07-24T14:56:01.298876Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Single-GPU token-budgeted generation (pre-tokenized + efficient CPU/GPU overlap with OOM retry)\nimport threading\nfrom queue import Queue\n\ndef generate_predictions_by_budget(\n    tokenizer,\n    model,\n    validation_path,\n    output_log_path,\n    token_budget=10_000,\n    budget_update_freq=5,\n    budget_update_quota=1000,\n    memory_usage_threshold=80\n):\n    import time, gc, json, traceback\n    import torch\n    import pandas as pd\n    from tqdm import tqdm\n    import pynvml\n    from transformers import DataCollatorWithPadding\n\n    pynvml.nvmlInit()\n    device = next(model.module.parameters() if hasattr(model, \"module\") else model.parameters()).device\n    handle = pynvml.nvmlDeviceGetHandleByIndex(device.index or 0)\n    total_vram_mb = pynvml.nvmlDeviceGetMemoryInfo(handle).total / 1e6\n\n    max_input_length = 3400\n    max_generation_length = 4000\n    max_pos = 100000000 #getattr(tokenizer, 'model_max_length', None) or model.config.max_position_embeddings\n    safe_input_length = min(max_input_length, max_pos)\n    safe_gen_length = min(max_generation_length, max_pos)\n    print(f\"▶ model.max_position_embeddings = {max_pos} , safe_input = {safe_input_length}, safe_generation = {safe_gen_length}\")\n\n    print(\"loading and tokenization start ...\")\n    df = pd.read_excel(validation_path)\n    df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n    texts = df[\"input_text\"].tolist()\n    encodings = tokenizer(\n        texts,\n        truncation=True,\n        max_length=safe_input_length,\n        return_attention_mask=True,\n        padding=False,\n    )\n    token_lens = [len(ids) for ids in encodings[\"input_ids\"]]\n    df[\"token_len\"] = token_lens\n    original = df[[\"id\"]].copy()\n    order = sorted(range(len(texts)), key=lambda i: token_lens[i])\n    collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")\n\n    print(\"loading and tokenization done ... loading the model to gpu...\")\n    base_model = model.module if hasattr(model, \"module\") else model\n    base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n    base_model.to(device)\n\n    records = []\n    current_budget = token_budget\n    clean_batches = 0\n\n    batch_queue = Queue(maxsize=2)\n    stop_event = threading.Event()\n\n    def batch_producer():\n        i = 0\n        while not stop_event.is_set() and i < len(order):\n            batch_idxs, sum_toks = [], 0\n            while i < len(order) and sum_toks + token_lens[order[i]] <= current_budget:\n                batch_idxs.append(order[i])\n                sum_toks += token_lens[order[i]]\n                i += 1\n\n            # Catch edge case: one sample too long\n            if not batch_idxs and i < len(order):\n                batch_idxs = [order[i]]\n                sum_toks = token_lens[order[i]]\n                i += 1\n\n            if batch_idxs:\n                batch_encoding = {k: [encodings[k][j] for j in batch_idxs] for k in encodings}\n                batch = collator(batch_encoding)\n                batch_queue.put((batch, batch_idxs, sum_toks, i))\n            else:\n                break  # nothing left to process\n\n        batch_queue.put(None)  # signal end\n\n    prod_thread = threading.Thread(target=batch_producer)\n    prod_thread.start()\n\n    try:\n        pbar = tqdm(total=len(order))\n        while True :\n            n=0\n            print(f\"retrieve batch {n} ...\")\n            item = batch_queue.get()\n            if item is None:\n                if len(records) >= len(order):\n                    break\n                else:\n                    time.sleep(0.1)\n                    continue\n\n            print(f\"start batch {n} processing...\")\n            \n            batch, batch_idxs, tok_sum, _ = item\n            batch = {k: v.to(device) for k, v in batch.items()}  # FIXED\n            lengths = batch[\"attention_mask\"].sum(1)\n\n            # gen_max = min(safe_gen_length, lengths.max().item() + 256)\n\n            # Precomputed from your EDA:\n            p25_ratio = 0.9291   # 25% quantile of (target/input)\n            p75_ratio = 1.0244   # 75% quantile\n            \n            # Add a small buffer around those:\n            min_frac = max(0.0, p25_ratio - 0.05)   # ~0.88\n            max_frac = p75_ratio + 0.10             # ~1.12\n            \n            # In your batch loop:\n            # batch_input_lens = batch[\"attention_mask\"].sum(dim=1)   # tensor of per-sample\n            max_in = int(lengths.max().item())\n            \n            min_new = int(max_in * min_frac)\n            max_new = int(max_in * max_frac)\n            \n            \n            gen_max = min(safe_gen_length, max_new)\n\n            print(f\" min gen = {min_new} , max_new={max_new} , max_gen = {gen_max}\")\n\n            try:\n                torch.cuda.synchronize()\n                t0 = time.time()\n                out_ids = base_model.generate(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],              \n                    # always use cache for speed\n                    use_cache = True,\n                    num_beams=3,\n                    # style‑aware length windows\n                    min_new_tokens = min_new,\n                    max_new_tokens=gen_max,\n                    # discourage repeats\n                    no_repeat_ngram_size = 3,\n                    repetition_penalty    = 1.1,\n                    # length bias\n                    length_penalty = 0.9,\n                    # batching can still parallelize\n                    pad_token_id = tokenizer.pad_token_id,\n                    eos_token_id = tokenizer.eos_token_id,\n                )\n                torch.cuda.synchronize()\n                dt = time.time() - t0\n\n                mem = pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1e6\n                pct = mem / total_vram_mb * 100\n                print(f\"[Gen] {len(batch_idxs)} samples | Time: {dt:.1f}s | VRAM: {mem:.0f}MB ({pct:.0f}%)\")\n\n                outs = tokenizer.batch_decode(out_ids.cpu(), skip_special_tokens=True)\n                for idx, text in zip(batch_idxs, outs):\n                    records.append({\n                        \"id\": int(df.at[idx, \"id\"]),\n                        \"author\": df.at[idx, \"author\"],\n                        \"output\": text\n                    })\n                clean_batches += 1\n                pbar.update(len(batch_idxs))\n\n            except RuntimeError as e:\n                if \"out of memory\" in str(e).lower() and len(batch_idxs) > 1:\n                    torch.cuda.empty_cache(); gc.collect()\n                    current_budget = max(current_budget // 2, tok_sum)\n                    clean_batches = 0\n                    print(f\"⚠️ OOM -> new budget {current_budget}\")\n                    mid = len(batch_idxs) // 2\n                    for half in (batch_idxs[:mid], batch_idxs[mid:]):\n                        half_enc = {k: batch[k][:len(half)] for k in batch}\n                        half_tok = int(half_enc[\"attention_mask\"].sum())\n                        batch_queue.put((half_enc, half, half_tok, _))\n                else:\n                    print(f\"Error: {e}\"); traceback.print_exc()\n                    clean_batches = 0\n\n            if clean_batches >= budget_update_freq and pct < memory_usage_threshold:\n                current_budget += budget_update_quota\n                print(f\"⚡ budget -> {current_budget}\")\n                clean_batches = 0\n\n            print(f\"complete batch {n} processing...\")\n            n += 1\n\n        pbar.close()\n\n    except KeyboardInterrupt:\n        print(\"⚠️ Interrupted — saving partial results...\")\n\n    finally:\n        stop_event.set()\n        prod_thread.join()\n        try:\n            df_out = original.merge(pd.DataFrame(records), on=\"id\", how=\"left\")\n            with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(df_out.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n            print(f\"✅ Saved {len(records)} records\")\n        except Exception as ex:\n            print(f\"❌ Save failed: {ex}\")\n        finally:\n            pynvml.nvmlShutdown()\n\n    return records\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.300303Z","iopub.execute_input":"2025-07-24T14:56:01.300586Z","iopub.status.idle":"2025-07-24T14:56:01.322908Z","shell.execute_reply.started":"2025-07-24T14:56:01.300564Z","shell.execute_reply":"2025-07-24T14:56:01.322344Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\nimport gc\ngc.collect()","metadata":{"id":"bQCIIwaEEEp9","papermill":{"duration":0.202454,"end_time":"2025-07-14T06:47:15.921644","exception":false,"start_time":"2025-07-14T06:47:15.71919","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.323492Z","iopub.execute_input":"2025-07-24T14:56:01.323653Z","iopub.status.idle":"2025-07-24T14:56:01.697413Z","shell.execute_reply.started":"2025-07-24T14:56:01.323641Z","shell.execute_reply":"2025-07-24T14:56:01.696762Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"Lkh_YWV5Exw8","papermill":{"duration":0.16832,"end_time":"2025-07-14T06:47:16.124774","exception":false,"start_time":"2025-07-14T06:47:15.956454","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.698330Z","iopub.execute_input":"2025-07-24T14:56:01.698631Z","iopub.status.idle":"2025-07-24T14:56:01.903590Z","shell.execute_reply.started":"2025-07-24T14:56:01.698609Z","shell.execute_reply":"2025-07-24T14:56:01.902662Z"}},"outputs":[{"name":"stdout","text":"Thu Jul 24 14:56:01 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0             25W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# !watch -n 0.5 nvidia-smi\ntorch.cuda.is_available()  # Should be True\ntorch.cuda.current_device()\ntorch.cuda.get_device_name()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.904880Z","iopub.execute_input":"2025-07-24T14:56:01.905206Z","iopub.status.idle":"2025-07-24T14:56:01.911516Z","shell.execute_reply.started":"2025-07-24T14:56:01.905173Z","shell.execute_reply":"2025-07-24T14:56:01.910892Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'Tesla P100-PCIE-16GB'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"## gpu usage rate  logging test\n\n# import torch\n# import time\n\n# torch.cuda.empty_cache()\n# a = torch.randn(10000, 10000, device=\"cuda\")\n\n# # Warm-up\n# for _ in range(5):\n#     a = torch.matmul(a, a)\n\n# # Now measure and keep GPU busy for a while\n# start = time.time()\n\n# for _ in range(100):\n#     a = torch.matmul(a, a)\n#     torch.cuda.synchronize()  # make it block until GPU is done\n\n# end = time.time()\n# print(f\"Time taken: {end - start:.2f} seconds\")\n\n\n\n# import torch\n# import pynvml\n# import time\n\n# # Start NVML\n# pynvml.nvmlInit()\n# handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n\n# # Heavy GPU computation\n# a = torch.randn(8192, 8192, device=\"cuda\")\n# b = torch.randn(8192, 8192, device=\"cuda\")\n\n# # Run and monitor in real-time\n# for _ in range(10):\n#     c = torch.matmul(a, b)  # triggers real GPU usage\n#     util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n#     print(f\"GPU Usage: {util.gpu}% | Memory Usage: {util.memory}%\")\n#     time.sleep(0.5)\n\n# pynvml.nvmlShutdown()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.912287Z","iopub.execute_input":"2025-07-24T14:56:01.912529Z","iopub.status.idle":"2025-07-24T14:56:01.927164Z","shell.execute_reply.started":"2025-07-24T14:56:01.912508Z","shell.execute_reply":"2025-07-24T14:56:01.926547Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# ---------------- Cell 4: run the pipeline ----------------\n\n# if __name__ == '__main__':\n# adjust these paths as needed\ntest_file = \"./data/PublicDataFinalPhaseTask1.xlsx\"\nlog_file = f\"models/fine_tuned_model/results/{safe_model_name(MODEL_NAME)}/generation_log_test.json\"\noutput_base_dir = \"analysis/fine_tuned_model\"\n\nos.makedirs(os.path.dirname(log_file), exist_ok=True)\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n\ntokenizer, model = load_model(MODEL_NAME)\n\n# base_model_path = \"models/base_models/facebook/arabart\"\n# lora_path       = \"models/fine_tuned_model/model_weights/lora_moussa_arabart\"\n\n# tokenizer, model = load_lora_model(base_model_path, lora_path)\n","metadata":{"id":"FzfhVKAGXa8M","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:01.928027Z","iopub.execute_input":"2025-07-24T14:56:01.928249Z","iopub.status.idle":"2025-07-24T14:56:04.844657Z","shell.execute_reply.started":"2025-07-24T14:56:01.928230Z","shell.execute_reply":"2025-07-24T14:56:04.844039Z"}},"outputs":[{"name":"stdout","text":"Loading model from models/fine_tuned_model/model_weights/UBC-NLP_AraT5v2-base-1024\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# generate_predictions(tokenizer, model, test_file, log_file, batch_size=128)\nout_df = generate_predictions_by_budget(tokenizer, model, test_file, log_file, token_budget=15_000)","metadata":{"id":"FzfhVKAGXa8M","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T14:56:04.845482Z","iopub.execute_input":"2025-07-24T14:56:04.845698Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"▶ model.max_position_embeddings = 100000000 , safe_input = 3400, safe_generation = 4000\nloading and tokenization start ...\nloading and tokenization done ... loading the model to gpu...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/8413 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"retrieve batch 0 ...\nstart batch 0 processing...\n min gen = 201 , max_new=257 , max_gen = 257\n[Gen] 87 samples | Time: 68.5s | VRAM: 13465MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 87/8413 [01:10<1:52:17,  1.24it/s]","output_type":"stream"},{"name":"stdout","text":"complete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 240 , max_new=308 , max_gen = 308\n[Gen] 59 samples | Time: 65.6s | VRAM: 13465MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 146/8413 [02:17<2:13:40,  1.03it/s]","output_type":"stream"},{"name":"stdout","text":"complete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 261 , max_new=333 , max_gen = 333\n[Gen] 52 samples | Time: 67.6s | VRAM: 13467MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 198/8413 [03:26<2:31:30,  1.11s/it]","output_type":"stream"},{"name":"stdout","text":"complete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 276 , max_new=354 , max_gen = 354\n[Gen] 48 samples | Time: 65.0s | VRAM: 13467MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 246/8413 [04:32<2:43:13,  1.20s/it]","output_type":"stream"},{"name":"stdout","text":"complete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 288 , max_new=368 , max_gen = 368\n[Gen] 46 samples | Time: 72.3s | VRAM: 13467MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 292/8413 [05:46<2:59:36,  1.33s/it]","output_type":"stream"},{"name":"stdout","text":"⚡ budget -> 16000\ncomplete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 296 , max_new=378 , max_gen = 378\n[Gen] 45 samples | Time: 73.5s | VRAM: 13467MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 337/8413 [07:01<3:12:29,  1.43s/it]","output_type":"stream"},{"name":"stdout","text":"complete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 305 , max_new=390 , max_gen = 390\n[Gen] 43 samples | Time: 67.1s | VRAM: 13467MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  5%|▍         | 380/8413 [08:09<3:17:46,  1.48s/it]","output_type":"stream"},{"name":"stdout","text":"complete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 310 , max_new=396 , max_gen = 396\n[Gen] 42 samples | Time: 63.6s | VRAM: 13467MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 422/8413 [09:14<3:19:24,  1.50s/it]","output_type":"stream"},{"name":"stdout","text":"complete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 314 , max_new=402 , max_gen = 402\n[Gen] 45 samples | Time: 81.8s | VRAM: 13467MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 467/8413 [10:38<3:32:40,  1.61s/it]","output_type":"stream"},{"name":"stdout","text":"complete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 320 , max_new=410 , max_gen = 410\n[Gen] 44 samples | Time: 71.8s | VRAM: 13467MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 511/8413 [11:51<3:33:46,  1.62s/it]","output_type":"stream"},{"name":"stdout","text":"⚡ budget -> 17000\ncomplete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 325 , max_new=416 , max_gen = 416\n[Gen] 43 samples | Time: 68.6s | VRAM: 13467MB (78%)\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 554/8413 [13:01<3:32:47,  1.62s/it]","output_type":"stream"},{"name":"stdout","text":"complete batch 0 processing...\nretrieve batch 0 ...\nstart batch 0 processing...\n min gen = 328 , max_new=420 , max_gen = 420\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def print_first_two_results(records):\n    print(\"\\n📄 First Two Output Records:\\n\" + \"-\"*30)\n    for i, rec in enumerate(records[:2]):\n        print(f\"\\n📝 Record #{i+1}\")\n        for key, value in rec.items():\n            print(f\"{key}: {value}\")\n\n\n# print_first_two_results(out_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# records, df = generate_predictions(tokenizer, model, validation_file, log_file)\n# If saving separately is needed\n# save_predictions_to_file(records, df, log_file)\n\nrun_dir = create_run_dir(output_base_dir, MODEL_NAME)\nos.system(f\"cp {log_file} {run_dir}/\")\nprint(f\"\\n✅ Pipeline done. Logs in: {run_dir} \\n LOG FILE: {log_file}\\n\")","metadata":{"id":"FzfhVKAGXa8M","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Function to regenerate missing samples one-by-one with stale‐assert flush\ndef regenerate_missing(\n    test_file,\n    log_file,\n    tokenizer,\n    model,\n    device,\n    max_input_length=3350,\n    max_generation_length=3500\n):\n    import json, time, torch, pandas as pd\n    from tqdm import tqdm\n\n    max_pos = model.config.max_position_embeddings\n    # min(max_input_length, max_pos)\n    gen_max = min(max_generation_length, max_pos)\n    print(f\"▶ model.max_position_embeddings = {max_pos}\")\n\n    df_test = pd.read_excel(test_file)[['id','author','text_in_msa']].astype({'id':int})\n    try:\n        existing = pd.DataFrame(json.load(open(log_file,'r',encoding='utf-8')))\n    except FileNotFoundError:\n        existing = pd.DataFrame(columns=['id','author','output'])\n    existing['id'] = existing['id'].astype(int)\n\n    merged = df_test.merge(existing, on=['id','author'], how='left')\n    missing = merged[ merged['output'].isna() | (merged['output'].str.strip()=='') ]\n    print(f\"🔍 {len(missing)} missing samples to regenerate\")\n\n    new_records = []\n    for _, row in tqdm(missing.iterrows(), total=len(missing), desc='Regenerating'):\n        # 1) Clear any *stale* device‐side assert\n        try:\n            torch.cuda.synchronize()\n        except Exception:\n            pass\n        torch.cuda.empty_cache()\n\n        text = f\"{row['author']} [SEP] {row['text_in_msa']}\"\n        enc = tokenizer(\n            text,\n            return_tensors='pt',\n            truncation=True,\n            max_length=min(max_input_length, max_pos),\n            padding=False\n        )\n\n        try:\n            # 2) Move to GPU and generate\n            enc = {k: v.to(device) for k,v in enc.items()}\n            input_len = enc['input_ids'].size(1)\n            # gen_max   = min(max_generation_length, max_pos - input_len)\n            gen_max = min(max_generation_length, max_pos)\n            if gen_max < 1:\n                raise RuntimeError(f\"Input length {input_len} ≥ max_pos {max_pos}\")\n\n            with torch.inference_mode():\n                torch.cuda.synchronize()\n                t0 = time.time()\n                out_ids = model.generate(\n                    input_ids=enc['input_ids'],\n                    attention_mask=enc.get('attention_mask'),\n                    max_new_tokens=gen_max,\n                    use_cache=True,\n                    num_beams=1,\n                    pad_token_id=tokenizer.pad_token_id\n                )\n                torch.cuda.synchronize()\n                dt = time.time() - t0\n\n            decoded = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n            print(f\"✓ ID {row['id']} ({input_len}+{gen_max})→{out_ids.size(1)} in {dt:.1f}s\")\n\n        except Exception as e:\n            print(f\"❌ ID {row['id']} failed: {e}\")\n            decoded = \"\"\n\n        new_records.append({'id':int(row['id']), 'author':row['author'], 'output':decoded})\n\n    # Merge back & save\n    all_df = pd.concat([existing, pd.DataFrame(new_records)], ignore_index=True)\n    out_df = df_test.merge(all_df[['id','author','output']], on=['id','author'], how='left')\n\n    with open(log_file,'w',encoding='utf-8') as f:\n        json.dump(out_df.to_dict(orient='records'), f, ensure_ascii=False, indent=2)\n\n    print(f\"✅ Regeneration complete. {len(out_df)} rows in log.\")\n    return out_df\n\n# Run it\nout_df = regenerate_missing(\n    test_file=test_file,\n    log_file=log_file,\n    tokenizer=tokenizer,\n    model=model,\n    device=next(model.parameters()).device,\n    max_input_length=3500,\n    max_generation_length=4000\n)\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Display a link to the copied log file in the run_dir\nlog_file_in_run_dir = os.path.join(run_dir, os.path.basename(log_file))\nFileLink(log_file_in_run_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import gc\n\n# def flush_gpu_memory():\n#     import torch, gc\n#     gc.collect()\n#     torch.cuda.empty_cache()\n\n#     # Kill CUDA tensors\n#     for obj in gc.get_objects():\n#         try:\n#             if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n#                 if obj.is_cuda:\n#                     del obj\n#         except Exception:\n#             pass\n\n#     gc.collect()\n#     torch.cuda.empty_cache()\n\n#     # Clear CUDA context if possible (dangerous but useful in some cases)\n#     try:\n#         torch.cuda.ipc_collect()\n#     except:\n#         pass\n\n\n# flush_gpu_memory()\n\n\n# ######\n\n# # Step 1: Delete large objects\n# # del model, tokenizer\n\n# # # Step 2: If model is still in GPU, move to CPU first\n# # model.to(\"cpu\")  \n# # # then \n# # del model\n\n# # Step 3: Run GC and empty cache\n# import gc, torch\n# gc.collect()\n# torch.cuda.empty_cache()\n\n# # Step 4: Confirm\n# print(f\"Allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n# print(f\"Cached:    {torch.cuda.memory_reserved() / 1e6:.1f} MB\")\n\n\n# # !nvidia-smi --gpu-reset -i 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Function to regenerate missing samples one-by-one with the same budgeted generation settings\n# def regenerate_missing(test_file, log_file, tokenizer, model, device,\n#                        max_input_length=3350, max_generation_length=3500,\n#                        token_budget=30000):\n#     import json, time, torch, pandas as pd\n#     from tqdm import tqdm\n\n#     # Load test file and existing log\n#     df_test = pd.read_excel(test_file)\n#     df_test = df_test[[\"id\", \"author\", \"text_in_msa\"]]\n#     df_test[\"id\"] = df_test[\"id\"].astype(int)\n#     try:\n#         with open(log_file, \"r\", encoding=\"utf-8\") as f:\n#             existing = pd.DataFrame(json.load(f))\n#     except FileNotFoundError:\n#         existing = pd.DataFrame(columns=[\"id\", \"author\", \"output\"]);\n\n#     existing[\"id\"] = existing[\"id\"].astype(int)\n#     # Identify missing IDs (no output or empty)\n#     merged = df_test.merge(existing, on=[\"id\", \"author\"], how=\"left\")\n#     missing = merged[merged[\"output\"].isna() | (merged[\"output\"].str.strip() == \"\")]\n#     missing_ids = missing[\"id\"].tolist()\n#     print(f\"🔍 {len(missing_ids)} missing samples to regenerate\")\n\n#     # Regenerate one-by-one\n#     new_records = []\n#     for _, row in tqdm(missing.iterrows(), total=len(missing), desc=\"Regenerating missing\"):\n#         input_text = row[\"author\"] + \" [SEP] \" + row[\"text_in_msa\"]\n#         enc = tokenizer(\n#             input_text,\n#             truncation=True,\n#             max_length=max_input_length,\n#             return_tensors=\"pt\",\n#             padding=True\n#         ).to(device)\n#         try:\n#             with torch.no_grad():\n#                 torch.cuda.synchronize()\n#                 t0 = time.time()\n#                 out_ids = model.generate(\n#                     input_ids=enc[\"input_ids\"],\n#                     attention_mask=enc[\"attention_mask\"],\n#                     max_new_tokens=max_generation_length,\n#                     use_cache=True,\n#                     num_beams=1\n#                 )\n#                 torch.cuda.synchronize()\n#                 dt = time.time() - t0\n#             decoded = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n#             print(f\"✓ ID {row['id']} generated in {dt:.1f}s\")\n#         except Exception as e:\n#             print(f\"❌ ID {row['id']} failed: {e}\")\n#             decoded = \"\"\n#         new_records.append({\"id\": int(row[\"id\"]), \"author\": row[\"author\"], \"output\": decoded})\n\n#     # Combine and reorder\n#     all_df = pd.concat([existing, pd.DataFrame(new_records)], ignore_index=True)\n#     all_df = df_test.merge(all_df, on=[\"id\", \"author\"], how=\"left\")\n#     # Save back to log_file in original test_file order\n#     records = all_df[[\"id\", \"author\", \"output\"]].to_dict(orient=\"records\")\n#     with open(log_file, \"w\", encoding=\"utf-8\") as f:\n#         json.dump(records, f, ensure_ascii=False, indent=2)\n#     print(f\"✅ Regeneration complete. Log updated with {len(records)} records.\")\n\n# # 2) Regenerate missing samples\n# regenerate_missing(\n#     test_file=test_file,\n#     log_file=log_file,\n#     tokenizer=tokenizer,\n#     model=model,\n#     device=next(model.parameters()).device,\n#     max_input_length=3350,\n#     max_generation_length=3500,\n#     token_budget=30_000\n# )\n\n# # 3) Archive log file\n# os.system(f\"cp {log_file} {run_dir}/\")\n# print(f\" ✅ Pipeline done. Logs in: {run_dir} LOG FILE: {log_file}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # For Colab or IPython environments\n# import os\n# os._exit(00)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}