{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12342718,"sourceType":"datasetVersion","datasetId":7780982},{"sourceId":12432038,"sourceType":"datasetVersion","datasetId":7841825},{"sourceId":12470700,"sourceType":"datasetVersion","datasetId":7867626}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":128.955805,"end_time":"2025-07-14T06:47:18.107738","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-14T06:45:09.151933","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"df00333d-fbfa-483f-8a56-4372415c5b31","cell_type":"code","source":"import os\n\npath = \"/kaggle/working\"\nis_empty = (len(os.listdir(path)) <= 3)\n\n!ls /kaggle/working/\n\nMODEL_WEIGHTS_SRC =\"/kaggle/input/agemagician-mlong-t5-tglobal-large-model/agemagician_mlong-t5-tglobal-large/agemagician_mlong-t5-tglobal-large\"\n\nMODEL_NAME = \"agemagician_mlong-t5-tglobal-large\"\n# MODEL_NAME = \"UBC-NLP/AraT5v2-base-1024\" #\"agemagician_mlong-t5-tglobal-large\" #\"facebook_mbart-large-50-many-to-many-mmt\" # \"UBC-NLP_AraT5-base\" #\"google/mt5-small\"\n\nMODEL_WEIGHTS_DEST_DIR =f\"/kaggle/working/models/fine_tuned_model/model_weights/{MODEL_NAME}\"\n\n\n\n# !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n\nif is_empty:\n    print(f\"✅ '{path}' is empty. copying project strcuture into it ...\")\n    !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n    !mkdir -p {MODEL_WEIGHTS_DEST_DIR}\n    !cp -r {MODEL_WEIGHTS_SRC} {MODEL_WEIGHTS_DEST_DIR}\nelse:\n    print(f\"❌ '{path}' is not empty — contains {len(os.listdir(path))} items.\")\n\n\n\n# Change to project directory\n%cd /kaggle/working/ \n\n# Confirm\n!ls","metadata":{"papermill":{"duration":19.447473,"end_time":"2025-07-14T06:47:03.311976","exception":false,"start_time":"2025-07-14T06:46:43.864503","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:35:37.541886Z","iopub.execute_input":"2025-07-14T23:35:37.542312Z","iopub.status.idle":"2025-07-14T23:35:37.784532Z","shell.execute_reply.started":"2025-07-14T23:35:37.542286Z","shell.execute_reply":"2025-07-14T23:35:37.783704Z"}},"outputs":[{"name":"stdout","text":"analysis   data        __init__.py  prompts\t run_pipeline.py  utils\nconfig.py  evaluation  models\t    __pycache__  state.db\n❌ '/kaggle/working' is not empty — contains 12 items.\n/kaggle/working\nanalysis   data        __init__.py  prompts\t run_pipeline.py  utils\nconfig.py  evaluation  models\t    __pycache__  state.db\n","output_type":"stream"}],"execution_count":1},{"id":"1d77799b-263e-4e36-b283-d3b4bf56f206","cell_type":"code","source":"!ls -R {MODEL_WEIGHTS_DEST_DIR}","metadata":{"papermill":{"duration":0.154964,"end_time":"2025-07-14T06:47:03.499832","exception":false,"start_time":"2025-07-14T06:47:03.344868","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:35:37.786449Z","iopub.execute_input":"2025-07-14T23:35:37.786663Z","iopub.status.idle":"2025-07-14T23:35:37.904171Z","shell.execute_reply.started":"2025-07-14T23:35:37.786641Z","shell.execute_reply":"2025-07-14T23:35:37.903340Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/models/fine_tuned_model/model_weights/facebook_mbart-large-50-many-to-many-mmt/:\nconfig.json\t\tsentencepiece.bpe.model  training_args.bin\ngeneration_config.json\tspecial_tokens_map.json\nmodel.safetensors\ttokenizer_config.json\n","output_type":"stream"}],"execution_count":2},{"id":"fTyhOO2v7ssz","cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":325},"id":"fTyhOO2v7ssz","outputId":"7eecf25f-5840-4ed0-c27a-fa479b3c0838","papermill":{"duration":0.038757,"end_time":"2025-07-14T06:46:43.831489","exception":false,"start_time":"2025-07-14T06:46:43.792732","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:35:37.905241Z","iopub.execute_input":"2025-07-14T23:35:37.905465Z","iopub.status.idle":"2025-07-14T23:35:37.909346Z","shell.execute_reply.started":"2025-07-14T23:35:37.905441Z","shell.execute_reply":"2025-07-14T23:35:37.908657Z"}},"outputs":[],"execution_count":3},{"id":"cab9aba0","cell_type":"code","source":"# project_dir = \"/content/drive/MyDrive/AraGenEval2025\"\n# os.chdir(project_dir)\n# print(f\"✅ Current directory: {os.getcwd()}\")\n","metadata":{"id":"cab9aba0","papermill":{"duration":0.041911,"end_time":"2025-07-14T06:47:15.232757","exception":false,"start_time":"2025-07-14T06:47:15.190846","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:35:37.910147Z","iopub.execute_input":"2025-07-14T23:35:37.910614Z","iopub.status.idle":"2025-07-14T23:35:37.933861Z","shell.execute_reply.started":"2025-07-14T23:35:37.910597Z","shell.execute_reply":"2025-07-14T23:35:37.933099Z"}},"outputs":[],"execution_count":4},{"id":"559ddd13","cell_type":"code","source":"!pip install transformers[sentencepiece] torch pandas openpyxl psutil python-dotenv  --quiet","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"559ddd13","outputId":"7921c04f-9a42-44dc-e548-5c1805145485","papermill":{"duration":90.123476,"end_time":"2025-07-14T06:46:43.758212","exception":false,"start_time":"2025-07-14T06:45:13.634736","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:35:37.934590Z","iopub.execute_input":"2025-07-14T23:35:37.934790Z","iopub.status.idle":"2025-07-14T23:37:01.612943Z","shell.execute_reply.started":"2025-07-14T23:35:37.934756Z","shell.execute_reply":"2025-07-14T23:37:01.612077Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":5},{"id":"b1rI7F4dlowA","cell_type":"code","source":"# Ensure PyTorch can expand allocations if needed\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"id":"b1rI7F4dlowA","papermill":{"duration":0.039224,"end_time":"2025-07-14T06:47:03.708322","exception":false,"start_time":"2025-07-14T06:47:03.669098","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:37:01.620154Z","iopub.execute_input":"2025-07-14T23:37:01.620356Z","iopub.status.idle":"2025-07-14T23:37:01.639575Z","shell.execute_reply.started":"2025-07-14T23:37:01.620341Z","shell.execute_reply":"2025-07-14T23:37:01.639103Z"}},"outputs":[],"execution_count":7},{"id":"390a0abf","cell_type":"code","source":"\nimport os\nimport json\nimport dotenv\nimport torch\nimport pandas as pd\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport gc\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfrom config import Config\n\n\n# project root in PYTHONPATH\nimport sys\nproject_root = os.getcwd()\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"✅ Using device: {device}\")\n\n# Device & parallel setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"✅ Using device: {device}, count: {torch.cuda.device_count()} GPUs\")","metadata":{"id":"390a0abf","papermill":{"duration":11.413602,"end_time":"2025-07-14T06:47:15.156088","exception":false,"start_time":"2025-07-14T06:47:03.742486","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:37:01.640195Z","iopub.execute_input":"2025-07-14T23:37:01.640368Z","iopub.status.idle":"2025-07-14T23:37:12.552879Z","shell.execute_reply.started":"2025-07-14T23:37:01.640345Z","shell.execute_reply":"2025-07-14T23:37:12.552183Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda, count: 2 GPUs\n","output_type":"stream"}],"execution_count":8},{"id":"d797f524","cell_type":"code","source":"\ndef safe_model_name(model_name):\n    return model_name.replace(\"/\", \"_\")\n\n# archive & inspect\ndef create_run_dir(base, name):\n    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    d  = os.path.join(base, safe_model_name(name), f\"run_{ts}\")\n    os.makedirs(d, exist_ok=True)\n    return d\n\ndef run_evaluation():\n  pass\n    # os.system(\"python utils/prepare_metrics_log.py\")\n    # os.system(\"python evaluation/evaluate.py\")\n\n\ndef archive_results(run_dir, files_to_archive):\n    for fpath in files_to_archive:\n        if os.path.exists(fpath):\n            dest = os.path.join(run_dir, os.path.basename(fpath))\n            os.system(f\"cp {fpath} {dest}\")\n","metadata":{"id":"d797f524","papermill":{"duration":0.040399,"end_time":"2025-07-14T06:47:15.604797","exception":false,"start_time":"2025-07-14T06:47:15.564398","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:37:12.553575Z","iopub.execute_input":"2025-07-14T23:37:12.553903Z","iopub.status.idle":"2025-07-14T23:37:12.559139Z","shell.execute_reply.started":"2025-07-14T23:37:12.553887Z","shell.execute_reply":"2025-07-14T23:37:12.558338Z"}},"outputs":[],"execution_count":9},{"id":"d09aa414","cell_type":"code","source":"def load_model(model_name: str):\n    secure = safe_model_name(model_name)\n    model_path = os.path.join(\"models/fine_tuned_model/model_weights\", secure)\n    print(f\"Loading model from {model_path}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, legacy=False)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n    # mBART language tags\n    if \"mbart\" in model_name.lower():\n        tokenizer.src_lang = tokenizer.tgt_lang = \"ar_AR\"\n    # wrap for multi‑GPU\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n    model.to(device).eval()\n    return tokenizer, model  \n","metadata":{"id":"d09aa414","papermill":{"duration":0.04142,"end_time":"2025-07-14T06:47:15.530296","exception":false,"start_time":"2025-07-14T06:47:15.488876","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:37:12.559867Z","iopub.execute_input":"2025-07-14T23:37:12.560197Z","iopub.status.idle":"2025-07-14T23:37:12.579337Z","shell.execute_reply.started":"2025-07-14T23:37:12.560170Z","shell.execute_reply":"2025-07-14T23:37:12.578802Z"}},"outputs":[],"execution_count":10},{"id":"5be418a3-d808-4d48-91f3-81a138163549","cell_type":"code","source":"### batched inference function (updated for DataParallel)\ndef generate_predictions(tokenizer, model, validation_path, output_log_path, batch_size=8):\n    # 1. Load & prepare\n    df = pd.read_excel(validation_path)\n    df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n\n    # Handle DataParallel wrapping\n    base_model = model.module if hasattr(model, \"module\") else model\n    base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n    assert base_model.config.eos_token_id is not None\n\n    records = []\n    # 2. DataLoader for batching\n    from torch.utils.data import DataLoader, Dataset\n\n    class TextDataset(Dataset):\n        def __init__(self, texts): self.texts = texts\n        def __len__(self): return len(self.texts)\n        def __getitem__(self, idx): return self.texts[idx]\n\n    def collate_fn(batch_texts):\n        return tokenizer(\n            batch_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,    # preserve full inputs\n            max_length=1300\n        )\n\n    loader = DataLoader(\n        TextDataset(df[\"input_text\"].tolist()),\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=4,\n        pin_memory=True\n    )\n\n    # 3. Generate in batches\n    for i, batch in enumerate(tqdm(loader, desc=\"Batches\")):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.inference_mode():\n            gen_ids = base_model.generate(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                num_beams=2,\n                early_stopping=True,\n                do_sample=False,\n                max_length=3000\n                max_new_tokens=1700\n            )\n        outs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n        # accumulate\n        start_idx = i * batch_size\n        for j, out in enumerate(outs):\n            row = df.iloc[start_idx + j]\n            records.append({\n                \"id\": int(row[\"id\"]),\n                \"author\": row[\"author\"],\n                \"output\": out,\n                \"ground_truth\": row[\"text_in_author_style\"]\n            })\n\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    # 4. Save\n    with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(records, f, ensure_ascii=False, indent=2)\n    print(f\"✅ Generated {len(records)} records to {output_log_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:37:12.579905Z","iopub.execute_input":"2025-07-14T23:37:12.580125Z","iopub.status.idle":"2025-07-14T23:37:12.606198Z","shell.execute_reply.started":"2025-07-14T23:37:12.580103Z","shell.execute_reply":"2025-07-14T23:37:12.605669Z"}},"outputs":[],"execution_count":11},{"id":"bQCIIwaEEEp9","cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\nimport gc\ngc.collect()","metadata":{"id":"bQCIIwaEEEp9","papermill":{"duration":0.202454,"end_time":"2025-07-14T06:47:15.921644","exception":false,"start_time":"2025-07-14T06:47:15.719190","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:37:12.606844Z","iopub.execute_input":"2025-07-14T23:37:12.607100Z","iopub.status.idle":"2025-07-14T23:37:12.756611Z","shell.execute_reply.started":"2025-07-14T23:37:12.607078Z","shell.execute_reply":"2025-07-14T23:37:12.755938Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"41"},"metadata":{}}],"execution_count":12},{"id":"Lkh_YWV5Exw8","cell_type":"code","source":"!nvidia-smi","metadata":{"id":"Lkh_YWV5Exw8","papermill":{"duration":0.16832,"end_time":"2025-07-14T06:47:16.124774","exception":false,"start_time":"2025-07-14T06:47:15.956454","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:37:12.757337Z","iopub.execute_input":"2025-07-14T23:37:12.757603Z","iopub.status.idle":"2025-07-14T23:37:13.008676Z","shell.execute_reply.started":"2025-07-14T23:37:12.757581Z","shell.execute_reply":"2025-07-14T23:37:13.007912Z"}},"outputs":[{"name":"stdout","text":"Mon Jul 14 23:37:12 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   43C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":13},{"id":"FzfhVKAGXa8M","cell_type":"code","source":"# Cell 4: run the pipeline\n# adjust these paths as needed\nMODEL_NAME      = \"agemagician_mlong-t5-tglobal-large\"\nvalidation_file = Config.VAL_FILE\nlog_file        = f\"models/fine_tuned_model/results/{safe_model_name(MODEL_NAME)}/generation_log.json\"\noutput_base_dir = \"analysis/fine_tuned_model\"\n\nos.makedirs(os.path.dirname(log_file), exist_ok=True)\n\ntokenizer, model = load_model(MODEL_NAME)\ngenerate_predictions(tokenizer, model, validation_file, log_file, batch_size=12)\n\n\n\nrun_dir = create_run_dir(output_base_dir, MODEL_NAME)\nos.system(f\"cp {log_file} {run_dir}/\")\nprint(f\"\\n✅ Pipeline done. Logs in: {run_dir} \\n LOG FILE: {log_file}\\n\")\n# !nvidia-smi","metadata":{"id":"FzfhVKAGXa8M","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T23:37:13.009866Z","iopub.execute_input":"2025-07-14T23:37:13.010295Z","iopub.status.idle":"2025-07-15T06:30:37.416075Z","shell.execute_reply.started":"2025-07-14T23:37:13.010260Z","shell.execute_reply":"2025-07-15T06:30:37.415201Z"}},"outputs":[{"name":"stdout","text":"Loading model from models/fine_tuned_model/model_weights/facebook_mbart-large-50-many-to-many-mmt\n","output_type":"stream"},{"name":"stderr","text":"2025-07-14 23:37:22.573478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752536242.907644      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752536243.008725      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nBatches:   0%|          | 0/520 [00:00<?, ?it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\nBatches: 100%|██████████| 520/520 [6:52:56<00:00, 47.65s/it]  \n","output_type":"stream"},{"name":"stdout","text":"✅ Generated 4157 records to models/fine_tuned_model/results/facebook_mbart-large-50-many-to-many-mmt/generation_log.json\n\n✅ Pipeline done. Logs in: analysis/fine_tuned_model/facebook_mbart-large-50-many-to-many-mmt/run_20250715_063037 \n LOG FILE: models/fine_tuned_model/results/facebook_mbart-large-50-many-to-many-mmt/generation_log.json\n\n","output_type":"stream"}],"execution_count":14}]}