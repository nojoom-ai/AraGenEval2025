{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":12410724,"sourceType":"datasetVersion","datasetId":7826999}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Setup","metadata":{}},{"cell_type":"code","source":"import os\n\npath = \"/kaggle/working\"\nis_empty = (len(os.listdir(path)) <= 3)\n\n!ls /kaggle/working/\n\n# !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n\nif is_empty:\n    print(f\"✅ '{path}' is empty. copying project strcuture into it ...\")\n    !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\nelse:\n    print(f\"❌ '{path}' is not empty — contains {len(os.listdir(path))} items.\")\n\n# Change to project directory\n%cd /kaggle/working/ \n\n# Confirm\n!ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Navigate to project folder\n# %cd /content/drive/MyDrive/AraGenEval2025\n# !ls","metadata":{"id":"snkePh0ID2ad","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add project root to Python path\nimport os\nimport sys\nsys.path.insert(0, os.getcwd())\nprint(\"✅ Project root added to sys.path:\", os.getcwd())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.41.2 datasets==2.19.1 torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 scikit-learn wandb python-dotenv evaluate --quiet\n!pip install -U peft==0.11.1 --quiet","metadata":{"id":"ZOea51qwDdX9","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import Dataset\nimport evaluate\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, f1_score  \nimport pandas as pd\nimport numpy as np\n","metadata":{"id":"vfNP0O6mDdYE","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update these paths with your actual dataset locations on Drive\ntrain_dataset_path ='data/AuthorshipStyleTransferTrain.xlsx'\nval_dataset_path =  'data/AuthorshipStyleTransferVal.xlsx'","metadata":{"id":"bzQpb7SADdYI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport os\n\nproject_root = os.getcwd()  # Should be /content/drive/MyDrive/AraGenEval2025\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\nprint(f\"Project root added to sys.path: {project_root}\")","metadata":{"id":"Do-3CdOID5G9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from config import Config\n\nprint(Config.TRAIN_FILE)","metadata":{"id":"yiLsK5teEGAM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"id":"6llJHQ9cDdYH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport gc\n\ndef load_and_sample_dataset(\n    file_path,\n    sample_mode=\"all\",    # Options: \"all\", \"random\", \"stratified\"\n    sample_size=None,     # For \"random\"/\"stratified\": float (proportion) or int (absolute count)\n    random_state=42\n):\n    \"\"\"\n    Load dataset from Excel and apply sampling.\n\n    Args:\n        file_path (str): Path to dataset file.\n        sample_mode (str): \"all\", \"random\", or \"stratified\".\n        sample_size (float|int): Proportion (0-1) or count for sampling.\n        random_state (int): Random seed for reproducibility.\n\n    Returns:\n        pd.DataFrame: Sampled dataset.\n    \"\"\"\n    print(f\"📂 Loading dataset from: {file_path}\")\n    df = pd.read_excel(file_path, engine='openpyxl')\n\n    if sample_mode == \"all\":\n        sampled_df = df.copy()\n        print(f\"✅ Loaded full dataset with {len(sampled_df)} samples.\")\n\n    elif sample_mode == \"random\":\n        if sample_size is None:\n            raise ValueError(\"For 'random' mode, SAMPLE_SIZE must be set.\")\n        sampled_df = df.sample(\n            n=sample_size if isinstance(sample_size, int) else int(len(df) * sample_size),\n            random_state=random_state\n        )\n        print(f\"✅ Randomly sampled {len(sampled_df)} samples.\")\n\n    elif sample_mode == \"stratified\":\n        if sample_size is None or not (0 < sample_size < 1):\n            raise ValueError(\"For 'stratified' mode, SAMPLE_SIZE must be a proportion between 0 and 1.\")\n        sampled_df, _ = train_test_split(\n            df,\n            train_size=sample_size,\n            stratify=df['author'],\n            random_state=random_state\n        )\n        print(f\"✅ Stratified sampled {len(sampled_df)} samples (author distribution preserved).\")\n\n    else:\n        raise ValueError(f\"Invalid sample_mode: {sample_mode}\")\n\n    del df\n    gc.collect()\n\n    return sampled_df\n","metadata":{"id":"pxre7AjAEQP8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# model_name = \"megabot131/m-e5-large-toxic-classification-lora\"   # \"microsoft/Multilingual-MiniLM-L12-H384\" #\"CAMeL-Lab/bert-base-arabic-camelbert-ca\" \"aubmindlab/aragpt2-mega-detector-long\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# print(\"Max tokens:\", tokenizer.model_max_length)\n\n# model = AutoModelForSequenceClassification.from_pretrained(\n#     model_name,\n#     # num_labels=num_labels\n#     # remove use_safetensors=True\n# )\n# print(\"Model context limit:\", model.config.max_position_embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"# === CONFIG ===\nSAMPLE_MODE = \"stratified\"  # \"all\", \"random\", \"stratified\"\nSAMPLE_SIZE = 0.1           # 10% for stratified or random. Set to None for full dataset.\n\n# === LOAD AND SAMPLE ===\ntrain_df = load_and_sample_dataset(\n    Config.TRAIN_FILE,\n    # sample_mode=SAMPLE_MODE,\n    # sample_size=SAMPLE_SIZE\n)\n\n# === LOAD AND SAMPLE ===\n# val_df = load_and_sample_dataset(\n#     Config.VAL_FILE,\n#     # sample_mode=SAMPLE_MODE,\n#     # sample_size=SAMPLE_SIZE\n# )\n\nprint(\"Train shape:\", train_df.shape)\n# print(\"Validation shape:\", val_df.shape)\nprint(\"Authors in Train:\", train_df['author'].nunique())\n# print(\"Authors in Val:\", val_df['author'].nunique())\n\n# train_df.head()","metadata":{"id":"xNLnmQKpEVTM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Labeling","metadata":{}},{"cell_type":"code","source":"# ── cell: mappings ──\n# Arabic→ID (from your JSON)\nauthor2id = {\n  \"أحمد أمين\": 0,  \"أحمد تيمور باشا\": 1,  \"أحمد شوقي\": 2,\n  \"أمين الريحاني\": 3,  \"ثروت أباظة\": 4,  \"جبران خليل جبران\": 5,\n  \"جُرجي زيدان\": 6,  \"حسن حنفي\": 7,  \"روبرت بار\": 8,\n  \"سلامة موسى\": 9,  \"طه حسين\": 10, \"عباس محمود العقاد\": 11,\n  \"عبد الغفار مكاوي\": 12, \"غوستاف لوبون\": 13, \"فؤاد زكريا\": 14,\n  \"كامل كيلاني\": 15, \"محمد حسين هيكل\": 16, \"نجيب محفوظ\": 17,\n  \"نوال السعداوي\": 18, \"ويليام شيكسبير\": 19, \"يوسف إدريس\": 20\n}\n\n# ID→Arabic\nid2author = {v:k for k,v in author2id.items()}\n\n# ID→English (you supply these)\nid2english = {\n   0:\"Ahmed_Amin\",      1:\"Ahmad_Taymour_Basha\", 2:\"Ahmed_Shawqi\",\n   3:\"Ameen_Rihani\",    4:\"Tharwat_Abaza\",        5:\"Gibran_Khalil_Gibran\",\n   6:\"Jurji_Zaydan\",    7:\"Hassan_Hanifi\",        8:\"Robert_Barr\",\n   9:\"Salama_Moussa\",   10:\"Taha_Hussein\",        11:\"Abbas_Al-Aqqad\",\n   12:\"AbdelGhaffar_Makawi\", 13:\"Gustave_Lebon\",   14:\"Fouad_Zakaria\",\n   15:\"Kamel_Kilani\",   16:\"Mohamed_Hosseini_Hekal\",17:\"Naguib_Mahfouz\",\n   18:\"Nawal_El_Saadawi\",19:\"William_Shakespeare\",20:\"Youssef_Edrees\"\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ── cell: build binary train/val lists ──\n\nfrom datasets import Dataset\n\n# 1) specify the target author by its ID\nTARGET_ID = 4  # ← e.g. 20 for يوسف إدريس\n\n# 2) your existing author2id + id2english dicts\n# (make sure these are defined above)\n# author2id = { \"أحمد أمين\":0, …, \"يوسف إدريس\":20 }\n# id2english  = { 0:\"Ahmed_Amin\", …, 20:\"Youssef_Edrees\" }\n\nTARGET_AR = id2author[TARGET_ID]\nTARGET_EN = id2english[TARGET_ID]\nprint(f\"▶ Building binary dataset for: ({TARGET_ID}) {TARGET_AR} / {TARGET_EN}\")\n\n# 3) map both train & val to 0/1\ntrain_data = [\n    {\"text\": row[\"text_in_author_style\"], \"label\": int(author2id[row[\"author\"]] == TARGET_ID)}\n    for _, row in train_df.iterrows()\n]\n# val_data   = [\n#     {\"text\": row[\"text_in_author_style\"], \"label\": int(author2id[row[\"author\"]] == TARGET_ID)}\n#     for _, row in val_df.iterrows()\n# ]\n\n# 4) wrap into 🤗 Dataset\ntrain_dataset = Dataset.from_list(train_data)\n# val_dataset   = Dataset.from_list(val_data)\n","metadata":{"id":"VDsAHV1VDdYJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n# with open(\"/kaggle/working/evaluation/ar_style_classifier/results/author2id.json\", \"w\") as f:\n#     json.dump(author2id, f, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"CAMeL-Lab/bert-base-arabic-camelbert-ca\" #\"allenai/longformer-base-4096\" #\"UBC-NLP/AraT5v2-base-1024\"\n# num_labels = len(unique_authors)\nnum_labels = 2\ntokenizer = AutoTokenizer.from_pretrained(model_name,use_safetensors=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels,\n    # use_safetensors=True\n).to(device)\n\ndef preprocess_sliding(examples):\n    chunk_size = 512\n    stride = 256\n    input_ids_list = []\n    attention_mask_list = []\n    label_list = []\n    sample_id_list = []  # NEW\n\n    for idx, (text, label) in enumerate(zip(examples[\"text\"], examples[\"label\"])):\n        encodings = tokenizer(\n            text,\n            truncation=True,\n            max_length=chunk_size,\n            stride=stride,\n            return_overflowing_tokens=True,\n            return_attention_mask=True,\n        )\n        \n        num_chunks = len(encodings[\"input_ids\"])\n        for input_ids, attention_mask in zip(encodings[\"input_ids\"], encodings[\"attention_mask\"]):\n            input_ids_list.append(input_ids)\n            attention_mask_list.append(attention_mask)\n            label_list.append(label)\n            sample_id_list.append(idx)  # Use the index as sample identifier\n\n    return {\n        \"input_ids\": input_ids_list,\n        \"attention_mask\": attention_mask_list,\n        \"label\": label_list,\n        \"sample_id\": sample_id_list\n    }\n\n\ntrain_dataset = train_dataset.map(preprocess_sliding, batched=True, remove_columns=[\"text\"])\n# val_dataset = val_dataset.map(preprocess_sliding, batched=True, remove_columns=[\"text\"])","metadata":{"id":"fBbSy6jBDdYK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_dataset[0])\n","metadata":{"id":"n8rJEWfPUE52","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"markdown","source":"# Training Setup","metadata":{}},{"cell_type":"code","source":"print(len(train_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.cpu_count()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n## batch size calculation\neffective_batch_size = 256\nbatch_size = 32\naccumulation_steps = effective_batch_size // batch_size\nepochs = 5\nOUTPUT_DIR=f\"evaluation/ar_style_classifier/sc_{TARGET_ID}_{TARGET_EN}\"  # output_dir = f\"sc_{TARGET_ID}_{TARGET_EN}\"\n\n#config training env\nmodel.gradient_checkpointing_disable()\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n# os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n\n\n# args\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    max_steps=-1,\n    save_strategy=\"epoch\",\n    # save_steps=500,\n    # save_strategy=\"steps\",\n    # save_steps=10,\n    # eval_strategy=\"epoch\",\n    # eval_steps=1000,\n    # eval_strategy=\"steps\",\n    # eval_steps=10,  # ⬅️ super frequent eval for debugging\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    # per_device_eval_batch_size=batch_size,\n    num_train_epochs=epochs,\n    gradient_accumulation_steps=accumulation_steps,\n    # load_best_model_at_end=True,\n    # metric_for_best_model=\"f1\",\n    save_total_limit=1, \n    logging_steps=accumulation_steps * 25,\n    logging_strategy=\"steps\",\n    dataloader_num_workers=0,\n    report_to=\"none\"\n)","metadata":{"id":"eDc-9AQiDdYN","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\nfrom transformers import Trainer, TrainingArguments, DefaultDataCollator\n\nfrom transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator,\n    callbacks=[],  # Completely remove default callbacks like WandB\n    # eval_dataset=val_dataset,\n    # compute_metrics=compute_metrics,\n)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\n# print(f\"Eval dataset size: {len(val_dataset)}\")","metadata":{"id":"Xc44ADNWDdYO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"id":"2w7G7WiWgiY0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Launch Training","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nimport time\nimport os\nfrom config import Config\n\n\nprint(f\"📢 Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\nstart_time = time.time()\n\n# Detect checkpoint\nlast_checkpoint = None\nCHECKPOINT_DIR = OUTPUT_DIR    #os.path.join(Config.MODEL_WEIGHTS_FOLDER,safe_model_name(MODEL_CONFIG[\"name\"]))\nRESUME_TRAINING=False\n\nif RESUME_TRAINING:\n    if os.path.isdir(CHECKPOINT_DIR):\n        checkpoints = [d for d in os.listdir(CHECKPOINT_DIR) if d.startswith(\"checkpoint-\")]\n\n        if checkpoints:\n            # Sort numerically based on step number\n            latest_checkpoint =  max(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n            last_checkpoint = os.path.join(CHECKPOINT_DIR, latest_checkpoint)\n            print(f\"✅ Found checkpoint at {last_checkpoint}. Will resume training from there.\")\n        else:\n            print(\"⚠️ No checkpoints found. Starting from scratch.\")\n    else:\n        print(\"⚠️ Checkpoint directory doesn't exist. Starting from scratch.\")\nelse:\n    print(\"🆕 Starting fresh training (no checkpoint resume).\")\n\n# Start training\ntry:\n    if last_checkpoint:\n        train_output = trainer.train(\n            resume_from_checkpoint=last_checkpoint,\n            # ignore_keys_for_eval=[\"optimizer\", \"scheduler\"]\n        )\n    else:\n        train_output = trainer.train()\n\n    total_time = time.time() - start_time\n    print(f\"✅ Training completed in {total_time/3600:.2f} hours\")\n\n    if train_output.metrics:\n        print(\"\\n📊 Final Training Metrics:\")\n        for key, value in train_output.metrics.items():\n            print(f\" {key}: {value:.4f}\")\n\nexcept KeyboardInterrupt:\n    print(\"\\n⚠️ Training interrupted by user. Saving checkpoint...\")\n    interrupted_path = os.path.join(CHECKPOINT_DIR, \"checkpoint-interrupted\")\n    trainer.save_model(interrupted_path)\n    print(f\"💾 Checkpoint saved at {interrupted_path}\")","metadata":{"id":"VAhhNrNhDdYP","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save Results","metadata":{}},{"cell_type":"code","source":"save_dir = f\"evaluation/ar_style_classifier/sc_{TARGET_ID}_{TARGET_EN}/results\"\nmodel.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\nprint(f\"Model saved to {save_dir}\")","metadata":{"id":"UoVDOv3kDdYP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save to Dataset","metadata":{}},{"cell_type":"code","source":"!pip install -q kaggle\n\n!mkdir -p ~/.kaggle\n# !echo '{\"username\": \"kaggelone\", \"key\": \"14a0b752458ec2faed37fb584b1bae9f\"}' > ~/.kaggle/kaggle.json\n!cp /kaggle/input/kaggle-json/kaggle.json  ~/.kaggle/  # Assuming you uploaded it as kaggle-json dataset\n!chmod 600 ~/.kaggle/kaggle.json\n!mkdir -p /root/.config/kaggle/\n!cp /kaggle/input/kaggle-json/kaggle.json /root/.config/kaggle/\n!chmod 600 /root/.config/kaggle/kaggle.json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/input/kaggle-json/kaggle.json /root/.config/kaggle\n!chmod 600 /root/.config/kaggle/kaggle.json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input/kaggle-json\n!ls ~/.kaggle\n!ls /root/.config/kaggle/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport shutil\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nfrom datetime import datetime\nimport zipfile\n\ndef upload_to_kaggle_dataset(dataset_name, folder_path, dataset_title=None,\n                             override=True, version_append=True,\n                             local_backup_path=None):\n    \"\"\"\n    Uploads a folder or file to a Kaggle dataset, preserving older structure when version_append=True.\n    If dataset_download_files fails (e.g. private dataset), falls back to local_backup_path.\n    \"\"\"\n    kaggle_username = \"hafsousaalilou\"\n    kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n    if os.path.exists(kaggle_json_path):\n        try:\n            with open(kaggle_json_path) as f:\n                creds = json.load(f)\n                kaggle_username = creds.get(\"username\", kaggle_username)\n        except:\n            pass\n    kaggle_username = os.environ.get(\"KAGGLE_USERNAME\", kaggle_username)\n    dataset_slug = dataset_name.lower().replace(\"_\", \"-\").replace(\" \", \"-\")\n    dataset_id = f\"{kaggle_username}/{dataset_slug}\"\n    if dataset_title is None:\n        dataset_title = dataset_name.replace(\"-\", \" \").title()\n\n    api = KaggleApi()\n    api.authenticate()\n\n    # Detect existence\n    dataset_exists = any(ds.ref.lower() == dataset_id.lower()\n                         for ds in api.dataset_list(user=kaggle_username))\n\n    temp_dir = f\"/tmp/{dataset_slug}_upload\"\n    if os.path.exists(temp_dir):\n        shutil.rmtree(temp_dir)\n    os.makedirs(temp_dir)\n\n    # 1) Download old version if required\n    if dataset_exists and version_append:\n        print(f\"⬇️ Downloading existing dataset '{dataset_id}'…\")\n        try:\n            api.dataset_download_files(dataset_id, path=temp_dir, unzip=True)\n        except Exception as e:\n            print(f\"⚠️ dataset_download_files failed: {e}\")\n            if local_backup_path and os.path.exists(local_backup_path):\n                print(f\"ℹ️ Falling back to local backup at '{local_backup_path}'\")\n                shutil.copytree(local_backup_path, temp_dir, dirs_exist_ok=True)\n            else:\n                print(\"❌ Cannot preserve old files—no download and no local backup.\")\n                version_append = False\n\n    # Debug: show what’s in temp_dir before adding new zip\n    print(\"📂 temp_dir contents BEFORE zipping:\")\n    for name in sorted(os.listdir(temp_dir)):\n        print(\"   \", name)\n\n    # 2) Create new folder zip\n    folder_basename = os.path.basename(os.path.normpath(folder_path))\n    zip_path = os.path.join(temp_dir, f\"{folder_basename}.zip\")\n\n    # Pre‑zip conflict check\n    if os.path.exists(zip_path):\n        if override:\n            os.remove(zip_path)\n        else:\n            print(f\"⚠️ '{folder_basename}.zip' already exists—skipping.\")\n            return\n\n    print(f\"📦 Zipping '{folder_path}' → '{zip_path}'…\")\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n        if os.path.isdir(folder_path):\n            for root, _, files in os.walk(folder_path):\n                for f in files:\n                    abs_p = os.path.join(root, f)\n                    rel_p = os.path.relpath(abs_p, os.path.dirname(folder_path))\n                    zf.write(abs_p, arcname=os.path.join(folder_basename, rel_p))\n        else:\n            zf.write(folder_path, arcname=folder_basename)\n\n    # 3) Final temp_dir debug\n    print(\"📂 temp_dir contents BEFORE upload:\")\n    for name in sorted(os.listdir(temp_dir)):\n        print(\"   \", name)\n\n    # 4) Metadata\n    meta = {\"title\": dataset_title, \"id\": dataset_id, \"licenses\":[{\"name\":\"CC0-1.0\"}]}\n    with open(os.path.join(temp_dir, \"dataset-metadata.json\"), \"w\") as f:\n        json.dump(meta, f, indent=2)\n\n    # 5) Upload\n    try:\n        if dataset_exists:\n            api.dataset_create_version(\n                temp_dir,\n                version_notes=f\"Append {folder_basename} @ {datetime.now()}\",\n                convert_to_csv=False,\n                dir_mode=\"zip\"\n            )\n        else:\n            api.dataset_create_new(\n                temp_dir,\n                convert_to_csv=False,\n                dir_mode=\"zip\"\n            )\n        print(\"✅ Upload successful.\")\n    except Exception as e:\n        print(f\"❌ Upload failed: {e}\")\n\n    shutil.rmtree(temp_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"upload_to_kaggle_dataset(\n    dataset_name=f\"binary-style-classifier-models-{TARGET_ID}\",\n    folder_path=f\"evaluation/ar_style_classifier/sc_{TARGET_ID}_{TARGET_EN}\", # \"/kaggle/working/output/evaluation/ar_style_classifier/results\", \n    dataset_title=f\"BSC Model author {TARGET_ID} {TARGET_EN}\",\n    override=True,\n    version_append=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# # --- Inference on full validation texts ---\n# def predict_full_text(text):\n#     enc = tokenizer(\n#         text,\n#         truncation=True,\n#         padding=\"max_length\",\n#         max_length=512,\n#         stride=256,\n#         return_overflowing_tokens=True,\n#         return_tensors=\"pt\"\n#     ).to(device)\n#     logits = model(**enc).logits  # [num_chunks, num_labels]\n#     avg_logits = logits.mean(dim=0)\n#     probs = torch.softmax(avg_logits, dim=0)\n#     return torch.argmax(probs).item(), probs.cpu().tolist()\n\n# # --- Evaluate across validation set ---\n# preds, refs = [], []\n# for example in val_df.itertuples():\n#     pred, _ = predict_full_text(example.text_in_author_style)\n#     preds.append(pred)\n#     refs.append(author2id[example.author])\n\n# from sklearn.metrics import accuracy_score, f1_score\n# print(\"Validation Accuracy:\", accuracy_score(refs, preds))\n# print(\"Validation F1‑macro:\", f1_score(refs, preds, average=\"macro\"))\n\n","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import evaluate\n\n# # Load metrics\n# metric_acc = evaluate.load(\"accuracy\")\n# metric_f1 = evaluate.load(\"f1\")\n\n# # Sanity check for labels\n# EXPECTED_NUM_CLASSES = len(unique_authors)  # make sure this is defined correctly\n\n\n# def compute_metrics(p):\n#     preds = np.argmax(p.predictions, axis=1)\n\n#     # Defensive check: Are labels within expected range?\n#     if np.max(preds) >= EXPECTED_NUM_CLASSES or np.min(preds) < 0:\n#         raise ValueError(f\"Predictions contain invalid class indices: {np.unique(preds)}\")\n\n#     if np.max(p.label_ids) >= EXPECTED_NUM_CLASSES or np.min(p.label_ids) < 0:\n#         raise ValueError(f\"Label IDs contain invalid class indices: {np.unique(p.label_ids)}\")\n\n#     acc = metric_acc.compute(predictions=preds, references=p.label_ids)[\"accuracy\"]\n    \n#     # Use macro average for multiclass tasks\n#     # f1 = metric_f1.compute(predictions=preds, references=p.label_ids, average=\"macro\")[\"f1\"]\n\n#     # binary F1 (pos_label=1)\n#     f1    = metric_f1.compute(\n#                 predictions=preds,\n#                 references=p.label_ids,\n#                 average=\"binary\",\n#                 pos_label=1\n#              )[\"f1\"]\n    \n#     return {\"accuracy\": acc, \"f1\": f1}\n\n","metadata":{"id":"igmtb3z6DdYM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Dummy sanity check\n# dummy_preds = np.random.randint(0, EXPECTED_NUM_CLASSES, size=100)\n# dummy_labels = np.random.randint(0, EXPECTED_NUM_CLASSES, size=100)\n\n# test_acc = metric_acc.compute(predictions=dummy_preds, references=dummy_labels)[\"accuracy\"]\n# test_f1 = metric_f1.compute(predictions=dummy_preds, references=dummy_labels, average=\"macro\")[\"f1\"]\n\n# print(f\"Sanity metric test - Accuracy: {test_acc:.4f}, F1 (macro): {test_f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}