{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12342718,"sourceType":"datasetVersion","datasetId":7780982},{"sourceId":12470700,"sourceType":"datasetVersion","datasetId":7867626}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":128.955805,"end_time":"2025-07-14T06:47:18.107738","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-14T06:45:09.151933","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\npath = \"/kaggle/working\"\nis_empty = (len(os.listdir(path)) <= 3)\n\n!ls /kaggle/working/\n\nMODEL_WEIGHTS_SRC =\"/kaggle/input/facebook-mbart-large-50-many-to-many-mmt-model/facebook_mbart-large-50-many-to-many-mmt/facebook_mbart-large-50-many-to-many-mmt/final/*\"\n\nMODEL_NAME = \"facebook_mbart-large-50-many-to-many-mmt\"\n# MODEL_NAME = \"UBC-NLP/AraT5v2-base-1024\" #\"agemagician_mlong-t5-tglobal-large\" #\"facebook_mbart-large-50-many-to-many-mmt\" # \"UBC-NLP_AraT5-base\" #\"google/mt5-small\"\n\nMODEL_WEIGHTS_DEST_DIR =f\"/kaggle/working/models/fine_tuned_model/model_weights/{MODEL_NAME}\"\n\n\n\n# !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n\nif is_empty:\n    print(f\"✅ '{path}' is empty. copying project strcuture into it ...\")\n    !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n    !mkdir -p {MODEL_WEIGHTS_DEST_DIR}\n    !cp -r {MODEL_WEIGHTS_SRC} {MODEL_WEIGHTS_DEST_DIR}\nelse:\n    print(f\"❌ '{path}' is not empty — contains {len(os.listdir(path))} items.\")\n\n\n\n# Change to project directory\n%cd /kaggle/working/ \n\n# Confirm\n!ls","metadata":{"papermill":{"duration":19.447473,"end_time":"2025-07-14T06:47:03.311976","exception":false,"start_time":"2025-07-14T06:46:43.864503","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:36:25.931436Z","iopub.execute_input":"2025-07-16T08:36:25.932054Z","iopub.status.idle":"2025-07-16T08:36:49.701261Z","shell.execute_reply.started":"2025-07-16T08:36:25.932030Z","shell.execute_reply":"2025-07-16T08:36:49.700588Z"}},"outputs":[{"name":"stdout","text":"✅ '/kaggle/working' is empty. copying project strcuture into it ...\n/kaggle/working\nconfig.py  evaluation\tmodels\t run_pipeline.py\ndata\t   __init__.py\tprompts  utils\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ls -R {MODEL_WEIGHTS_DEST_DIR}","metadata":{"papermill":{"duration":0.154964,"end_time":"2025-07-14T06:47:03.499832","exception":false,"start_time":"2025-07-14T06:47:03.344868","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:36:49.702628Z","iopub.execute_input":"2025-07-16T08:36:49.702842Z","iopub.status.idle":"2025-07-16T08:36:49.818994Z","shell.execute_reply.started":"2025-07-16T08:36:49.702819Z","shell.execute_reply":"2025-07-16T08:36:49.818366Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/models/fine_tuned_model/model_weights/facebook_mbart-large-50-many-to-many-mmt:\nconfig.json\t\tsentencepiece.bpe.model  training_args.bin\ngeneration_config.json\tspecial_tokens_map.json\nmodel.safetensors\ttokenizer_config.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"fTyhOO2v7ssz","outputId":"7eecf25f-5840-4ed0-c27a-fa479b3c0838","papermill":{"duration":0.038757,"end_time":"2025-07-14T06:46:43.831489","exception":false,"start_time":"2025-07-14T06:46:43.792732","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:36:49.819860Z","iopub.execute_input":"2025-07-16T08:36:49.820105Z","iopub.status.idle":"2025-07-16T08:36:49.823279Z","shell.execute_reply.started":"2025-07-16T08:36:49.820074Z","shell.execute_reply":"2025-07-16T08:36:49.822732Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# project_dir = \"/content/drive/MyDrive/AraGenEval2025\"\n# os.chdir(project_dir)\n# print(f\"✅ Current directory: {os.getcwd()}\")\n","metadata":{"id":"cab9aba0","papermill":{"duration":0.041911,"end_time":"2025-07-14T06:47:15.232757","exception":false,"start_time":"2025-07-14T06:47:15.190846","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:36:49.824639Z","iopub.execute_input":"2025-07-16T08:36:49.824823Z","iopub.status.idle":"2025-07-16T08:36:49.841174Z","shell.execute_reply.started":"2025-07-16T08:36:49.824808Z","shell.execute_reply":"2025-07-16T08:36:49.840647Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip install transformers[sentencepiece] torch pandas openpyxl psutil python-dotenv  --quiet","metadata":{"id":"559ddd13","outputId":"7921c04f-9a42-44dc-e548-5c1805145485","papermill":{"duration":90.123476,"end_time":"2025-07-14T06:46:43.758212","exception":false,"start_time":"2025-07-14T06:45:13.634736","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:36:49.841780Z","iopub.execute_input":"2025-07-16T08:36:49.841964Z","iopub.status.idle":"2025-07-16T08:38:15.581817Z","shell.execute_reply.started":"2025-07-16T08:36:49.841928Z","shell.execute_reply":"2025-07-16T08:38:15.580922Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Ensure PyTorch can expand allocations if needed\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"id":"b1rI7F4dlowA","papermill":{"duration":0.039224,"end_time":"2025-07-14T06:47:03.708322","exception":false,"start_time":"2025-07-14T06:47:03.669098","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:38:15.582799Z","iopub.execute_input":"2025-07-16T08:38:15.583032Z","iopub.status.idle":"2025-07-16T08:38:15.586966Z","shell.execute_reply.started":"2025-07-16T08:38:15.583005Z","shell.execute_reply":"2025-07-16T08:38:15.586379Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\nimport os\nimport json\nimport dotenv\nimport torch\nimport pandas as pd\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport gc\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfrom config import Config\n\n\n# project root in PYTHONPATH\nimport sys\nproject_root = os.getcwd()\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"✅ Using device: {device}\")\n\n# Device & parallel setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"✅ Using device: {device}, count: {torch.cuda.device_count()} GPUs\")","metadata":{"id":"390a0abf","papermill":{"duration":11.413602,"end_time":"2025-07-14T06:47:15.156088","exception":false,"start_time":"2025-07-14T06:47:03.742486","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:38:15.587604Z","iopub.execute_input":"2025-07-16T08:38:15.587791Z","iopub.status.idle":"2025-07-16T08:38:25.870916Z","shell.execute_reply.started":"2025-07-16T08:38:15.587776Z","shell.execute_reply":"2025-07-16T08:38:25.870297Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda, count: 2 GPUs\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\ndef safe_model_name(model_name):\n    return model_name.replace(\"/\", \"_\")\n\n# archive & inspect\ndef create_run_dir(base, name):\n    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    d  = os.path.join(base, safe_model_name(name), f\"run_{ts}\")\n    os.makedirs(d, exist_ok=True)\n    return d\n\ndef run_evaluation():\n  pass\n    # os.system(\"python utils/prepare_metrics_log.py\")\n    # os.system(\"python evaluation/evaluate.py\")\n\n\ndef archive_results(run_dir, files_to_archive):\n    for fpath in files_to_archive:\n        if os.path.exists(fpath):\n            dest = os.path.join(run_dir, os.path.basename(fpath))\n            os.system(f\"cp {fpath} {dest}\")\n","metadata":{"id":"d797f524","papermill":{"duration":0.040399,"end_time":"2025-07-14T06:47:15.604797","exception":false,"start_time":"2025-07-14T06:47:15.564398","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:38:25.871642Z","iopub.execute_input":"2025-07-16T08:38:25.872082Z","iopub.status.idle":"2025-07-16T08:38:25.877276Z","shell.execute_reply.started":"2025-07-16T08:38:25.872038Z","shell.execute_reply":"2025-07-16T08:38:25.876727Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def load_model(model_name: str):\n    secure = safe_model_name(model_name)\n    model_path = os.path.join(\"models/fine_tuned_model/model_weights\", secure)\n    print(f\"Loading model from {model_path}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, legacy=False)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n    # mBART language tags\n    if \"mbart\" in model_name.lower():\n        tokenizer.src_lang = tokenizer.tgt_lang = \"ar_AR\"\n    # wrap for multi‑GPU\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n    model.to(device).eval()\n    return tokenizer, model  \n","metadata":{"id":"d09aa414","papermill":{"duration":0.04142,"end_time":"2025-07-14T06:47:15.530296","exception":false,"start_time":"2025-07-14T06:47:15.488876","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:38:25.877920Z","iopub.execute_input":"2025-07-16T08:38:25.878189Z","iopub.status.idle":"2025-07-16T08:38:25.898778Z","shell.execute_reply.started":"2025-07-16T08:38:25.878172Z","shell.execute_reply":"2025-07-16T08:38:25.898283Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ### batched inference function (updated for DataParallel)\n# def generate_predictions(tokenizer, model, validation_path, output_log_path, batch_size=8):\n#     # 1. Load & prepare\n#     df = pd.read_excel(validation_path)\n#     df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n\n#     # Handle DataParallel wrapping\n#     base_model = model.module if hasattr(model, \"module\") else model\n#     base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n#     assert base_model.config.eos_token_id is not None\n\n#     records = []\n#     # 2. DataLoader for batching\n#     from torch.utils.data import DataLoader, Dataset\n\n#     class TextDataset(Dataset):\n#         def __init__(self, texts): self.texts = texts\n#         def __len__(self): return len(self.texts)\n#         def __getitem__(self, idx): return self.texts[idx]\n\n#     def collate_fn(batch_texts):\n#         return tokenizer(\n#             batch_texts,\n#             return_tensors=\"pt\",\n#             padding=True,\n#             truncation=True,    # preserve full inputs\n#             max_length=1024\n#         )\n\n#     loader = DataLoader(\n#         TextDataset(df[\"input_text\"].tolist()),\n#         batch_size=batch_size,\n#         shuffle=False,\n#         collate_fn=collate_fn,\n#         num_workers=4,\n#         pin_memory=True\n#     )\n\n#     # 3. Generate in batches\n#     for i, batch in enumerate(tqdm(loader, desc=\"Batches\")):\n#         batch = {k: v.to(device) for k, v in batch.items()}\n#         with torch.inference_mode():\n#             gen_ids = base_model.generate(\n#                 input_ids=batch[\"input_ids\"],\n#                 attention_mask=batch[\"attention_mask\"],\n#                 num_beams=3,\n#                 early_stopping=True,\n#                 do_sample=False,\n#                 length_penalty=1.2,\n#                 repetition_penalty=1.2,\n#                 no_repeat_ngram_size=2,\n#                 # max_length=3000,\n#                 max_new_tokens=2000\n#             )\n#         outs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n#         # accumulate\n#         start_idx = i * batch_size\n#         for j, out in enumerate(outs):\n#             row = df.iloc[start_idx + j]\n#             records.append({\n#                 \"id\": int(row[\"id\"]),\n#                 \"author\": row[\"author\"],\n#                 \"output\": out,\n#                 \"ground_truth\": row[\"text_in_author_style\"]\n#             })\n\n#         torch.cuda.empty_cache()\n#         gc.collect()\n\n#     # 4. Save\n#     with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n#         json.dump(records, f, ensure_ascii=False, indent=2)\n#     print(f\"✅ Generated {len(records)} records to {output_log_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:38:25.900604Z","iopub.execute_input":"2025-07-16T08:38:25.900815Z","iopub.status.idle":"2025-07-16T08:38:25.918661Z","shell.execute_reply.started":"2025-07-16T08:38:25.900799Z","shell.execute_reply":"2025-07-16T08:38:25.917987Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"### Batched inference function (robust, with error recovery)\ndef generate_predictions(tokenizer, model, validation_path, output_log_path, batch_size=8):\n    import time\n    import traceback\n\n    # 1. Load & prepare\n    df = pd.read_excel(validation_path)\n    df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n\n    # Handle DataParallel wrapping\n    base_model = model.module if hasattr(model, \"module\") else model\n    base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n    assert base_model.config.eos_token_id is not None\n\n    records = []\n\n    from torch.utils.data import DataLoader, Dataset\n\n    class TextDataset(Dataset):\n        def __init__(self, texts): self.texts = texts\n        def __len__(self): return len(self.texts)\n        def __getitem__(self, idx): return self.texts[idx]\n\n    def collate_fn(batch_texts):\n        return tokenizer(\n            batch_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=1024\n        )\n\n    loader = DataLoader(\n        TextDataset(df[\"input_text\"].tolist()),\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=4,\n        pin_memory=True\n    )\n\n    for i, batch in enumerate(tqdm(loader, desc=\"Batches\")):\n        success = False\n        attempts = 0\n        while not success and attempts < 3:\n            try:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with torch.inference_mode():\n                    gen_ids = base_model.generate(\n                        input_ids=batch[\"input_ids\"],\n                        attention_mask=batch[\"attention_mask\"],\n                        num_beams=3,\n                        early_stopping=True,\n                        do_sample=False,\n                        length_penalty=1.2,\n                        repetition_penalty=1.2,\n                        no_repeat_ngram_size=2,\n                        max_new_tokens=2000\n                    )\n                outs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n                start_idx = i * batch_size\n                for j, out in enumerate(outs):\n                    row = df.iloc[start_idx + j]\n                    records.append({\n                        \"id\": int(row[\"id\"]),\n                        \"author\": row[\"author\"],\n                        \"output\": out,\n                        \"ground_truth\": row[\"text_in_author_style\"]\n                    })\n                success = True\n\n            except RuntimeError as e:\n                attempts += 1\n                print(f\"❌ RuntimeError on batch {i}, attempt {attempts}: {str(e)}\")\n                traceback.print_exc()\n\n                torch.cuda.empty_cache()\n                gc.collect()\n\n                if \"CUDA out of memory\" in str(e):\n                    print(\"⚠️ Reducing batch size temporarily and retrying...\")\n                    time.sleep(2)\n                    if batch[\"input_ids\"].shape[0] > 1:\n                        # Retry each sample individually\n                        for j in range(batch[\"input_ids\"].shape[0]):\n                            try:\n                                single_batch = {k: v[j:j+1] for k, v in batch.items()}\n                                with torch.inference_mode():\n                                    gen_ids = base_model.generate(\n                                        input_ids=single_batch[\"input_ids\"],\n                                        attention_mask=single_batch[\"attention_mask\"],\n                                        num_beams=3,\n                                        early_stopping=True,\n                                        do_sample=False,\n                                        length_penalty=1.2,\n                                        repetition_penalty=1.2,\n                                        no_repeat_ngram_size=2,\n                                        max_new_tokens=2000\n                                    )\n                                out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n                                row = df.iloc[i * batch_size + j]\n                                records.append({\n                                    \"id\": int(row[\"id\"]),\n                                    \"author\": row[\"author\"],\n                                    \"output\": out,\n                                    \"ground_truth\": row[\"text_in_author_style\"]\n                                })\n                            except Exception as inner_e:\n                                print(f\"⚠️ Skipping failed sample {i * batch_size + j} id = {int(row['id'])}: {str(inner_e)}\")\n                                traceback.print_exc()\n                    success = True  # Continue to next batch regardless\n                else:\n                    raise e\n\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    # 4. Save\n    with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(records, f, ensure_ascii=False, indent=2)\n    print(f\"✅ Generated {len(records)} records to {output_log_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:38:25.919379Z","iopub.execute_input":"2025-07-16T08:38:25.919562Z","iopub.status.idle":"2025-07-16T08:38:25.947742Z","shell.execute_reply.started":"2025-07-16T08:38:25.919547Z","shell.execute_reply":"2025-07-16T08:38:25.947071Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\nimport gc\ngc.collect()","metadata":{"id":"bQCIIwaEEEp9","papermill":{"duration":0.202454,"end_time":"2025-07-14T06:47:15.921644","exception":false,"start_time":"2025-07-14T06:47:15.719190","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:38:25.948469Z","iopub.execute_input":"2025-07-16T08:38:25.948731Z","iopub.status.idle":"2025-07-16T08:38:26.095819Z","shell.execute_reply.started":"2025-07-16T08:38:25.948716Z","shell.execute_reply":"2025-07-16T08:38:26.095253Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"Lkh_YWV5Exw8","papermill":{"duration":0.16832,"end_time":"2025-07-14T06:47:16.124774","exception":false,"start_time":"2025-07-14T06:47:15.956454","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:38:26.096569Z","iopub.execute_input":"2025-07-16T08:38:26.096812Z","iopub.status.idle":"2025-07-16T08:38:26.348161Z","shell.execute_reply.started":"2025-07-16T08:38:26.096790Z","shell.execute_reply":"2025-07-16T08:38:26.347332Z"}},"outputs":[{"name":"stdout","text":"Wed Jul 16 08:38:26 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   43C    P8             12W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   43C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Cell 4: run the pipeline\n# adjust these paths as needed\nvalidation_file = Config.VAL_FILE\nlog_file        = f\"models/fine_tuned_model/results/{safe_model_name(MODEL_NAME)}/generation_log.json\"\noutput_base_dir = \"analysis/fine_tuned_model\"\n\nos.makedirs(os.path.dirname(log_file), exist_ok=True)\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\ntokenizer, model = load_model(MODEL_NAME)\ngenerate_predictions(tokenizer, model, validation_file, log_file, batch_size=16)\n\n\n\nrun_dir = create_run_dir(output_base_dir, MODEL_NAME)\nos.system(f\"cp {log_file} {run_dir}/\")\nprint(f\"\\n✅ Pipeline done. Logs in: {run_dir} \\n LOG FILE: {log_file}\\n\")\n# !nvidia-smi","metadata":{"id":"FzfhVKAGXa8M","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-07-16T08:38:26.349401Z","iopub.execute_input":"2025-07-16T08:38:26.350117Z"}},"outputs":[{"name":"stdout","text":"Loading model from models/fine_tuned_model/model_weights/facebook_mbart-large-50-many-to-many-mmt\n","output_type":"stream"},{"name":"stderr","text":"2025-07-16 08:38:35.471288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752655115.821561      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752655115.918998      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nBatches:   2%|▏         | 4/260 [09:45<10:23:24, 146.11s/it]","output_type":"stream"}],"execution_count":null}]}