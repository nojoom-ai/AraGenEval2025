{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12342718,"sourceType":"datasetVersion","datasetId":7780982},{"sourceId":12359253,"sourceType":"datasetVersion","datasetId":7792091},{"sourceId":12458325,"sourceType":"datasetVersion","datasetId":7858910},{"sourceId":12658673,"sourceType":"datasetVersion","datasetId":7863991},{"sourceId":12524371,"sourceType":"datasetVersion","datasetId":7905884}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ‚îÄ‚îÄ cell: 1 ‚îÄ‚îÄ\n# üìÅ Prepare Workspace\n\n\nMODEL_NAME= \"facebook_mbart-large-50-many-to-many-mmt\" # \"fewshot_gemini\" #\"agemagician_mlong-t5-tglobal-large\"  #\"UBC-NLP_AraT5-base\"\nMODEL_TYPE= \"fine_tuned_model\"\n\nimport os\n\npath = \"/kaggle/working\"\nis_empty = (len(os.listdir(path)) <= 5)\n\n# !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n\nif is_empty:\n    print(f\"‚úÖ '{path}' is empty. copying project strcuture into it ...\")\n    !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n    !mkdir -p /kaggle/working/evaluation/ar_style_classifier/\n    !cp -r /kaggle/input/binary-style-classifier-models-21-authors/working/* /kaggle/working/evaluation/ar_style_classifier/\n    !cp -r /kaggle/input/ast-fine-tuned-models-generations/results/results/* /kaggle/working/models/fine_tuned_model/results/\n\n    !mkdir -p /kaggle/working/models/baseline_model/results/fewshot_gemini\n    !cp -r /kaggle/input/ast-fine-tuned-models-generations/baseline_model/baseline_model/results/fewshot_gemini/generation_log.json /kaggle/working/models/baseline_model/results/fewshot_gemini\nelse:\n    print(f\"‚ùå '{path}' is not empty ‚Äî contains {len(os.listdir(path))} items.\")\n\n\n# !cp -r /kaggle/input/binary-style-classifier-models-21-authors/working/* /kaggle/working/evaluation/ar_style_classifier/\n!cp -r /kaggle/input/ast-fine-tuned-models-generations/results/results/* /kaggle/working/models/fine_tuned_model/results/\n\n\n\n# Change to project directory\n%cd /kaggle/working/ \n\n# Confirm\n!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:38.779444Z","iopub.execute_input":"2025-08-03T12:48:38.779796Z","iopub.status.idle":"2025-08-03T12:48:40.116486Z","shell.execute_reply.started":"2025-08-03T12:48:38.779772Z","shell.execute_reply":"2025-08-03T12:48:40.115730Z"}},"outputs":[{"name":"stdout","text":"‚ùå '/kaggle/working' is not empty ‚Äî contains 10 items.\n/kaggle/working\nconfig.py  evaluation\tmodels\t __pycache__\t  utils\ndata\t   __init__.py\tprompts  run_pipeline.py\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!cp -r /kaggle/input/arageneval2025-task1-finaldataset/PublicDataFinalPhaseTask1.xlsx /kaggle/working/data\n\n# data/PublicDataFinalPhaseTask1.xlsx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:40.117812Z","iopub.execute_input":"2025-08-03T12:48:40.118023Z","iopub.status.idle":"2025-08-03T12:48:40.279292Z","shell.execute_reply.started":"2025-08-03T12:48:40.118004Z","shell.execute_reply":"2025-08-03T12:48:40.278463Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!ls -R /kaggle/working/models/{MODEL_TYPE}/results/\n!ls -R /kaggle/working/models/{MODEL_TYPE}/results/{MODEL_NAME}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:40.280351Z","iopub.execute_input":"2025-08-03T12:48:40.280647Z","iopub.status.idle":"2025-08-03T12:48:42.237175Z","shell.execute_reply.started":"2025-08-03T12:48:40.280624Z","shell.execute_reply":"2025-08-03T12:48:42.236425Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/models/fine_tuned_model/results/:\nagemagician_mlong-t5-tglobal-large\t  sultan_ArabicT5-49GB-base\nfacebook_mbart-large-50-many-to-many-mmt  UBC-NLP_AraT5-base\ngoogle_mt5-small\t\t\t  UBC-NLP_AraT5v2-base-1024\nmoussaKam_AraBART\t\t\t  UBC-NLP_AraT5v2-base-1024_with_lora\nmoussaKam_AraBART_with_lora\n\n/kaggle/working/models/fine_tuned_model/results/agemagician_mlong-t5-tglobal-large:\ngeneration_log.json  generation_log_v1.json  generation_log_v2.json\n\n/kaggle/working/models/fine_tuned_model/results/facebook_mbart-large-50-many-to-many-mmt:\ngeneration_log.json\t  generation_log_v2.json\ngeneration_log_test.json  generation_log_v3.json\n\n/kaggle/working/models/fine_tuned_model/results/google_mt5-small:\ngeneration_log_ERROR.json  generation_log.json\n\n/kaggle/working/models/fine_tuned_model/results/moussaKam_AraBART:\ngeneration_log.json\n\n/kaggle/working/models/fine_tuned_model/results/moussaKam_AraBART_with_lora:\ngeneration_log.json\t  generation_log_v1.json\ngeneration_log_test.json  generation_log_v2.json\n\n/kaggle/working/models/fine_tuned_model/results/sultan_ArabicT5-49GB-base:\ngeneration_log.json\n\n/kaggle/working/models/fine_tuned_model/results/UBC-NLP_AraT5-base:\ngeneration_log.json\n\n/kaggle/working/models/fine_tuned_model/results/UBC-NLP_AraT5v2-base-1024:\ngeneration_log.json\n\n/kaggle/working/models/fine_tuned_model/results/UBC-NLP_AraT5v2-base-1024_with_lora:\ngeneration_log_test_error.json\tgeneration_log_test_v1.json\n/kaggle/working/models/fine_tuned_model/results/facebook_mbart-large-50-many-to-many-mmt:\ngeneration_log.json\t  generation_log_v2.json\ngeneration_log_test.json  generation_log_v3.json\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers==4.41.2 datasets==2.19.1 scikit-learn wandb python-dotenv evaluate --quiet\n!pip install -U peft==0.11.1 --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:42.239062Z","iopub.execute_input":"2025-08-03T12:48:42.239295Z","iopub.status.idle":"2025-08-03T12:48:49.371271Z","shell.execute_reply.started":"2025-08-03T12:48:42.239275Z","shell.execute_reply":"2025-08-03T12:48:49.370247Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import sys\nimport os\nimport torch\n\nproject_root = os.getcwd()  # Should be /content/drive/MyDrive/AraGenEval2025\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\nprint(f\"Project root added to sys.path: {project_root}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nfrom config import Config\n\nprint(Config.VAL_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:49.372514Z","iopub.execute_input":"2025-08-03T12:48:49.372829Z","iopub.status.idle":"2025-08-03T12:48:49.378971Z","shell.execute_reply.started":"2025-08-03T12:48:49.372805Z","shell.execute_reply":"2025-08-03T12:48:49.378156Z"}},"outputs":[{"name":"stdout","text":"Project root added to sys.path: /kaggle/working\nUsing device: cuda\ndata/AuthorshipStyleTransferVal.xlsx\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Mapping","metadata":{}},{"cell_type":"code","source":"# ‚îÄ‚îÄ cell: 3 ‚îÄ‚îÄ\n# üó∫ Author Mappings\nauthor2id = {\n    \"ÿ£ÿ≠ŸÖÿØ ÿ£ŸÖŸäŸÜ\": 0,  \"ÿ£ÿ≠ŸÖÿØ ÿ™ŸäŸÖŸàÿ± ÿ®ÿßÿ¥ÿß\": 1,  \"ÿ£ÿ≠ŸÖÿØ ÿ¥ŸàŸÇŸä\": 2,\n    \"ÿ£ŸÖŸäŸÜ ÿßŸÑÿ±Ÿäÿ≠ÿßŸÜŸä\": 3,  \"ÿ´ÿ±Ÿàÿ™ ÿ£ÿ®ÿßÿ∏ÿ©\": 4,  \"ÿ¨ÿ®ÿ±ÿßŸÜ ÿÆŸÑŸäŸÑ ÿ¨ÿ®ÿ±ÿßŸÜ\": 5,\n    \"ÿ¨Ÿèÿ±ÿ¨Ÿä ÿ≤ŸäÿØÿßŸÜ\": 6,  \"ÿ≠ÿ≥ŸÜ ÿ≠ŸÜŸÅŸä\": 7,  \"ÿ±Ÿàÿ®ÿ±ÿ™ ÿ®ÿßÿ±\": 8,\n    \"ÿ≥ŸÑÿßŸÖÿ© ŸÖŸàÿ≥Ÿâ\": 9,  \"ÿ∑Ÿá ÿ≠ÿ≥ŸäŸÜ\": 10, \"ÿπÿ®ÿßÿ≥ ŸÖÿ≠ŸÖŸàÿØ ÿßŸÑÿπŸÇÿßÿØ\": 11,\n    \"ÿπÿ®ÿØ ÿßŸÑÿ∫ŸÅÿßÿ± ŸÖŸÉÿßŸàŸä\": 12, \"ÿ∫Ÿàÿ≥ÿ™ÿßŸÅ ŸÑŸàÿ®ŸàŸÜ\": 13,  \"ŸÅÿ§ÿßÿØ ÿ≤ŸÉÿ±Ÿäÿß\": 14,\n    \"ŸÉÿßŸÖŸÑ ŸÉŸäŸÑÿßŸÜŸä\": 15,  \"ŸÖÿ≠ŸÖÿØ ÿ≠ÿ≥ŸäŸÜ ŸáŸäŸÉŸÑ\": 16,  \"ŸÜÿ¨Ÿäÿ® ŸÖÿ≠ŸÅŸàÿ∏\": 17,\n    \"ŸÜŸàÿßŸÑ ÿßŸÑÿ≥ÿπÿØÿßŸàŸä\": 18, \"ŸàŸäŸÑŸäÿßŸÖ ÿ¥ŸäŸÉÿ≥ÿ®Ÿäÿ±\": 19, \"ŸäŸàÿ≥ŸÅ ÿ•ÿØÿ±Ÿäÿ≥\": 20\n}\nid2author   = {v: k for k, v in author2id.items()}\nid2english  = {\n    0:\"Ahmed_Amin\",    1:\"Ahmad_Taymour_Basha\",   2:\"Ahmed_Shawqi\",\n    3:\"Ameen_Rihani\",  4:\"Tharwat_Abaza\",         5:\"Gibran_Khalil_Gibran\",\n    6:\"Jurji_Zaydan\",  7:\"Hassan_Hanifi\",         8:\"Robert_Barr\",\n    9:\"Salama_Moussa\",10:\"Taha_Hussein\",         11:\"Abbas_Al-Aqqad\",\n   12:\"AbdelGhaffar_Makawi\",13:\"Gustave_Lebon\",    14:\"Fouad_Zakaria\",\n   15:\"Kamel_Kilani\",16:\"Mohamed_Hosseini_Hekal\",17:\"Naguib_Mahfouz\",\n   18:\"Nawal_El_Saadawi\",19:\"William_Shakespeare\",20:\"Youssef_Edrees\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:49.379802Z","iopub.execute_input":"2025-08-03T12:48:49.380059Z","iopub.status.idle":"2025-08-03T12:48:49.394804Z","shell.execute_reply.started":"2025-08-03T12:48:49.380039Z","shell.execute_reply":"2025-08-03T12:48:49.394047Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport gc\n\ndef load_and_sample_dataset(\n    file_path,\n    sample_mode=\"all\",    # Options: \"all\", \"random\", \"stratified\"\n    sample_size=None,     # For \"random\"/\"stratified\": float (proportion) or int (absolute count)\n    random_state=42\n):\n    \"\"\"\n    Load dataset from Excel and apply sampling.\n\n    Args:\n        file_path (str): Path to dataset file.\n        sample_mode (str): \"all\", \"random\", or \"stratified\".\n        sample_size (float|int): Proportion (0-1) or count for sampling.\n        random_state (int): Random seed for reproducibility.\n\n    Returns:\n        pd.DataFrame: Sampled dataset.\n    \"\"\"\n    print(f\"üìÇ Loading dataset from: {file_path}\")\n    df = pd.read_excel(file_path, engine='openpyxl')\n\n    if sample_mode == \"all\":\n        sampled_df = df.copy()\n        print(f\"‚úÖ Loaded full dataset with {len(sampled_df)} samples.\")\n\n    elif sample_mode == \"random\":\n        if sample_size is None:\n            raise ValueError(\"For 'random' mode, SAMPLE_SIZE must be set.\")\n        sampled_df = df.sample(\n            n=sample_size if isinstance(sample_size, int) else int(len(df) * sample_size),\n            random_state=random_state\n        )\n        print(f\"‚úÖ Randomly sampled {len(sampled_df)} samples.\")\n\n    elif sample_mode == \"stratified\":\n        if sample_size is None or not (0 < sample_size < 1):\n            raise ValueError(\"For 'stratified' mode, SAMPLE_SIZE must be a proportion between 0 and 1.\")\n        sampled_df, _ = train_test_split(\n            df,\n            train_size=sample_size,\n            stratify=df['author'],\n            random_state=random_state\n        )\n        print(f\"‚úÖ Stratified sampled {len(sampled_df)} samples (author distribution preserved).\")\n\n    else:\n        raise ValueError(f\"Invalid sample_mode: {sample_mode}\")\n\n    del df\n    gc.collect()\n\n    return sampled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:49.395581Z","iopub.execute_input":"2025-08-03T12:48:49.395831Z","iopub.status.idle":"2025-08-03T12:48:49.410207Z","shell.execute_reply.started":"2025-08-03T12:48:49.395815Z","shell.execute_reply":"2025-08-03T12:48:49.409659Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# === CONFIG ===\nSAMPLE_MODE = \"stratified\"  # \"all\", \"random\", \"stratified\"\nSAMPLE_SIZE = 0.1           # 10% for stratified or random. Set to None for full dataset.\n\n# === LOAD AND SAMPLE ===\nval_df = load_and_sample_dataset(\n    \"data/PublicDataFinalPhaseTask1.xlsx\",\n    # Config.VAL_FILE,\n    # sample_mode=SAMPLE_MODE,\n    # sample_size=SAMPLE_SIZE\n)\n\n# Expect columns: id, text_in_msa\nval_df = val_df.rename(columns={\"id\":\"sample_id\",\"text_in_msa\":\"neutral_text\"})\nprint(f\"‚úÖ Loaded {len(val_df)} neutral texts.\")\n\nprint(\"Validation shape:\", val_df.shape)\nprint(\"Authors in Val:\", val_df['author'].nunique())\n\n# train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:49.411083Z","iopub.execute_input":"2025-08-03T12:48:49.411338Z","iopub.status.idle":"2025-08-03T12:48:50.689000Z","shell.execute_reply.started":"2025-08-03T12:48:49.411319Z","shell.execute_reply":"2025-08-03T12:48:50.688151Z"}},"outputs":[{"name":"stdout","text":"üìÇ Loading dataset from: data/PublicDataFinalPhaseTask1.xlsx\n‚úÖ Loaded full dataset with 8413 samples.\n‚úÖ Loaded 8413 neutral texts.\nValidation shape: (8413, 3)\nAuthors in Val: 21\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## Sliding Window","metadata":{}},{"cell_type":"code","source":"# ‚îÄ‚îÄ cell: 6 ‚îÄ‚îÄ\n# ü™ì Sliding Window Chunking Utility\nfrom transformers import AutoTokenizer\nimport torch\n\ndef get_chunks(text, tokenizer, max_len=512, stride=256):\n    return tokenizer(\n        text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_len,\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_tensors=\"pt\"\n    )\n\ndef device():\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:50.689802Z","iopub.execute_input":"2025-08-03T12:48:50.690018Z","iopub.status.idle":"2025-08-03T12:48:50.694869Z","shell.execute_reply.started":"2025-08-03T12:48:50.690001Z","shell.execute_reply":"2025-08-03T12:48:50.694114Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Load Model ","metadata":{}},{"cell_type":"code","source":"# ‚îÄ‚îÄ cell: 7 ‚îÄ‚îÄ\n# üîß Load Classifier Function\nfrom transformers import AutoModelForSequenceClassification\n\ndef load_binary_classifier(author_id):\n    path = f\"evaluation/ar_style_classifier/author_SC_{author_id}\"\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.to(device())\n    model.eval()\n    return tokenizer, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:50.697033Z","iopub.execute_input":"2025-08-03T12:48:50.697635Z","iopub.status.idle":"2025-08-03T12:48:50.710330Z","shell.execute_reply.started":"2025-08-03T12:48:50.697607Z","shell.execute_reply":"2025-08-03T12:48:50.709747Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Preparations","metadata":{}},{"cell_type":"code","source":"# ‚îÄ‚îÄ cell: 5 ‚îÄ‚îÄ\n# üìù Load Generation Log\nimport json\nwith open(f\"models/{MODEL_TYPE}/results/{MODEL_NAME}/generation_log_test.json\",\"r\", encoding=\"utf-8\") as f:\n    gen = json.load(f)\n# gen: list of {id, author, output, ground_truth}\nlog_df = pd.DataFrame(gen)\nlog_df = log_df.rename(columns={\"id\":\"sample_id\",\"output\":\"generated_text\",\"author\":\"true_author\"})\nprint(f\"‚úÖ Loaded {len(log_df)} generated entries.\")\n\n# Merge neutral and generated\ndf = pd.merge(log_df, val_df, on=\"sample_id\", how=\"left\")\ndf[\"true_id\"] = df[\"true_author\"].map(author2id)\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:50.710959Z","iopub.execute_input":"2025-08-03T12:48:50.711120Z","iopub.status.idle":"2025-08-03T12:48:51.135017Z","shell.execute_reply.started":"2025-08-03T12:48:50.711108Z","shell.execute_reply":"2025-08-03T12:48:51.134339Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Loaded 8413 generated entries.\n   sample_id       true_author  \\\n0          1         ÿ±Ÿàÿ®ÿ±ÿ™ ÿ®ÿßÿ±   \n1          2       ÿ¨Ÿèÿ±ÿ¨Ÿä ÿ≤ŸäÿØÿßŸÜ   \n2          3        ŸÜÿ¨Ÿäÿ® ŸÖÿ≠ŸÅŸàÿ∏   \n3          4  ÿ¨ÿ®ÿ±ÿßŸÜ ÿÆŸÑŸäŸÑ ÿ¨ÿ®ÿ±ÿßŸÜ   \n4          5        ŸäŸàÿ≥ŸÅ ÿ•ÿØÿ±Ÿäÿ≥   \n\n                                         text_in_msa  \\\n0  ÿ•ŸÜ ŸÉÿßŸÜ ÿπÿØÿØŸáÿß Ÿáÿßÿ¶ŸÑŸãÿß ÿ•ŸÑŸâ ÿ≠ÿØŸëŸç Ÿäÿ™ÿ∑ŸÑÿ® ÿ¨ŸáÿØŸãÿß ŸÖŸÜ ÿßŸÑ...   \n1  ŸÇÿßŸÑ: \"Ÿáÿ∞ÿß ŸáŸà ÿ≥ŸÑŸÖÿßŸÜ ÿßŸÑÿ∞Ÿä ÿ¨ÿßÿ°ŸÜÿß ÿ®ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸäŸÇŸäŸÜŸä...   \n2  ÿ≠ÿØŸëŸéŸÇ ÿ•ŸÑŸäŸáÿß ÿ®ŸÜÿ∏ÿ±ÿ© ŸÅÿßÿ≠ÿµÿ© ŸàŸÇÿßÿ≥Ÿäÿ©ÿå ŸÅÿ±ÿ¢Ÿáÿß ÿ™ÿ™ÿ≥ŸÖ ÿ®ÿßŸÑ...   \n3  ÿßŸÜÿµÿ±ŸÅ ŸÑŸàŸäÿµ ŸÖŸÜ ÿ£ŸÖÿßŸÖ ÿ≤ÿπŸäŸÖ ÿßŸÑŸÇÿ®ŸäŸÑÿ©ÿå ŸÖÿ™Ÿàÿ¨ŸáŸãÿß ÿ•ŸÑŸâ ŸÖ...   \n4  ÿ•ŸÜŸëŸéŸá ŸÑŸÖŸÜ ÿßŸÑÿπÿ®ÿ´ ÿßŸÑÿßÿ≥ÿ™ÿ∑ÿ±ÿßÿØ ŸÅŸä ÿ™Ÿàÿ∂Ÿäÿ≠ ÿ£Ÿà ÿ™ÿµŸàŸäÿ± ÿÆÿ∑...   \n\n                                      generated_text  \\\n0  ÿ•ŸÜ ŸÉÿßŸÜ ÿπÿØÿØŸáÿß Ÿáÿßÿ¶ŸÑŸãÿß ÿ•ŸÑŸâ ÿ≠ÿØŸëŸç ŸÖÿßÿå ŸÅŸÑŸÜ ŸÜÿ≠ÿµŸëŸêŸÑŸáÿå ...   \n1  ŸÇÿßŸÑ: Ÿáÿ∞ÿß ŸáŸà ÿ≥ŸÑŸÖÿßŸÜ ÿßŸÑÿ∞Ÿä ÿ¨ÿßÿ°ŸÜÿß ÿ®ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸäŸÇŸäŸÜŸäÿ©...   \n2  Ÿàÿ≠ÿØŸëŸéŸÇ ÿ•ŸÑŸäŸáÿß ÿ®ŸÜÿ∏ÿ±ÿ©Ÿç ŸÅÿßÿ≠ÿµÿ©Ÿç ŸÇÿßÿ≥Ÿäÿ©ÿå ŸÅÿ±ÿ¢Ÿáÿß ŸÖŸèŸÉÿ®ÿ±ÿ©...   \n3  ÿßŸÜÿµÿ±ŸÅ ŸÑŸàŸäÿµ ŸÖŸÜ ÿ£ŸÖÿßŸÖ ÿ≤ÿπŸäŸÖ ÿßŸÑŸÇÿ®ŸäŸÑÿ©ÿå Ÿàÿ∞Ÿáÿ® ÿ•ŸÑŸâ ŸÖÿ±ŸÇÿØ...   \n4  ŸàŸÖŸÜ ÿßŸÑÿπÿ®ÿ´ ÿ£ŸÜ ÿ£ÿ≥ÿ™ÿ∫ÿ±ŸÇ ŸÅŸä ÿ®ŸäÿßŸÜ ÿ£Ÿà ÿ™ÿµŸàŸäÿ± ÿÆÿ∑Ÿàÿ±ÿ© ŸÖÿß ...   \n\n                                        neutral_text            author  \\\n0  ÿ•ŸÜ ŸÉÿßŸÜ ÿπÿØÿØŸáÿß Ÿáÿßÿ¶ŸÑŸãÿß ÿ•ŸÑŸâ ÿ≠ÿØŸëŸç Ÿäÿ™ÿ∑ŸÑÿ® ÿ¨ŸáÿØŸãÿß ŸÖŸÜ ÿßŸÑ...         ÿ±Ÿàÿ®ÿ±ÿ™ ÿ®ÿßÿ±   \n1  ŸÇÿßŸÑ: \"Ÿáÿ∞ÿß ŸáŸà ÿ≥ŸÑŸÖÿßŸÜ ÿßŸÑÿ∞Ÿä ÿ¨ÿßÿ°ŸÜÿß ÿ®ÿßŸÑÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸäŸÇŸäŸÜŸä...       ÿ¨Ÿèÿ±ÿ¨Ÿä ÿ≤ŸäÿØÿßŸÜ   \n2  ÿ≠ÿØŸëŸéŸÇ ÿ•ŸÑŸäŸáÿß ÿ®ŸÜÿ∏ÿ±ÿ© ŸÅÿßÿ≠ÿµÿ© ŸàŸÇÿßÿ≥Ÿäÿ©ÿå ŸÅÿ±ÿ¢Ÿáÿß ÿ™ÿ™ÿ≥ŸÖ ÿ®ÿßŸÑ...        ŸÜÿ¨Ÿäÿ® ŸÖÿ≠ŸÅŸàÿ∏   \n3  ÿßŸÜÿµÿ±ŸÅ ŸÑŸàŸäÿµ ŸÖŸÜ ÿ£ŸÖÿßŸÖ ÿ≤ÿπŸäŸÖ ÿßŸÑŸÇÿ®ŸäŸÑÿ©ÿå ŸÖÿ™Ÿàÿ¨ŸáŸãÿß ÿ•ŸÑŸâ ŸÖ...  ÿ¨ÿ®ÿ±ÿßŸÜ ÿÆŸÑŸäŸÑ ÿ¨ÿ®ÿ±ÿßŸÜ   \n4  ÿ•ŸÜŸëŸéŸá ŸÑŸÖŸÜ ÿßŸÑÿπÿ®ÿ´ ÿßŸÑÿßÿ≥ÿ™ÿ∑ÿ±ÿßÿØ ŸÅŸä ÿ™Ÿàÿ∂Ÿäÿ≠ ÿ£Ÿà ÿ™ÿµŸàŸäÿ± ÿÆÿ∑...        ŸäŸàÿ≥ŸÅ ÿ•ÿØÿ±Ÿäÿ≥   \n\n   true_id  \n0        8  \n1        6  \n2       17  \n3        5  \n4       20  \n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# ‚îÄ‚îÄ cell: 8 ‚îÄ‚îÄ\n# üîç Probability Prediction (in-style)\nimport numpy as np\n\n# def predict_instyle_prob(text, tokenizer, model):\n#     enc = get_chunks(text, tokenizer)\n#     enc = {k: v.to(device()) for k,v in enc.items()}\n#     with torch.no_grad():\n#         logits = model(**enc).logits  # [chunks, 2]\n#         probs = torch.softmax(logits, dim=-1)  # same shape\n#     instyle_probs = probs[:,1]  # author style class\n#     return instyle_probs.mean().item()\n\n\nVALID_KEYS = {\"input_ids\", \"attention_mask\", \"token_type_ids\"}\n\ndef predict_instyle_prob(text, tokenizer, model):\n    enc = get_chunks(text, tokenizer)\n    enc = {k: v.to(device()) for k,v in enc.items() if k in VALID_KEYS}\n    with torch.no_grad():\n        logits = model(**enc).logits  # Now works\n        probs = torch.softmax(logits, dim=-1)\n    return probs[:,1].mean().item() # author style class\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:51.135766Z","iopub.execute_input":"2025-08-03T12:48:51.136008Z","iopub.status.idle":"2025-08-03T12:48:51.141676Z","shell.execute_reply.started":"2025-08-03T12:48:51.135983Z","shell.execute_reply":"2025-08-03T12:48:51.140976Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# # ‚îÄ‚îÄ cell: 9 ‚îÄ‚îÄ\n# # üìä Style Evaluation Loop\n# from tqdm import tqdm\n# results = []\n# for author_id in range(21):\n#     tokenizer, model = load_binary_classifier(author_id)\n#     for row in tqdm(df.itertuples(), desc=f\"Author {author_id}\"):\n#         px = predict_instyle_prob(row.neutral_text, tokenizer, model)\n#         py = predict_instyle_prob(row.generated_text, tokenizer, model)\n#         diff = py - px\n#         success = diff > 0\n#         results.append({\n#             \"sample_id\": row.sample_id,\n#             \"true_id\": row.true_id,\n#             \"classifier_id\": author_id,\n#             \"neutral_prob\": px,\n#             \"generated_prob\": py,\n#             \"diff\": diff,\n#             \"success\": int(success)\n#         })\n# results_df = pd.DataFrame(results)\n\n\n\n# ‚îÄ‚îÄ cell: 9 ‚îÄ‚îÄ\n# üìä Style Evaluation Loop (per‚Äëauthor filtering)\nfrom tqdm import tqdm\n\nresults = []\n\nfor author_id in range(21):\n    # 1) select only the samples meant for this author\n    author_df = df[df[\"true_id\"] == author_id]\n\n    # 2) load that author‚Äôs classifier\n    tokenizer, model = load_binary_classifier(author_id)\n\n    # 3) run only their own samples\n    for row in tqdm(author_df.itertuples(), desc=f\"Author {author_id}\"):\n        px   = predict_instyle_prob(row.neutral_text,   tokenizer, model)\n        py   = predict_instyle_prob(row.generated_text, tokenizer, model)\n        diff = py - px\n        success = int(diff > 0)\n\n        results.append({\n            \"sample_id\":      row.sample_id,\n            \"true_id\":        row.true_id,\n            \"classifier_id\":  author_id,\n            \"neutral_prob\":   px,\n            \"generated_prob\": py,\n            \"diff\":           diff,\n            \"success\":        success\n        })\n\n# build DataFrame after the loop\nresults_df = pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:48:51.142471Z","iopub.execute_input":"2025-08-03T12:48:51.142796Z","iopub.status.idle":"2025-08-03T12:56:45.937359Z","shell.execute_reply.started":"2025-08-03T12:48:51.142777Z","shell.execute_reply":"2025-08-03T12:56:45.936727Z"}},"outputs":[{"name":"stderr","text":"Author 0: 597it [00:31, 18.87it/s]\nAuthor 1: 147it [00:08, 18.01it/s]\nAuthor 2: 55it [00:03, 16.61it/s]\nAuthor 3: 624it [00:34, 17.97it/s]\nAuthor 4: 191it [00:10, 17.94it/s]\nAuthor 5: 240it [00:11, 20.13it/s]\nAuthor 6: 570it [00:32, 17.71it/s]\nAuthor 7: 1004it [00:53, 18.66it/s]\nAuthor 8: 512it [00:29, 17.34it/s]\nAuthor 9: 282it [00:13, 21.32it/s]\nAuthor 10: 538it [00:23, 23.30it/s]\nAuthor 11: 500it [00:22, 22.15it/s]\nAuthor 12: 464it [00:22, 20.79it/s]\nAuthor 13: 358it [00:15, 23.79it/s]\nAuthor 14: 294it [00:12, 24.34it/s]\nAuthor 15: 110it [00:07, 15.43it/s]\nAuthor 16: 492it [00:23, 21.14it/s]\nAuthor 17: 343it [00:18, 19.03it/s]\nAuthor 18: 383it [00:20, 19.11it/s]\nAuthor 19: 360it [00:20, 17.26it/s]\nAuthor 20: 349it [00:14, 23.73it/s]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Save results","metadata":{}},{"cell_type":"code","source":"save_dir = f\"evaluation/ar_style_classifier/results/{MODEL_NAME}\"\n!mkdir -p {save_dir}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:56:45.938065Z","iopub.execute_input":"2025-08-03T12:56:45.938342Z","iopub.status.idle":"2025-08-03T12:56:46.113497Z","shell.execute_reply.started":"2025-08-03T12:56:45.938316Z","shell.execute_reply":"2025-08-03T12:56:46.112760Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# ‚îÄ‚îÄ cell: 10 ‚îÄ‚îÄ\n# üíæ Save Results and Metrics\nresults_df.to_csv(\n    f\"evaluation/ar_style_classifier/results/{MODEL_NAME}/ar_bsc_evaluation_results_.csv\",\n    index=False\n)\n\n# üßæ Aggregate metrics per classifier\nmetrics = results_df.groupby(\"classifier_id\").agg(\n    accuracy=(\"success\", \"mean\"),\n    avg_diff=(\"diff\", \"mean\"),\n    count=(\"success\", \"size\")\n).reset_index()\n\n# üåê Compute overall metrics across all classifiers and samples\noverall_accuracy = results_df[\"success\"].mean()\noverall_avg_diff = results_df[\"diff\"].mean()\n\n# Create an overall-metrics DataFrame row\noverall_row = pd.DataFrame([{\n    \"classifier_id\": -1,\n    \"accuracy\": overall_accuracy,\n    \"avg_diff\": overall_avg_diff,\n    \"count\": len(results_df)\n}])\n\n# Concatenate to include overall stats\nmetrics = pd.concat([metrics, overall_row], ignore_index=True)\n\n# üñ® Print overall metrics\nprint(f\"Overall Style-Transfer Accuracy: {overall_accuracy:.4f}\")\nprint(f\"Overall Average Diff (strength): {overall_avg_diff:.4f}\")\n\n# üíæ Save both per-author and overall metrics\nmetrics.to_csv(\n    f\"evaluation/ar_style_classifier/results/{MODEL_NAME}/ar_bsc_metrics_per_author_and_overall.csv\",\n    index=False\n)\n\nprint(\"‚úÖ Saved evaluation and per-author metrics CSVs (including overall metric).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:56:46.114737Z","iopub.execute_input":"2025-08-03T12:56:46.114993Z","iopub.status.idle":"2025-08-03T12:56:46.206386Z","shell.execute_reply.started":"2025-08-03T12:56:46.114969Z","shell.execute_reply":"2025-08-03T12:56:46.205627Z"}},"outputs":[{"name":"stdout","text":"Overall Style-Transfer Accuracy: 0.8610\nOverall Average Diff (strength): 0.2932\n‚úÖ Saved evaluation and per-author metrics CSVs (including overall metric).\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ‚îÄ‚îÄ cell: 11 ‚îÄ‚îÄ\n# üìà Per-Sample Summary: Identify strongest style match and overall success rate\n# For each sample, find the classifier with highest generated_prob\ntop_matches = results_df.loc[results_df.groupby('sample_id')['generated_prob'].idxmax()]\n# top_matches = top_matches.merge(id2author.to_frame('classifier_id').reset_index(), on='classifier_id')\nid2author_df = pd.DataFrame(list(id2author.items()), columns=['classifier_id', 'predicted_author'])\ntop_matches = top_matches.merge(id2author_df, on='classifier_id')\ntop_matches = top_matches.rename(columns={\n    'classifier_id': 'predicted_id',\n    'index': 'predicted_author'\n})\n# Calculate overall success: did the true author classifier succeed?\ntrue_success = results_df[\n    (results_df['classifier_id'] == results_df['true_id'])\n].groupby('sample_id')['success'].first().reset_index()\n\n# Merge for overview\noverview = top_matches.merge(true_success, on='sample_id')\n\noverview.to_csv(f\"evaluation/ar_style_classifier/results/{MODEL_NAME}/ar_bsc_evaluation_per_sample_overview.csv\", index=False)\nprint(\"‚úÖ Saved per-sample overview CSV: strongest match and true-author success.\")\n\n# üí° Note: Probabilities are averaged across all chunks of a sample, so each sample has a single neutral_prob and generated_prob per classifier.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:56:46.207277Z","iopub.execute_input":"2025-08-03T12:56:46.207523Z","iopub.status.idle":"2025-08-03T12:56:46.288609Z","shell.execute_reply.started":"2025-08-03T12:56:46.207506Z","shell.execute_reply":"2025-08-03T12:56:46.287890Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Saved per-sample overview CSV: strongest match and true-author success.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# ‚îÄ‚îÄ cell: 12 ‚îÄ‚îÄ\n# üì¶ Package Results Archive\nimport os, subprocess\n\nzip_name = f\"ar_style_classifier_results_{MODEL_NAME}.zip\"\nresults_dir = os.path.join(\"evaluation\", \"ar_style_classifier\", \"results\", MODEL_NAME)\n\n# Run compression\n!cd evaluation/ar_style_classifier && zip -r {zip_name} results/{MODEL_NAME}\n\nprint(f\"‚úÖ Created archive: evaluation/ar_style_classifier/{zip_name}\")\n\n# !cd evaluation/ar_style_classifier && zip -r ar_style_classifier_results_{MODEL_NAME}.zip results/{MODEL_NAME}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:56:46.289477Z","iopub.execute_input":"2025-08-03T12:56:46.289749Z","iopub.status.idle":"2025-08-03T12:56:46.606646Z","shell.execute_reply.started":"2025-08-03T12:56:46.289724Z","shell.execute_reply":"2025-08-03T12:56:46.605772Z"}},"outputs":[{"name":"stdout","text":"  adding: results/facebook_mbart-large-50-many-to-many-mmt/ (stored 0%)\n  adding: results/facebook_mbart-large-50-many-to-many-mmt/ar_bsc_metrics_per_author_and_overall.csv (deflated 47%)\n  adding: results/facebook_mbart-large-50-many-to-many-mmt/ar_bsc_evaluation_results_.csv (deflated 60%)\n  adding: results/facebook_mbart-large-50-many-to-many-mmt/ar_bsc_evaluation_per_sample_overview.csv (deflated 65%)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Created archive: evaluation/ar_style_classifier/ar_style_classifier_results_facebook_mbart-large-50-many-to-many-mmt.zip\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# # --- Inference on full validation texts ---\n# def predict_full_text(text):\n#     enc = tokenizer(\n#         text,\n#         truncation=True,\n#         padding=\"max_length\",\n#         max_length=512,\n#         stride=256,\n#         return_overflowing_tokens=True,\n#         return_tensors=\"pt\"\n#     ).to(device)\n#     logits = model(**enc).logits  # [num_chunks, num_labels]\n#     avg_logits = logits.mean(dim=0)\n#     probs = torch.softmax(avg_logits, dim=0)\n#     return torch.argmax(probs).item(), probs.cpu().tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:56:46.615543Z","iopub.execute_input":"2025-08-03T12:56:46.615822Z","iopub.status.idle":"2025-08-03T12:56:46.629830Z","shell.execute_reply.started":"2025-08-03T12:56:46.615802Z","shell.execute_reply":"2025-08-03T12:56:46.629184Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# # --- Evaluate across validation set ---\n# preds, refs = [], []\n# for example in val_df.itertuples():\n#     pred, _ = predict_full_text(example.text_in_author_style)\n#     preds.append(pred)\n#     refs.append(author2id[example.author])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:56:46.630600Z","iopub.execute_input":"2025-08-03T12:56:46.630913Z","iopub.status.idle":"2025-08-03T12:56:46.647460Z","shell.execute_reply.started":"2025-08-03T12:56:46.630892Z","shell.execute_reply":"2025-08-03T12:56:46.646898Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# from sklearn.metrics import accuracy_score, f1_score\n# print(\"Validation Accuracy:\", accuracy_score(refs, preds))\n# print(\"Validation F1‚Äëmacro:\", f1_score(refs, preds, average=\"macro\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:56:46.648161Z","iopub.execute_input":"2025-08-03T12:56:46.648316Z","iopub.status.idle":"2025-08-03T12:56:46.659604Z","shell.execute_reply.started":"2025-08-03T12:56:46.648304Z","shell.execute_reply":"2025-08-03T12:56:46.659008Z"}},"outputs":[],"execution_count":38}]}