{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559ddd13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-14T06:45:13.641353Z",
     "iopub.status.busy": "2025-07-14T06:45:13.641015Z",
     "iopub.status.idle": "2025-07-14T06:46:43.756338Z",
     "shell.execute_reply": "2025-07-14T06:46:43.755209Z"
    },
    "id": "559ddd13",
    "outputId": "7921c04f-9a42-44dc-e548-5c1805145485",
    "papermill": {
     "duration": 90.123476,
     "end_time": "2025-07-14T06:46:43.758212",
     "exception": false,
     "start_time": "2025-07-14T06:45:13.634736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install transformers[sentencepiece] --quiet\n",
    "!pip install torch --quiet\n",
    "!pip install pandas openpyxl psutil python-dotenv --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fTyhOO2v7ssz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "execution": {
     "iopub.execute_input": "2025-07-14T06:46:43.827090Z",
     "iopub.status.busy": "2025-07-14T06:46:43.826179Z",
     "iopub.status.idle": "2025-07-14T06:46:43.830249Z",
     "shell.execute_reply": "2025-07-14T06:46:43.829574Z"
    },
    "id": "fTyhOO2v7ssz",
    "outputId": "7eecf25f-5840-4ed0-c27a-fa479b3c0838",
    "papermill": {
     "duration": 0.038757,
     "end_time": "2025-07-14T06:46:43.831489",
     "exception": false,
     "start_time": "2025-07-14T06:46:43.792732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df00333d-fbfa-483f-8a56-4372415c5b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:46:43.898584Z",
     "iopub.status.busy": "2025-07-14T06:46:43.897896Z",
     "iopub.status.idle": "2025-07-14T06:47:03.310100Z",
     "shell.execute_reply": "2025-07-14T06:47:03.309104Z"
    },
    "papermill": {
     "duration": 19.447473,
     "end_time": "2025-07-14T06:47:03.311976",
     "exception": false,
     "start_time": "2025-07-14T06:46:43.864503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb\r\n",
      "✅ '/kaggle/working' is empty. copying project strcuture into it ...\n",
      "/kaggle/working\n",
      "analysis   data        __init__.py  __notebook__.ipynb\trun_pipeline.py\r\n",
      "config.py  evaluation  models\t    prompts\t\tutils\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/kaggle/working\"\n",
    "is_empty = (len(os.listdir(path)) <= 3)\n",
    "\n",
    "!ls /kaggle/working/\n",
    "\n",
    "# !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n",
    "\n",
    "if is_empty:\n",
    "    print(f\"✅ '{path}' is empty. copying project strcuture into it ...\")\n",
    "    !cp -r /kaggle/input/arageneval2025-task1-sc/* /kaggle/working/\n",
    "    !mkdir -p /kaggle/working/analysis/fine_tuned_model/UBC-NLP_AraT5v2-base-1024/final/\n",
    "    !cp -r /kaggle/input/ubc-nlp-rat5v2-base-1024-model/UBC-NLP_AraT5v2-base-1024/final/* /kaggle/working/analysis/fine_tuned_model/UBC-NLP_AraT5v2-base-1024/final/\n",
    "else:\n",
    "    print(f\"❌ '{path}' is not empty — contains {len(os.listdir(path))} items.\")\n",
    "\n",
    "# Change to project directory\n",
    "%cd /kaggle/working/ \n",
    "\n",
    "# Confirm\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d77799b-263e-4e36-b283-d3b4bf56f206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:03.378879Z",
     "iopub.status.busy": "2025-07-14T06:47:03.378545Z",
     "iopub.status.idle": "2025-07-14T06:47:03.497955Z",
     "shell.execute_reply": "2025-07-14T06:47:03.496997Z"
    },
    "papermill": {
     "duration": 0.154964,
     "end_time": "2025-07-14T06:47:03.499832",
     "exception": false,
     "start_time": "2025-07-14T06:47:03.344868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/analysis/fine_tuned_model/UBC-NLP_AraT5v2-base-1024:\r\n",
      "final\r\n",
      "\r\n",
      "/kaggle/working/analysis/fine_tuned_model/UBC-NLP_AraT5v2-base-1024/final:\r\n",
      "added_tokens.json\tmodel.safetensors\t tokenizer_config.json\r\n",
      "config.json\t\tspecial_tokens_map.json  training_args.bin\r\n",
      "generation_config.json\tspiece.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R /kaggle/working/analysis/fine_tuned_model/UBC-NLP_AraT5v2-base-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4i7DxBmWC5tQ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:03.566842Z",
     "iopub.status.busy": "2025-07-14T06:47:03.566550Z",
     "iopub.status.idle": "2025-07-14T06:47:03.571400Z",
     "shell.execute_reply": "2025-07-14T06:47:03.570302Z"
    },
    "id": "4i7DxBmWC5tQ",
    "papermill": {
     "duration": 0.040029,
     "end_time": "2025-07-14T06:47:03.572770",
     "exception": false,
     "start_time": "2025-07-14T06:47:03.532741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"UBC-NLP/AraT5v2-base-1024\" #\"agemagician_mlong-t5-tglobal-large\" #\"facebook_mbart-large-50-many-to-many-mmt\" # \"UBC-NLP_AraT5-base\" #\"google/mt5-small\"\n",
    "\n",
    "# !unzip -o /content/drive/MyDrive/AraGenEval2025/models/fine_tuned_model/model_weights/{model_name}/final_model_weights.zip -d /content/drive/MyDrive/AraGenEval2025/models/fine_tuned_model/model_weights/{model_name}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1rI7F4dlowA",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:03.703027Z",
     "iopub.status.busy": "2025-07-14T06:47:03.702739Z",
     "iopub.status.idle": "2025-07-14T06:47:03.706859Z",
     "shell.execute_reply": "2025-07-14T06:47:03.706085Z"
    },
    "id": "b1rI7F4dlowA",
    "papermill": {
     "duration": 0.039224,
     "end_time": "2025-07-14T06:47:03.708322",
     "exception": false,
     "start_time": "2025-07-14T06:47:03.669098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390a0abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:03.776573Z",
     "iopub.status.busy": "2025-07-14T06:47:03.776294Z",
     "iopub.status.idle": "2025-07-14T06:47:15.154370Z",
     "shell.execute_reply": "2025-07-14T06:47:15.153445Z"
    },
    "id": "390a0abf",
    "papermill": {
     "duration": 11.413602,
     "end_time": "2025-07-14T06:47:15.156088",
     "exception": false,
     "start_time": "2025-07-14T06:47:03.742486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import dotenv\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab9aba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:15.227826Z",
     "iopub.status.busy": "2025-07-14T06:47:15.227094Z",
     "iopub.status.idle": "2025-07-14T06:47:15.231450Z",
     "shell.execute_reply": "2025-07-14T06:47:15.230488Z"
    },
    "id": "cab9aba0",
    "papermill": {
     "duration": 0.041911,
     "end_time": "2025-07-14T06:47:15.232757",
     "exception": false,
     "start_time": "2025-07-14T06:47:15.190846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# project_dir = \"/content/drive/MyDrive/AraGenEval2025\"\n",
    "# os.chdir(project_dir)\n",
    "# print(f\"✅ Current directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c83cd-2210-4703-8ffe-2fd5b0a53590",
   "metadata": {
    "papermill": {
     "duration": 0.033772,
     "end_time": "2025-07-14T06:47:15.300326",
     "exception": false,
     "start_time": "2025-07-14T06:47:15.266554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643eee59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:15.368153Z",
     "iopub.status.busy": "2025-07-14T06:47:15.367826Z",
     "iopub.status.idle": "2025-07-14T06:47:15.375327Z",
     "shell.execute_reply": "2025-07-14T06:47:15.374509Z"
    },
    "id": "643eee59",
    "papermill": {
     "duration": 0.043203,
     "end_time": "2025-07-14T06:47:15.376704",
     "exception": false,
     "start_time": "2025-07-14T06:47:15.333501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "from config import Config\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbf67812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:15.446966Z",
     "iopub.status.busy": "2025-07-14T06:47:15.446676Z",
     "iopub.status.idle": "2025-07-14T06:47:15.451766Z",
     "shell.execute_reply": "2025-07-14T06:47:15.451078Z"
    },
    "id": "cbf67812",
    "papermill": {
     "duration": 0.041296,
     "end_time": "2025-07-14T06:47:15.453014",
     "exception": false,
     "start_time": "2025-07-14T06:47:15.411718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def safe_model_name(model_name):\n",
    "    return model_name.replace(\"/\", \"_\")\n",
    "\n",
    "def create_run_dir(base_dir, model_name):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = os.path.join(base_dir, safe_model_name(model_name), f\"run_{timestamp}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d09aa414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:15.524065Z",
     "iopub.status.busy": "2025-07-14T06:47:15.523392Z",
     "iopub.status.idle": "2025-07-14T06:47:15.528972Z",
     "shell.execute_reply": "2025-07-14T06:47:15.528255Z"
    },
    "id": "d09aa414",
    "papermill": {
     "duration": 0.04142,
     "end_time": "2025-07-14T06:47:15.530296",
     "exception": false,
     "start_time": "2025-07-14T06:47:15.488876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_name):\n",
    "    secure_name = safe_model_name(model_name)\n",
    "    print(f\"Loading model: {secure_name}\")\n",
    "    model_path = os.path.join(\"models/fine_tuned_model/model_weights\", secure_name, \"final\")\n",
    "    print(f\" loading fine tuned model from {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, legacy=False)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "    if \"mbart\" in model_name.lower():\n",
    "        tokenizer.src_lang = \"ar_AR\"\n",
    "        tokenizer.tgt_lang = \"ar_AR\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d797f524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:15.599483Z",
     "iopub.status.busy": "2025-07-14T06:47:15.598803Z",
     "iopub.status.idle": "2025-07-14T06:47:15.603537Z",
     "shell.execute_reply": "2025-07-14T06:47:15.602881Z"
    },
    "id": "d797f524",
    "papermill": {
     "duration": 0.040399,
     "end_time": "2025-07-14T06:47:15.604797",
     "exception": false,
     "start_time": "2025-07-14T06:47:15.564398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_evaluation():\n",
    "  pass\n",
    "    # os.system(\"python utils/prepare_metrics_log.py\")\n",
    "    # os.system(\"python evaluation/evaluate.py\")\n",
    "\n",
    "def archive_results(run_dir, files_to_archive):\n",
    "    for fpath in files_to_archive:\n",
    "        if os.path.exists(fpath):\n",
    "            dest = os.path.join(run_dir, os.path.basename(fpath))\n",
    "            os.system(f\"cp {fpath} {dest}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0a4921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:15.675217Z",
     "iopub.status.busy": "2025-07-14T06:47:15.674658Z",
     "iopub.status.idle": "2025-07-14T06:47:15.684138Z",
     "shell.execute_reply": "2025-07-14T06:47:15.683282Z"
    },
    "id": "fb0a4921",
    "papermill": {
     "duration": 0.046629,
     "end_time": "2025-07-14T06:47:15.685688",
     "exception": false,
     "start_time": "2025-07-14T06:47:15.639059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "\n",
    "def generate_predictions(tokenizer, model, validation_path, output_log_path, batch_size=8):\n",
    "    # Load validation data\n",
    "    df = pd.read_excel(validation_path)\n",
    "    df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n",
    "\n",
    "    # Ensure correct special token settings\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    assert model.config.eos_token_id is not None, \"EOS token must be defined\"\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for start in tqdm(range(0, len(df), batch_size)):\n",
    "        batch_texts = df[\"input_text\"].iloc[start:start+batch_size].tolist()\n",
    "\n",
    "        # Tokenize full input without truncation\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False  # ensures full input preservation :contentReference[oaicite:1]{index=1}\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                num_beams=4,           # beam search for coherent output :contentReference[oaicite:2]{index=2}\n",
    "                early_stopping=True,   # stops when beams finish naturally\n",
    "                do_sample=False,        # deterministic decoding\n",
    "                max_length=4096  # Set this one line only!\n",
    "            )\n",
    "\n",
    "        outputs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        for idx, output_text in enumerate(outputs):\n",
    "            row = df.iloc[start + idx]\n",
    "            records.append({\n",
    "                \"id\": int(row[\"id\"]),\n",
    "                \"author\": str(row[\"author\"]),\n",
    "                \"output\": output_text,\n",
    "                \"ground_truth\": str(row[\"text_in_author_style\"])\n",
    "            })\n",
    "\n",
    "        # Clean up memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    with open(output_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ Finished generation for {len(records)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bQCIIwaEEEp9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:15.754098Z",
     "iopub.status.busy": "2025-07-14T06:47:15.753309Z",
     "iopub.status.idle": "2025-07-14T06:47:15.920324Z",
     "shell.execute_reply": "2025-07-14T06:47:15.919328Z"
    },
    "id": "bQCIIwaEEEp9",
    "papermill": {
     "duration": 0.202454,
     "end_time": "2025-07-14T06:47:15.921644",
     "exception": false,
     "start_time": "2025-07-14T06:47:15.719190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Lkh_YWV5Exw8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:15.992131Z",
     "iopub.status.busy": "2025-07-14T06:47:15.991430Z",
     "iopub.status.idle": "2025-07-14T06:47:16.123105Z",
     "shell.execute_reply": "2025-07-14T06:47:16.122006Z"
    },
    "id": "Lkh_YWV5Exw8",
    "papermill": {
     "duration": 0.16832,
     "end_time": "2025-07-14T06:47:16.124774",
     "exception": false,
     "start_time": "2025-07-14T06:47:15.956454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a9c2ada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:16.195216Z",
     "iopub.status.busy": "2025-07-14T06:47:16.194495Z",
     "iopub.status.idle": "2025-07-14T06:47:16.200282Z",
     "shell.execute_reply": "2025-07-14T06:47:16.199362Z"
    },
    "id": "2a9c2ada",
    "papermill": {
     "duration": 0.042368,
     "end_time": "2025-07-14T06:47:16.201654",
     "exception": false,
     "start_time": "2025-07-14T06:47:16.159286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/fine_tuned_model/results/UBC-NLP/AraT5v2-base-1024/generation_log.json\n",
      "analysis/fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_name = \"facebook/mbart-large-50-many-to-many-mmt\" #\"UBC-NLP_AraT5-base\" #\"google/mt5-small\"\n",
    "validation_file = Config.VAL_FILE\n",
    "\n",
    "log_file = f\"models/fine_tuned_model/results/{MODEL_NAME}/generation_log.json\"   #Config.LOG_FILE\n",
    "output_base_dir =  'analysis/fine_tuned_model' #Config.FINE_TUNED_ANALYSIS_DIR\n",
    "\n",
    "print(log_file)\n",
    "print(output_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "EaajhPZnCztI",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:16.274578Z",
     "iopub.status.busy": "2025-07-14T06:47:16.273734Z",
     "iopub.status.idle": "2025-07-14T06:47:16.284870Z",
     "shell.execute_reply": "2025-07-14T06:47:16.283971Z"
    },
    "id": "EaajhPZnCztI",
    "papermill": {
     "duration": 0.049343,
     "end_time": "2025-07-14T06:47:16.286282",
     "exception": false,
     "start_time": "2025-07-14T06:47:16.236939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "\n",
    "def generate_sample_predictions(tokenizer, model, validation_path, sample_count=3):\n",
    "    # Load validation data\n",
    "    df = pd.read_excel(validation_path)\n",
    "    df[\"input_text\"] = df[\"author\"].astype(str) + \" [SEP] \" + df[\"text_in_msa\"].astype(str)\n",
    "\n",
    "    # Use only first few samples\n",
    "    df = df.iloc[:sample_count].copy()\n",
    "\n",
    "    # Ensure correct special token settings\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    assert model.config.eos_token_id is not None, \"EOS token must be defined\"\n",
    "\n",
    "    print(f\"\\n🔍 Testing on {sample_count} samples...\\n\")\n",
    "    records = []\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        input_text = df.iloc[i][\"input_text\"]\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,  # Full input preserved\n",
    "            max_length=1280\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                # num_beams=1,\n",
    "                # early_stopping=True,\n",
    "                # do_sample=False,\n",
    "                max_length=3000  # Important to allow full output\n",
    "            )\n",
    "\n",
    "        output_text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        record = {\n",
    "            \"id\": int(df.iloc[i][\"id\"]),\n",
    "            \"author\": str(df.iloc[i][\"author\"]),\n",
    "            \"input\": str(df.iloc[i][\"text_in_msa\"]),\n",
    "            \"output\": output_text,\n",
    "            \"ground_truth\": str(df.iloc[i][\"text_in_author_style\"])\n",
    "        }\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "        # Print result for inspection\n",
    "        print(f\"\\n📝 Sample {i+1}\")\n",
    "        print(f\"Author: {record['author']}\")\n",
    "        print(f\"Input:\\n{record['input'][:500]}{'...' if len(record['input']) > 500 else ''}\")\n",
    "        print(f\"\\n🔁 Output:\\n{record['output']}\")\n",
    "        print(f\"\\n✅ Ground Truth:\\n{record['ground_truth'][:500]}{'...' if len(record['ground_truth']) > 500 else ''}\")\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        # Memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "    print(f\"\\n✅ Test finished for {num_samples} samples.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tADTP4ZUC09n",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:16.358929Z",
     "iopub.status.busy": "2025-07-14T06:47:16.358141Z",
     "iopub.status.idle": "2025-07-14T06:47:16.361771Z",
     "shell.execute_reply": "2025-07-14T06:47:16.361108Z"
    },
    "id": "tADTP4ZUC09n",
    "papermill": {
     "duration": 0.040039,
     "end_time": "2025-07-14T06:47:16.363068",
     "exception": false,
     "start_time": "2025-07-14T06:47:16.323029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer, model = load_model(MODEL_NAME)\n",
    "# generate_sample_predictions(tokenizer, model, validation_file, sample_count=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "BkVNsoeKcOz0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T06:47:16.432961Z",
     "iopub.status.busy": "2025-07-14T06:47:16.432655Z",
     "iopub.status.idle": "2025-07-14T06:47:16.549384Z",
     "shell.execute_reply": "2025-07-14T06:47:16.548090Z"
    },
    "id": "BkVNsoeKcOz0",
    "papermill": {
     "duration": 0.153237,
     "end_time": "2025-07-14T06:47:16.550607",
     "exception": true,
     "start_time": "2025-07-14T06:47:16.397370",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: UBC-NLP_AraT5v2-base-1024\n",
      " loading fine tuned model from models/fine_tuned_model/model_weights/UBC-NLP_AraT5v2-base-1024/final\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'models/fine_tuned_model/model_weights/UBC-NLP_AraT5v2-base-1024/final'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    471\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'models/fine_tuned_model/model_weights/UBC-NLP_AraT5v2-base-1024/final'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/1812247779.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13/59642045.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/fine_tuned_model/model_weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecure_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"final\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" loading fine tuned model from {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    783\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \"\"\"\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         resolved_files = [\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         ]\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         resolved_files = [\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         ]\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision)\u001b[0m\n\u001b[1;32m    138\u001b[0m ):\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mresolved_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_to_load_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_CACHED_NO_EXIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'models/fine_tuned_model/model_weights/UBC-NLP_AraT5v2-base-1024/final'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "tokenizer, model = load_model(MODEL_NAME)\n",
    "\n",
    "os.makedirs(os.path.dirname(log_file) or \".\", exist_ok=True)\n",
    "generate_predictions(tokenizer, model, validation_file, log_file, batch_size=4)\n",
    "\n",
    "# labeled_log = Config.LABELED_LOG\n",
    "# metrics_log = Config.PER_AUTHOR_METRICS_LOG\n",
    "# run_evaluation()\n",
    "\n",
    "output_dir = create_run_dir(output_base_dir, MODEL_NAME)\n",
    "archive_results(output_dir, [log_file]) # , labeled_log, metrics_log\n",
    "print(log_file)\n",
    "\n",
    "print(f\"\\n✅ Pipeline completed. Results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FzfhVKAGXa8M",
   "metadata": {
    "id": "FzfhVKAGXa8M",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XfgZ4e95XcBU",
   "metadata": {
    "id": "XfgZ4e95XcBU",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7780982,
     "sourceId": 12342718,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7856142,
     "sourceId": 12454188,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 128.955805,
   "end_time": "2025-07-14T06:47:18.107738",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-14T06:45:09.151933",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
